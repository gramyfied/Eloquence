# Diagnostic et Correction de l'Erreur Pydantic `ChatMessage`

## Contexte de l'Erreur

L'application Eloquence rencontre une erreur persistante li√©e √† la validation Pydantic du champ `content` de la classe `livekit.agents.llm.ChatMessage`. Malgr√© les tentatives pr√©c√©dentes de formater le contenu en `[{"type": "text", "text": "..."}]`, le syst√®me LiveKit continue de g√©n√©rer des erreurs telles que :

```
5 validation errors for ChatMessage
content.0.ImageContent.type
  Input should be 'image_content' [type=literal_error, input_value='text', input_type=str]
content.0.ImageContent.image
  Field required [type=missing, input_value={'type': 'text', 'text': ...}, input_type=dict]
content.0.AudioContent.type
  Input should be 'audio_content' [type=literal_error, input_value='text', input_type=str]
content.0.AudioContent.frame
  Field required [type=missing, input_value={'type': 'text', 'text': ...}, input_type=dict]
content.0.str
  Input should be a valid string [type=string_type, input_value={'type': 'text', 'text': ...}, input_type=dict]
```

Cela indique que Pydantic tente de d√©s√©rialiser le contenu texte en tant que `ImageContent` ou `AudioContent` (qui ont des champs obligatoires comme `image` ou `frame`) avant de reconna√Ætre le format `TextContent` correct, ou que la sp√©cification du `ChatMessage` dans la version actuelle de `livekit.agents` a une particularit√©.

## Parties du Code LLM o√π l'Erreur se Produit

L'erreur est principalement rencontr√©e lors de la pr√©paration des messages pour le service LLM, sp√©cifiquement au moment o√π le `llm.ChatMessage` est valid√©. Les points cl√©s o√π les `ChatMessage` sont construites ou trait√©es sont :

1.  **Dans `_call_llm_service` (lignes 559-566 de `real_time_voice_agent_force_audio.py`)**:
    Initialisation des messages syst√®me et utilisateur avant le premier appel au LLM.

    ```python
    # services/api-backend/services/real_time_voice_agent_force_audio.py
    # ...
    553 |             # 1. Cr√©er un ChatContext compatible avec le format liste requis
    554 |             chat_ctx = llm.ChatContext()
    555 |             
    556 |             # CORRECTION: Cr√©er les messages avec le format LISTE requis par Pydantic validation
    557 |             system_msg = llm.ChatMessage(
    558 |                 role="system",
    559 |                 content=[{"type": "text", "text": "Tu es un coach vocal expert. Sois concis et encourageant."}]
    560 |             )
    561 |             user_msg = llm.ChatMessage(
    562 |                 role="user",
    563 |                 content=[{"type": "text", "text": transcription}]
    564 |             )
    565 |             
    566 |             # Ajouter les messages au contexte
    567 |             chat_ctx.messages = [system_msg, user_msg]
    # ...
    ```

2.  **Dans `process_with_llm` (lignes 1787-1791 de `real_time_voice_agent_force_audio.py`)**:
    Fonction utilitaire qui pr√©pare √©galement les messages pour le LLM dans le pipeline VAD -> STT -> LLM.

    ```python
    # services/api-backend/services/real_time_voice_agent_force_audio.py
    # ...
    1784 |         chat_ctx = agents.llm.ChatContext()
    1785 |         
    1786 |         # CORRECTION: Cr√©er les messages avec le format LISTE requis par Pydantic validation
    1787 |         system_msg = agents.llm.ChatMessage(role="system", content=[{"type": "text", "text": "Tu es un assistant vocal. R√©ponds de mani√®re concise et naturelle."}])
    1788 |         user_msg = agents.llm.ChatMessage(role="user", content=[{"type": "text", "text": text}])
    1789 |         
    1790 |         chat_ctx.messages = [system_msg, user_msg]
    # ...
    ```

3.  **Dans `CustomLLM.chat()` (lignes 890-906 de `real_time_voice_agent_force_audio.py`)**:
    La logique d'adaptation du contenu des messages est appliqu√©e ici, o√π les `ChatMessage` transitent avant d'√™tre envoy√©es au fournisseur Mistral. C'est visiblement l√† que les erreurs de validation Pydantic se manifestent, indiquant que m√™me apr√®s cette conversion, le format final n'est pas celui attendu par la validation interne de `livekit.agents`.

    ```python
    # services/api-backend/services/real_time_voice_agent_force_audio.py
    # ...
    888 |         messages_from_ctx = []
    889 |         if hasattr(chat_ctx, 'messages'):
    890 |             for msg in chat_ctx.messages:
    891 |                 # V√©rifier si msg.content est une cha√Æne ou une liste de dictionnaires
    892 |                 if isinstance(msg.content, str):
    893 |                     # Si c'est une cha√Æne, la transformer en format [{'type': 'text', 'text': '...'}]
    894 |                     processed_content = [{"type": "text", "text": msg.content}]
    895 |                 elif isinstance(msg.content, list):
    896 |                     # Si c'est une liste, v√©rifier si les √©l√©ments sont des dictionnaires valides
    897 |                     # Pour simplifier, nous allons supposer que c'est le format correct d√©sir√©
    898 |                     processed_content = msg.content
    899 |                 else:
    900 |                     # G√©rer les cas inattendus, par exemple en retournant une cha√Æne vide ou en loggant une erreur
    901 |                     logger.warning(f"‚ö†Ô∏è Type de contenu inattendu pour ChatMessage: {type(msg.content)}")
    902 |                     processed_content = [{"type": "text", "text": str(msg.content)}] # Convertir en cha√Æne au cas o√π
    903 | 
    904 |                 messages_from_ctx.append({"role": str(msg.role), "content": processed_content})
    # ...
    ```

## Configuration du Service LLM (Mistral)

Le service LLM utilise l'API Mistral. Les param√®tres de configuration sont d√©finis via des variables d'environnement et utilis√©s dans la classe `CustomLLMStream`:

```python
# services/api-backend/services/real_time_voice_agent_force_audio.py
# ...
77 | MISTRAL_BASE_URL = os.environ.get("MISTRAL_BASE_URL")
78 | MISTRAL_API_KEY = os.environ.get("MISTRAL_API_KEY")
79 | MISTRAL_MODEL = os.environ.get("MISTRAL_MODEL", "mistral-nemo-instruct-2407")
# ...
1024 |             data = {
1025 |                 "model": MISTRAL_MODEL,
1026 |                 "messages": self._messages,
1027 |                 "temperature": self._temperature,
1028 |                 "stream": True
1029 |             }
# ...
```

## Formatage des Messages avant Envoi au LLM

L'objectif est d'assurer que le contenu des messages envoy√©s au LLM est toujours une liste de dictionnaires, o√π chaque dictionnaire sp√©cifie le type de contenu (ex. `text`) et sa valeur. Le format attendu pour le contenu textuel est `[{"type": "text", "text": "Votre texte ici"}]`.

## Conversion STT ‚Üí LLM

Le flux de conversion de la transcription (Speech-to-Text) vers l'appel au mod√®le de langage (LLM) est g√©r√© comme suit :

1.  **Dans `_process_chunk_with_whisper`**:
    Apr√®s que Whisper ait transcris l'audio (lignes 451-477), la transcription est pass√©e √† `_call_llm_service` (l. 495).

    ```python
    # services/api-backend/services/real_time_voice_agent_force_audio.py
    # ...
    495 |                         llm_response = await self._call_llm_service(transcription)
    # ...
    ```

2.  **Dans `process_with_llm`**:
    Cette fonction est appel√©e quand le VAD d√©tecte la fin d'une parole. Elle prend la transcription et cr√©e un `ChatContext` pour le LLM.

    ```python
    # services/api-backend/services/real_time_voice_agent_force_audio.py
    # ...
    1695 |                                 transcription = await transcribe_audio_with_whisper(wav_data)
    1696 |                                 
    1697 |                                 if transcription and transcription.strip():
    1698 |                                     logger.info(f"üìù [STT] Transcription: {transcription}")
    1699 |                                     
    1700 |                                     # Traitement LLM
    1701 |                                     response = await process_with_llm(transcription, llm)
    # ...
    ```

## Analyse Approfondie de l'Erreur Pydantic Persistante

L'erreur indiquant que Pydantic attend `'image_content'` ou `'audio_content'` mais re√ßoit `'text'` pour le champ `content.0.type` sugg√®re un probl√®me fondamental avec la mani√®re dont `livekit.agents.llm.ChatMessage` (et ses composants `llm.Content`, `llm.TextContent`, `llm.ImageContent`, etc.) g√®re la validation des types.

**Hypoth√®se :** Il est probable que l'impl√©mentation de Pydantic dans la version de `livekit.agents` utilis√©e tente de valider les √©l√©ments de `content` contre une union de types (`Union[TextContent, ImageContent, AudioContent, ...]`). Si les types `ImageContent` ou `AudioContent` sont prioritaires dans cette union, ou si leur validation est moins permissive, cela peut provoquer l'√©chec. Notre format `{"type": "text", "text": "..."}` devrait correspondre √† `TextContent`, mais la validation √©choue comme si elle for√ßait un autre type compatible.

## Proposition de Correction

Puisque la modification directe du code de `livekit.agents` n'est pas possible, la solution doit r√©sider dans des ajustements de l'appel √† la biblioth√®que. Il y a plusieurs pistes :

1.  **V√©rifier la version de `livekit.agents`**: S'assurer que nous utilisons une version stable et bien document√©e, et rechercher sp√©cifiquement tout `breaking change` concernant `llm.ChatMessage` et ses types de contenu. Une mise √† jour ou un retour √† une version pr√©c√©dente pourrait r√©soudre le probl√®me si c'est un bug dans une version sp√©cifique.

2.  **Double-v√©rifier les types `livekit.agents.llm`**: Examiner si des utilitaires sont fournis par `livekit.agents` pour construire ces messages de mani√®re plus robuste ou si un format diff√©rent (ou une encapsulation) est n√©cessaire. Une introspection du module `livekit.agents.llm` dans un environnement Python peut r√©v√©ler la structure exacte.

3.  **Encapsuler le contenu de mani√®re plus explicite (si le SDK le permet)**: Si `llm.ChatMessage` a une m√©thode `add_text` ou √©quivalente, l'utiliser pourrait contourner la validation directe de `content`. Cependant, l'inspection montre que `ChatMessage` est un mod√®le Pydantic o√π `content` est un champ direct.

4.  **Consid√©rer un contournement temporaire**: Si le temps presse et qu'aucune solution directe n'est trouv√©e, un contournement au niveau de l'API Mistral pourrait √™tre d'envoyer un simple string comme contenu au lieu de la liste de dictionnaires, si l'API Mistral le permet, mais cela irait √† l'encontre de la sp√©cification de `livekit.agents`.

**Action Recommand√©e :**

La premi√®re √©tape la plus logique est de tenter de forcer la validation des types de contenu dans `CustomLLM.chat()` en utilisant explicitement les classes de types de contenu de LiveKit. Si `livekit.agents.llm.TextContent` est r√©ellement la classe attendue, l'instancier directement pourrait r√©soudre le probl√®me.

```python
# services/api-backend/services/real_time_voice_agent_force_audio.py
# ... dans CustomLLM.chat()
                for msg in chat_ctx.messages:
                    processed_content = []
                    if isinstance(msg.content, str):
                        # Cr√©er explicitement un TextContent de livekit.agents.llm
                        processed_content = [llm.TextContent(text=msg.content)]
                    elif isinstance(msg.content, list):
                        for item in msg.content:
                            if isinstance(item, dict) and item.get("type") == "text":
                                processed_content.append(llm.TextContent(text=item["text"]))
                            # Ajouter la gestion d'autres types si n√©cessaire (ImageContent, AudioContent)
                            # else: laisser tel quel ou logguer l'erreur
                    else:
                        logger.warning(f"‚ö†Ô∏è Type de contenu inattendu pour ChatMessage: {type(msg.content)}")
                        processed_content = [llm.TextContent(text=str(msg.content))] # Fallback

                    messages_from_ctx.append({"role": str(msg.role), "content": processed_content})
# ...
```

Et de m√™me dans les fonctions `_call_llm_service` et `process_with_llm` o√π `llm.ChatMessage` est instanci√© :

```python
# services/api-backend/services/real_time_voice_agent_force_audio.py
# ... dans _call_llm_service et process_with_llm
            system_msg = llm.ChatMessage(
                role="system",
                content=[llm.TextContent(text="Tu es un coach vocal expert. Sois concis et encourageant.")]
            )
            user_msg = llm.ChatMessage(
                role="user",
                content=[llm.TextContent(text=transcription)]
            )
# ...
```

Cette approche tente de fournir √† Pydantic l'objet de type `TextContent` *directement* plut√¥t qu'un dictionnaire qu'il doit tenter de d√©s√©rialiser en `TextContent`. Si le probl√®me vient de la d√©s√©rialisation trop "intelligente" de Pydantic, cela pourrait le r√©soudre.

Apr√®s l'application de ce changement, il sera n√©cessaire de reconstruire le conteneur Docker avec `docker-compose build --no-cache` et de red√©marrer le service pour que les modifications prennent effet. Il est ensuite imp√©ratif de surveiller attentivement les logs pour valider la correction de l'erreur Pydantic.