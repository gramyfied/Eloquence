import asyncio
import json
import logging
import os
import time
import io
from dataclasses import dataclass
from typing import Any, AsyncIterator, Callable, Dict, List, Optional

import numpy as np
from livekit import rtc
from livekit import agents
from livekit.agents import JobContext, stt, tts, Agent, AgentSession
from livekit.agents.llm import ChatMessage, ChatContext
from livekit.plugins import silero
# from livekit.plugins.turn_detector.english import EnglishModel  # D√©sactiv√© temporairement
from datetime import datetime
import aiohttp
import tempfile
import wave
import base64
from scipy.signal import resample
from unittest.mock import AsyncMock

import uuid

# Initialiser le logger AVANT de l'utiliser avec un handler
logging.basicConfig(level=logging.DEBUG, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Registre global des sessions HTTP pour fermeture propre
_global_http_sessions = set()

def register_http_session(session):
    """Enregistrer une session HTTP pour fermeture propre"""
    _global_http_sessions.add(session)
    logger.info(f"üìù [GLOBAL REGISTRY] Session HTTP enregistr√©e: {session} (Total: {len(_global_http_sessions)})")

def unregister_http_session(session):
    """D√©senregistrer une session HTTP"""
    _global_http_sessions.discard(session)
    logger.info(f"üóëÔ∏è [GLOBAL REGISTRY] Session HTTP d√©senregistr√©e: {session} (Total: {len(_global_http_sessions)})")

async def close_all_http_sessions():
    """Fermer toutes les sessions HTTP enregistr√©es"""
    sessions_to_close = list(_global_http_sessions)
    for session in sessions_to_close:
        if not session.closed:
            try:
                await session.close()
                logger.debug(f"‚úÖ [GLOBAL CLEANUP] Session HTTP ferm√©e: {session}")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è [GLOBAL CLEANUP] Erreur fermeture session: {e}")
        unregister_http_session(session)
    logger.info(f"‚úÖ [GLOBAL CLEANUP] {len(sessions_to_close)} sessions HTTP ferm√©es")

# CORRECTION: Charger les variables d'environnement depuis le fichier .env
try:
    from dotenv import load_dotenv
    load_dotenv()
    logger.info("[OK] [ENV] Variables d'environnement chargees depuis .env")
except ImportError:
    logger.warning("[WARN] [ENV] python-dotenv non installe - chargement manuel des variables")
    # Chargement manuel du fichier .env
    env_file = os.path.join(os.path.dirname(__file__), '.env')
    if os.path.exists(env_file):
        with open(env_file, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ[key.strip()] = value.strip()
        logger.info("[OK] [ENV] Variables d'environnement chargees manuellement depuis .env")
    else:
        logger.warning("[WARN] [ENV] Fichier .env non trouve")

# Configuration d√©taill√©e - API Mistral via Scaleway
MISTRAL_BASE_URL = os.environ.get("MISTRAL_BASE_URL")
MISTRAL_API_KEY = os.environ.get("MISTRAL_API_KEY")
MISTRAL_MODEL = os.environ.get("MISTRAL_MODEL", "mistral-nemo-instruct-2407")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

# Param√®tres de gestion du VAD et Whisper
MIN_CHUNK_SIZE = 6000
MAX_CHUNK_SIZE = 48000

# AUDIO_INTERVAL_MS contr√¥le le temps d'attente
AUDIO_INTERVAL_MS = 1000  # R√©duit de 3000ms √† 1000ms pour une r√©ponse plus rapide

class CustomSTT(agents.stt.STT):
    """Service STT personnalis√© utilisant l'API Whisper avec mode continu"""
    
    def __init__(self):
        super().__init__(
            capabilities=agents.stt.STTCapabilities(
                streaming=True,
                interim_results=True
            )
        )
        logger.info("üé§ CustomSTT initialis√©")
        logger.info("üîç DIAGNOSTIC: M√©thodes requises par agents.stt.STT")
        logger.info(f"üîç DIAGNOSTIC: Type de la classe parente: {type(agents.stt.STT)}")
        
        # V√©rifier les m√©thodes abstraites
        try:
            import inspect
            abstract_methods = getattr(agents.stt.STT, '__abstractmethods__', set())
            logger.info(f"üîç DIAGNOSTIC: M√©thodes abstraites trouv√©es: {abstract_methods}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è DIAGNOSTIC: Impossible de lister les m√©thodes abstraites: {e}")
    
    async def recognize(
        self,
        *,
        buffer: "agents.utils.AudioBuffer",
        language: Optional[str] = None,
    ) -> agents.stt.SpeechEvent:
        """Ne devrait pas √™tre appel√© dans ce contexte"""
        logger.warning("‚ö†Ô∏è CustomSTT.recognize() appel√© - redirection vers _recognize_impl")
        return await self._recognize_impl(buffer=buffer, language=language)
    
    async def _recognize_impl(
        self,
        *,
        buffer: "agents.utils.AudioBuffer",
        language: Optional[str] = None,
    ) -> agents.stt.SpeechEvent:
        """Impl√©mentation de la m√©thode abstraite requise"""
        logger.info("üé§ DIAGNOSTIC: CustomSTT._recognize_impl() appel√©")
        
        # Cette m√©thode ne devrait pas √™tre utilis√©e dans notre contexte de streaming
        # mais elle est requise par la classe abstraite
        logger.warning("‚ö†Ô∏è _recognize_impl appel√© mais nous utilisons stream()")
        
        # Retourner un √©v√©nement vide pour satisfaire l'interface
        return agents.stt.SpeechEvent(
            type=agents.stt.SpeechEventType.END_OF_SPEECH
        )
    
    def stream(
        self,
        *,
        language: Optional[str] = None,
        conn_options: Optional[Any] = None,
    ) -> "agents.stt.SpeechStream":
        """Cr√©er un flux de reconnaissance vocale"""
        logger.info("üé§ CustomSTT: Cr√©ation d'un nouveau SpeechStream")
        # Ignorer conn_options pour notre impl√©mentation
        return CustomSpeechStream(language=language or "fr")

class CustomSpeechStream(agents.stt.SpeechStream):
    """Flux personnalis√© qui simule un VAD manuel"""
    
    def __init__(self, *, language: str = "fr", stt: Optional[Any] = None, conn_options: Optional[Any] = None):
        # Cr√©er un objet conn_options par d√©faut si None
        if conn_options is None:
            from types import SimpleNamespace
            conn_options = SimpleNamespace()
            conn_options.max_retry = 3
            conn_options.retry_interval = 1.0
        
        super().__init__(stt=stt, conn_options=conn_options)
        self._language = language
        self._closed = False
        logger.info(f"üé§ CustomSpeechStream initialis√© (langue: {language})")
    
    def push_frame(self, frame: Optional[rtc.AudioFrame]) -> None:
        """Recevoir les frames audio - trait√© par StreamAdapter"""
        if frame is None:
            logger.debug("üîß CustomSpeechStream: Frame None re√ßue (fin de flux)")
            return
        # Les frames sont g√©r√©es par StreamAdapterContext
        logger.debug(f"üîß CustomSpeechStream.push_frame() appel√© - frame: {frame}. Samples: {frame.samples_per_channel}, Rate: {frame.sample_rate}")
        # V√©rifiez ici si le frame contient des donn√©es audio significatives
        if frame.data:
            audio_data_np = np.frombuffer(frame.data, dtype=np.int16)
            if audio_data_np.size > 0:
                # Calcul de l'√©nergie (RMS)
                rms = np.sqrt(np.mean(audio_data_np.astype(np.float64)**2))
                logger.debug(f"üîä CustomSpeechStream: RMS de la frame: {rms:.2f}")
                if rms < 100: # Un seuil arbitraire pour "silence" √† revoir
                    logger.debug("‚ö†Ô∏è CustomSpeechStream: Frame potentiellement silencieuse ou tr√®s faible.")
            else:
                logger.debug("‚ö†Ô∏è CustomSpeechStream: Frame audio data vide (apr√®s conversion np).")
        else:
            logger.debug("‚ö†Ô∏è CustomSpeechStream: Frame audio data est None ou vide.")
    
    async def _run(self) -> None:
        """M√©thode abstraite requise par SpeechStream dans LiveKit v1.x"""
        logger.info("üé§ CustomSpeechStream._run() d√©marr√©")
        try:
            # Attendre que le flux soit ferm√©
            while not self._closed:
                await asyncio.sleep(0.1)
        except asyncio.CancelledError:
            logger.info("üé§ CustomSpeechStream._run() annul√©")
        finally:
            logger.info("üé§ CustomSpeechStream._run() termin√©")
    
    async def aclose(self, *, wait: bool = True) -> None:
        """Fermer le flux"""
        self._closed = True
        logger.info("üé§ CustomSpeechStream ferm√©")


class StreamAdapterContext:
    """Contexte pour adapter le flux STT"""
    
    def __init__(self, stt_stream: agents.stt.SpeechStream, room=None, llm_service=None, tts_service=None, audio_source=None, audio_track=None):
        self.stt_stream = stt_stream
        self._audio_buffer: List[np.ndarray] = []
        self._task: Optional[asyncio.Task] = None
        self._closed = False
        self._frame_count = 0
        self._last_process_time = time.time()
        self._http_session: Optional[aiohttp.ClientSession] = None
        
        # Services pour le pipeline complet
        self.room = room
        self.llm_service = llm_service
        self.tts_service = tts_service
        
        # OPTIMISATION M√âMOIRE: Objets audio persistants pour √©viter les fuites
        self.audio_source = audio_source
        self.audio_track = audio_track
        self._publication = None  # Publication persistante
        
        # Configuration du traitement
        self.process_interval = AUDIO_INTERVAL_MS / 1000.0  # 1 seconde maintenant
        self.frames_per_chunk = 200  # Augment√© de 100 √† 200 pour plus d'audio
        
        # Statistiques du pipeline
        self.pipeline_stats = {
            'stt_calls': 0,
            'stt_success': 0,
            'llm_calls': 0,
            'llm_success': 0,
            'tts_calls': 0,
            'tts_success': 0,
            'audio_published': 0
        }
        
        logger.info(f"üîß StreamAdapterContext initialis√© (interval: {self.process_interval}s)")
        logger.info(f"üîß Pipeline configur√©: STT‚ÜíLLM‚ÜíTTS‚ÜíLiveKit")
        logger.info(f"üîß OPTIMISATION: Audio objects persistants: source={self.audio_source is not None}, track={self.audio_track is not None}")
    
    async def __aenter__(self):
        """D√©marrer le contexte et la t√¢che de traitement"""
        logger.info("üîß StreamAdapterContext.__aenter__()")
        self._http_session = aiohttp.ClientSession()
        register_http_session(self._http_session)  # Enregistrer pour fermeture propre
        self._task = asyncio.create_task(self._process_audio_continuous())
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Nettoyer le contexte"""
        logger.info("üîß OPTIMISATION M√âMOIRE: StreamAdapterContext.__aexit__()")
        self._closed = True
        
        # OPTIMISATION M√âMOIRE: Nettoyer les objets audio persistants
        await self._cleanup_audio_resources()
        
        # Nettoyer la session HTTP avec v√©rification d'√©tat
        if self._http_session:
            logger.info(f"üßπ [HTTP SESSION CLEANUP] V√©rification de l'√©tat de la session HTTP: {self._http_session}")
            logger.info(f"üßπ [HTTP SESSION CLEANUP] Session ferm√©e: {self._http_session.closed}")
            logger.info(f"üßπ [HTTP SESSION CLEANUP] Connecteur: {getattr(self._http_session, '_connector', 'N/A')}")
            
            # D√©senregistrer du registre global
            unregister_http_session(self._http_session)
            
            if not self._http_session.closed:
                logger.info("üßπ [HTTP SESSION CLEANUP] Fermeture de la session HTTP active...")
                try:
                    await self._http_session.close()
                    logger.info("‚úÖ [HTTP SESSION CLEANUP] Session HTTP ferm√©e avec succ√®s")
                except Exception as close_error:
                    logger.error(f"‚ùå [HTTP SESSION CLEANUP] Erreur lors de la fermeture: {close_error}")
            else:
                logger.info("‚ÑπÔ∏è [HTTP SESSION CLEANUP] Session HTTP d√©j√† ferm√©e - pas d'action n√©cessaire")
        
        # Annuler la t√¢che
        if self._task and not self._task.done():
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
    
    async def _cleanup_audio_resources(self):
        """Nettoyer les ressources audio persistantes pour √©viter les fuites m√©moire"""
        try:
            logger.info("üßπ [OPTIMISATION M√âMOIRE] Nettoyage des ressources audio...")
            
            # D√©publier le track si n√©cessaire
            if self._publication and self.room and self.room.local_participant:
                try:
                    logger.info("üßπ [OPTIMISATION M√âMOIRE] D√©publication du track persistant...")
                    await self.room.local_participant.unpublish_track(self._publication.sid)
                    logger.info("‚úÖ [OPTIMISATION M√âMOIRE] Track d√©publi√© avec succ√®s")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è [OPTIMISATION M√âMOIRE] Erreur lors de la d√©publication: {e}")
            
            # Nettoyer les r√©f√©rences
            if self.audio_source:
                logger.info("üßπ [OPTIMISATION M√âMOIRE] Nettoyage AudioSource...")
                self.audio_source = None
            
            if self.audio_track:
                logger.info("üßπ [OPTIMISATION M√âMOIRE] Nettoyage LocalAudioTrack...")
                self.audio_track = None
            
            if self._publication:
                logger.info("üßπ [OPTIMISATION M√âMOIRE] Nettoyage Publication...")
                self._publication = None
            
            logger.info("‚úÖ [OPTIMISATION M√âMOIRE] Ressources audio nettoy√©es")
            
        except Exception as e:
            logger.error(f"‚ùå [OPTIMISATION M√âMOIRE] Erreur lors du nettoyage des ressources audio: {e}", exc_info=True)
    
    def push_frame(self, frame: rtc.AudioFrame) -> None:
        """Ajouter une frame au buffer"""
        if self._closed:
            return
        
        # Log d√©taill√© toutes les 10 frames pour √©viter le spam
        if self._frame_count % 10 == 0:
            logger.info(f"üîß [AUDIO FLOW] Frame #{self._frame_count}: {frame.samples_per_channel} samples @ {frame.sample_rate}Hz")
            logger.info(f"üîß [AUDIO FLOW] Buffer actuel: {len(self._audio_buffer)} frames accumul√©es")
        
        # V√©rifier le taux d'√©chantillonnage
        if frame.sample_rate != 48000:
            logger.warning(f"‚ö†Ô∏è [AUDIO FLOW] Taux d'√©chantillonnage inattendu: {frame.sample_rate}Hz (attendu: 48000Hz)")
        
        # Convertir la frame en numpy array
        # Normalise les donn√©es audio de int16 (-32768 √† 32767) √† float32 (-1.0 √† 1.0)
        audio_data = np.frombuffer(frame.data, dtype=np.int16).astype(np.float32) / 32768.0
        
        # V√©rifier si l'audio contient du signal
        if self._frame_count % 50 == 0:  # V√©rifier toutes les 50 frames
            energy = np.sqrt(np.mean(audio_data**2))
            logger.info(f"üîä [AUDIO FLOW] √ânergie audio frame #{self._frame_count}: {energy:.6f}")
        
        self._audio_buffer.append(audio_data)
        self._frame_count += 1
    
    async def _process_audio_continuous(self):
        """Traiter l'audio de mani√®re continue toutes les X secondes"""
        logger.info("üîß CORRECTION: D√©marrage du traitement audio CONTINU")
        
        while not self._closed:
            try:
                # Attendre l'intervalle configur√©
                await asyncio.sleep(self.process_interval)
                
                # CORRECTION: R√©duire le seuil et forcer le traitement pour tester
                if len(self._audio_buffer) >= 200:  # Augment√© √† 200 frames (4 secondes d'audio)
                    # Prendre les frames disponibles
                    frames_to_process = min(len(self._audio_buffer), self.frames_per_chunk)
                    audio_chunk = np.concatenate(self._audio_buffer[:frames_to_process])
                    
                    # Vider le buffer des frames trait√©es
                    self._audio_buffer = self._audio_buffer[frames_to_process:]
                    
                    logger.info(f"üîß CORRECTION: Traitement FORC√â de {frames_to_process} frames audio")
                    
                    # Calculer l'√©nergie pour diagnostic
                    energy = float(np.sqrt(np.mean(audio_chunk**2)))
                    min_val = np.min(audio_chunk)
                    max_val = np.max(audio_chunk)
                    mean_val = np.mean(audio_chunk)
                    
                    logger.info(f"üîä DIAGNOSTIC AUDIO: √©nergie={energy:.8f}, min={min_val:.8f}, max={max_val:.8f}, mean={mean_val:.8f}")
                    
                    # FORCER le traitement m√™me avec audio silencieux pour tester le pipeline
                    logger.info("üîß CORRECTION: FOR√áAGE du traitement STT‚ÜíLLM‚ÜíTTS (test pipeline)")
                    
                    # Toujours √©mettre START_OF_SPEECH
                    await self._emit_event(agents.stt.SpeechEvent(
                        type=agents.stt.SpeechEventType.START_OF_SPEECH
                    ))
                    logger.info("üîß CORRECTION: START_OF_SPEECH √©mis (traitement forc√©)")
                    
                    # TOUJOURS traiter l'audio pour tester le pipeline complet
                    await self._process_chunk_with_whisper(audio_chunk)
                    
                    # √âmettre END_OF_SPEECH
                    await self._emit_event(agents.stt.SpeechEvent(
                        type=agents.stt.SpeechEventType.END_OF_SPEECH
                    ))
                    logger.info("üîß CORRECTION: END_OF_SPEECH √©mis")
                else:
                    logger.debug(f"üîß CORRECTION: Accumulation en cours ({len(self._audio_buffer)}/200 frames)")
                    
            except asyncio.CancelledError:
                logger.info("üîß CORRECTION: StreamAdapter annul√©")
                break
            except Exception as e:
                logger.error(f"‚ùå CORRECTION: Erreur dans _process_audio_continuous: {e}", exc_info=True)
                await asyncio.sleep(0.1)
        
        logger.info("üîß CORRECTION: StreamAdapter termin√© avec succ√®s")
    
    async def _process_chunk_with_whisper(self, audio_chunk: np.ndarray):
        """Traiter un chunk audio avec Whisper et continuer le pipeline complet"""
        try:
            logger.info("=" * 80)
            logger.info("üé§ [PIPELINE] D√âBUT DU TRAITEMENT COMPLET STT‚ÜíLLM‚ÜíTTS")
            logger.info("=" * 80)
            
            # 1. DIAGNOSTIC STT
            logger.info("üé§ [DIAGNOSTIC PIPELINE] √âtape 1: STT")
            self.pipeline_stats['stt_calls'] += 1
            
            # Calculer l'√©nergie audio pour debug
            energy = int(np.sqrt(np.mean(audio_chunk**2)) * 32768)
            logger.info(f"üé§ [DIAGNOSTIC STT] Audio stats: {len(audio_chunk)} √©chantillons, √©nergie: {energy}")
            
            # Convertir en int16 pour Whisper
            audio_int16 = (audio_chunk * 32768).astype(np.int16)
            
            # Cr√©er un fichier WAV temporaire
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_file:
                with wave.open(tmp_file.name, 'wb') as wav_file:
                    wav_file.setnchannels(1)
                    wav_file.setsampwidth(2)
                    wav_file.setframerate(24000)
                    wav_file.writeframes(audio_int16.tobytes())
                
                # Lire le fichier WAV pour l'envoyer √† Whisper
                with open(tmp_file.name, 'rb') as audio_file:
                    wav_data = audio_file.read()
                
                # Appeler le service Whisper local
                whisper_url = "http://whisper-stt:8001/transcribe"
                logger.info(f"üé§ [STT] Envoi √† Whisper: {len(wav_data)} bytes")
                
                # Utiliser la session HTTP existante
                if not self._http_session:
                    logger.error("‚ùå [STT] Pas de session HTTP disponible")
                    return
                
                # Pr√©parer la requ√™te multipart
                form = aiohttp.FormData()
                form.add_field('audio', wav_data, filename='audio.wav', content_type='audio/wav')
                form.add_field('language', 'fr')
                form.add_field('model', 'whisper-large-v3-turbo')
                
                # Envoyer √† Whisper
                transcription = ""
                try:
                    async with self._http_session.post(
                        whisper_url,
                        data=form,
                        timeout=aiohttp.ClientTimeout(total=30)
                    ) as response:
                        if response.status == 200:
                            data = await response.json()
                            transcription = data.get('text', '').strip()
                            
                            if transcription:
                                self.pipeline_stats['stt_success'] += 1
                                logger.info(f"‚úÖ [STT] Transcription: '{transcription}'")
                                
                                # √âmettre la transcription finale
                                await self._emit_event(agents.stt.SpeechEvent(
                                    type=agents.stt.SpeechEventType.FINAL_TRANSCRIPT,
                                    alternatives=[agents.stt.SpeechData(text=transcription, language="fr")]
                                ))
                            else:
                                logger.warning("‚ö†Ô∏è [STT] Transcription vide")
                                return
                        else:
                            error_text = await response.text()
                            logger.error(f"‚ùå [STT] Erreur Whisper {response.status}: {error_text}")
                            return
                            
                except (aiohttp.ClientError, asyncio.TimeoutError) as e:
                    logger.error(f"‚ùå [STT] Erreur r√©seau ou Timeout Whisper: {e}")
                    return # Pas de transcription en cas d'erreur r√©seau
                except Exception as e:
                    logger.error(f"‚ùå [STT] Erreur inattendue lors de la transcription Whisper: {e}", exc_info=True)
                    return
                finally:
                    # Nettoyer
                    if os.path.exists(tmp_file.name):
                        os.unlink(tmp_file.name)
                
                # 2. DIAGNOSTIC LLM
                try: # Ajout du try-except autour du bloc LLM
                    if transcription and self.llm_service:  # Conserver la v√©rification du service LLM
                        logger.info("üß† [DIAGNOSTIC PIPELINE] √âtape 2: LLM")
                        self.pipeline_stats['llm_calls'] += 1
                        
                        llm_response = await self._call_llm_service(transcription)
                        
                        if llm_response:
                            self.pipeline_stats['llm_success'] += 1
                            logger.info(f"‚úÖ [DIAGNOSTIC LLM] R√©ponse: '{llm_response[:100]}...'")
                            
                            # 3. DIAGNOSTIC TTS
                            try: # Ajout du try-except autour du bloc TTS
                                if self.tts_service:
                                    logger.info("üîä [DIAGNOSTIC PIPELINE] √âtape 3: TTS")
                                    self.pipeline_stats['tts_calls'] += 1
                                    
                                    tts_audio = await self._call_tts_service(llm_response)
                                    
                                    if tts_audio and len(tts_audio) > 1000:
                                        self.pipeline_stats['tts_success'] += 1
                                        logger.info(f"‚úÖ [DIAGNOSTIC TTS] Audio synth√©tis√©: {len(tts_audio)} bytes")
                                        
                                        # 4. DIAGNOSTIC DIFFUSION
                                        try: # Ajout du try-except autour du bloc de diffusion
                                            logger.info("üì° [DIAGNOSTIC PIPELINE] √âtape 4: Diffusion")
                                            await self._stream_tts_audio(tts_audio)
                                            self.pipeline_stats['audio_published'] += 1
                                            logger.info("‚úÖ [DIAGNOSTIC DIFFUSION] Audio publi√© sur LiveKit")
                                        except Exception as e:
                                            logger.error(f"‚ùå [DIFFUSION] Erreur lors de la diffusion de l'audio: {e}", exc_info=True)
                                    else:
                                        logger.error(f"‚ùå [DIAGNOSTIC TTS] Audio vide ou trop petit. Taille: {len(tts_audio) if tts_audio else 0}")
                                else:
                                    logger.warning("‚ö†Ô∏è [DIAGNOSTIC TTS] Service TTS non configur√©")
                            except Exception as e:
                                logger.error(f"‚ùå [TTS] Erreur lors de l'appel au service TTS: {e}", exc_info=True)
                        else:
                            logger.error("‚ùå [DIAGNOSTIC LLM] Aucune r√©ponse g√©n√©r√©e")
                    else:
                        logger.warning("‚ö†Ô∏è [DIAGNOSTIC LLM] Service LLM non configur√© ou transcription vide")
                except Exception as e:
                    logger.error(f"‚ùå [LLM] Erreur lors de l'appel au service LLM: {e}", exc_info=True)
                
                # Afficher les statistiques
                logger.info("=" * 80)
                logger.info("üìä [PIPELINE] Statistiques:")
                logger.info(f"  - STT: {self.pipeline_stats['stt_success']}/{self.pipeline_stats['stt_calls']}")
                logger.info(f"  - LLM: {self.pipeline_stats['llm_success']}/{self.pipeline_stats['llm_calls']}")
                logger.info(f"  - TTS: {self.pipeline_stats['tts_success']}/{self.pipeline_stats['tts_calls']}")
                logger.info(f"  - Audio publi√©: {self.pipeline_stats['audio_published']}")
                logger.info("=" * 80)
                
        except Exception as e:
            logger.error(f"‚ùå [PIPELINE] Erreur g√©n√©rale dans le traitement du chunk: {e}", exc_info=True)
    
    async def _call_llm_service(self, transcription: str) -> str:
        """Appeler le service LLM en utilisant l'API LiveKit Agents v1.x correctement."""
        try:
            if not self.llm_service:
                logger.error("‚ùå [LLM] Service non configur√©")
                return "Service vocal non disponible."
    
            logger.info("üß† [LLM] Utilisation de l'API LiveKit Agents pour le chat")
    
            # 1. Cr√©er un ChatContext compatible avec le format liste requis
            chat_ctx = ChatContext()
            
            # CORRECTION: Cr√©er les messages avec le format LISTE requis par Pydantic validation
            system_msg = ChatMessage(role="system", content=["Tu es un coach vocal expert. Sois concis et encourageant."])
            user_msg = ChatMessage(role="user", content=[transcription])
            
            # Ajouter les messages au contexte
            chat_ctx.messages = [system_msg, user_msg]
            logger.info(f"üß† [LLM] ChatContext cr√©√© avec {len(chat_ctx.messages)} messages au format liste.")
    
            # 2. Appeler la m√©thode chat du service LLM
            llm_stream = await self.llm_service.chat(chat_ctx=chat_ctx)
            logger.info("üß† [LLM] Stream de r√©ponse LLM obtenu.")
    
            # CORRECTION: Forcer le d√©marrage du stream avec __aenter__
            logger.info("üß† [LLM] D√©marrage forc√© du stream LLM...")
            async with llm_stream as active_stream:
                logger.info("üß† [LLM] Stream LLM actif, collecte des chunks...")
                
                # 3. Collecter la r√©ponse compl√®te du stream
                response_text = ""
                chunk_count = 0
                async for chunk in active_stream:
                    chunk_count += 1
                    logger.info(f"üß† [LLM] Chunk #{chunk_count} re√ßu (type: {type(chunk)}): {chunk}")
                    try:
                        if isinstance(chunk, dict) and chunk.get('choices'):
                            delta = chunk['choices'][0].get('delta', {})
                            content = delta.get('content')
                            if content:
                                response_text += content
                                logger.info(f"üß† [LLM] Contenu ajout√©: '{content}'")
                        elif hasattr(chunk, 'choices') and chunk.choices:
                            delta = chunk.choices[0].delta
                            if delta and delta.content:
                                response_text += delta.content
                                logger.info(f"üß† [LLM] Contenu ajout√©: '{delta.content}'")
                    except (KeyError, IndexError, AttributeError) as e:
                        logger.warning(f"‚ö†Ô∏è [LLM] Erreur de parsing du chunk: {e} - chunk: {chunk}")
                
                logger.info(f"üß† [LLM] Total chunks re√ßus: {chunk_count}")
                
                if response_text:
                    logger.info(f"‚úÖ [LLM] R√©ponse compl√®te: '{response_text[:100]}...'")
                    return response_text
                else:
                    logger.error("‚ùå [LLM] Le stream n'a produit aucun contenu.")
                    return "Je n'ai pas de r√©ponse pour le moment."
    
        except Exception as e:
            logger.error(f"‚ùå [LLM] Erreur critique dans _call_llm_service: {e}", exc_info=True)
            return "Une erreur interne est survenue dans le service de langage."
    
    async def _call_tts_service(self, text: str) -> bytes:
        """Appeler le service TTS - VERSION DIAGNOSTIC"""
        try:
            logger.info("üîä [DIAGNOSTIC TTS SERVICE] D√©but appel TTS")
            
            if not self.tts_service:
                logger.error("‚ùå [DIAGNOSTIC TTS SERVICE] Service non configur√©")
                return b""
            
            logger.info(f"üîä [DIAGNOSTIC TTS SERVICE] Synth√®se du texte: '{text[:50]}...'")
            
            # Synth√©tiser l'audio
            stream = await self.tts_service.synthesize(text)
            logger.info("üîä [DIAGNOSTIC TTS SERVICE] Stream TTS cr√©√©")
            
            # Collecter les chunks audio
            audio_chunks = []
            chunk_count = 0
            total_bytes = 0
            
            async for chunk in stream:
                if hasattr(chunk, 'data'):
                    chunk_size = len(chunk.data)
                    audio_chunks.append(chunk.data)
                    chunk_count += 1
                    total_bytes += chunk_size
                    
                    if chunk_count <= 3:
                        logger.info(f"üîä [DIAGNOSTIC TTS SERVICE] Chunk {chunk_count}: {chunk_size} bytes")
            
            logger.info(f"üîä [DIAGNOSTIC TTS SERVICE] Total: {chunk_count} chunks, {total_bytes} bytes")
            
            # Combiner tous les chunks
            if audio_chunks:
                combined_audio = b''.join(audio_chunks)
                logger.info(f"‚úÖ [DIAGNOSTIC TTS SERVICE] Audio PCM combin√©: {len(combined_audio)} bytes")
                
                # L'audio est d√©j√† en PCM gr√¢ce √† response_format="pcm" !
                logger.info("‚úÖ [DIAGNOSTIC TTS SERVICE] Audio d√©j√† en PCM 16-bit 24kHz - pr√™t pour LiveKit!")
                return combined_audio
            else:
                logger.warning("‚ö†Ô∏è [DIAGNOSTIC TTS SERVICE] Aucune donn√©e audio re√ßue")
                return b""
                
        except (aiohttp.ClientError, asyncio.TimeoutError) as e:
            logger.error(f"‚ùå [DIAGNOSTIC TTS SERVICE] Erreur r√©seau/timeout: {e}")
            return b""
        except Exception as e:
            logger.error(f"‚ùå [DIAGNOSTIC TTS SERVICE] Erreur inattendue: {e}", exc_info=True)
            return b""
    
    async def _stream_tts_audio(self, audio_data: bytes):
        """Diffuser l'audio TTS vers LiveKit - VERSION OPTIMIS√âE M√âMOIRE"""
        try:
            logger.info("üîä [OPTIMISATION M√âMOIRE] D√âBUT DE LA DIFFUSION AUDIO")
            logger.info(f"üîä [OPTIMISATION M√âMOIRE] Taille des donn√©es: {len(audio_data)} bytes")
            
            if not self.room:
                logger.error("‚ùå [OPTIMISATION M√âMOIRE] Room non configur√©e - IMPOSSIBLE DE DIFFUSER")
                return
            
            # Diagnostic de la room
            logger.info(f"üîä [OPTIMISATION M√âMOIRE] Room configur√©e: {type(self.room)}")
            logger.info(f"üîä [OPTIMISATION M√âMOIRE] Local participant: {self.room.local_participant}")
            
            # OPTIMISATION M√âMOIRE: Utiliser les objets audio persistants ou les cr√©er une seule fois
            if self.audio_source is None or self.audio_track is None:
                logger.info("üîß [OPTIMISATION M√âMOIRE] Cr√©ation des objets audio persistants (premi√®re fois)")
                
                # Cr√©er une source audio persistante
                self.audio_source = rtc.AudioSource(sample_rate=48000, num_channels=1)
                logger.info(f"üîß [OPTIMISATION M√âMOIRE] AudioSource persistante cr√©√©e: {self.audio_source}")
                
                # Cr√©er un track audio local persistant
                self.audio_track = rtc.LocalAudioTrack.create_audio_track(
                    "eloquence_tts_response",
                    self.audio_source
                )
                logger.info(f"üîß [OPTIMISATION M√âMOIRE] LocalAudioTrack persistant cr√©√©: {self.audio_track}")
                
                # Options de publication
                publish_options = rtc.TrackPublishOptions(
                    source=rtc.TrackSource.SOURCE_MICROPHONE,
                    dtx=False,
                    red=True
                )
                logger.info(f"üîß [OPTIMISATION M√âMOIRE] Options de publication: source=MICROPHONE, stereo=False, dtx=False, red=True")
                
                # Publier le track une seule fois
                logger.info("üîß [OPTIMISATION M√âMOIRE] Publication du track persistant...")
                self._publication = await self.room.local_participant.publish_track(
                    self.audio_track,
                    publish_options
                )
                
                logger.info(f"üì° [OPTIMISATION M√âMOIRE] Track persistant publi√© avec succ√®s!")
                logger.info(f"üì° [OPTIMISATION M√âMOIRE] Publication SID: {self._publication.sid}")
                logger.info(f"üì° [OPTIMISATION M√âMOIRE] Publication kind: {self._publication.kind if hasattr(self._publication, 'kind') else 'N/A'}")
                logger.info(f"üì° [OPTIMISATION M√âMOIRE] Publication muted: {self._publication.muted if hasattr(self._publication, 'muted') else 'N/A'}")
            else:
                logger.info("‚úÖ [OPTIMISATION M√âMOIRE] R√©utilisation des objets audio persistants")
                logger.info(f"‚úÖ [OPTIMISATION M√âMOIRE] AudioSource r√©utilis√©e: {self.audio_source}")
                logger.info(f"‚úÖ [OPTIMISATION M√âMOIRE] LocalAudioTrack r√©utilis√©: {self.audio_track}")
            
            # Convertir les donn√©es audio en frames
            logger.info("üîä [DIAGNOSTIC AUDIO] Conversion et (si n√©cessaire) r√©√©chantillonnage des donn√©es audio...")
            
            # V√©rifier le format des donn√©es
            if len(audio_data) < 100:
                logger.error(f"‚ùå [DIAGNOSTIC AUDIO] Donn√©es audio trop petites: {len(audio_data)} bytes")
                return
            
            # Analyser les premi√®res donn√©es
            first_bytes = audio_data[:20]
            logger.info(f"üîä [DIAGNOSTIC AUDIO] Premiers bytes (RAW): {first_bytes.hex()}")
            
            # Convertir en numpy array int16
            try:
                audio_array_int16 = np.frombuffer(audio_data, dtype=np.int16)
                logger.info(f"üîä [DIAGNOSTIC AUDIO] Array audio int16 cr√©√©: shape={audio_array_int16.shape}, dtype={audio_array_int16.dtype}")
                
                # Statistiques audio
                audio_min_int16 = np.min(audio_array_int16)
                audio_max_int16 = np.max(audio_array_int16)
                audio_mean_int16 = np.mean(audio_array_int16)
                audio_std_int16 = np.std(audio_array_int16)
                logger.info(f"üîä [DIAGNOSTIC AUDIO] Stats RAW int16: min={audio_min_int16}, max={audio_max_int16}, mean={audio_mean_int16:.2f}, std={audio_std_int16:.2f}")
                
                # V√©rifier si l'audio n'est pas silencieux (bas√© sur l'√©cart-type)
                if audio_std_int16 < 50: # Seuil si les valeurs sont tr√®s proches de z√©ro
                    logger.warning("‚ö†Ô∏è [DIAGNOSTIC AUDIO] L'audio RAW semble √™tre silencieux (std tr√®s faible).")
                
                # R√©√©chantillonnage de 24kHz √† 48kHz
                original_sample_rate = 24000
                target_sample_rate = 48000
                
                if original_sample_rate != target_sample_rate:
                    logger.info(f"üîä [DIAGNOSTIC AUDIO] R√©√©chantillonnage de {original_sample_rate}Hz √† {target_sample_rate}Hz...")
                    # Passer en float pour le r√©√©chantillonnage
                    audio_array_float = audio_array_int16.astype(np.float32)
                    num_samples_resampled = int(len(audio_array_float) * (target_sample_rate / original_sample_rate))
                    audio_resampled_float = resample(audio_array_float, num_samples_resampled)
                    
                    # Revenir en int16 et normaliser
                    audio_resampled_int16 = (audio_resampled_float * (2**15 - 1) / np.max(np.abs(audio_resampled_float))).astype(np.int16)
                    logger.info(f"‚úÖ [DIAGNOSTIC AUDIO] Audio r√©√©chantillonn√©: shape={audio_resampled_int16.shape}, dtype={audio_resampled_int16.dtype}")
                    audio_to_stream = audio_resampled_int16
                    current_sample_rate = target_sample_rate
                else:
                    audio_to_stream = audio_array_int16
                    current_sample_rate = original_sample_rate
                
                # V√©rifier l'audio apr√®s r√©√©chantillonnage
                audio_stream_min_int16 = np.min(audio_to_stream)
                audio_stream_max_int16 = np.max(audio_to_stream)
                audio_stream_mean_int16 = np.mean(audio_to_stream)
                audio_stream_std_int16 = np.std(audio_to_stream)
                logger.info(f"üîä [DIAGNOSTIC AUDIO] Stats STREAM int16: min={audio_stream_min_int16}, max={audio_stream_max_int16}, mean={audio_stream_mean_int16:.2f}, std={audio_stream_std_int16:.2f}")

                if audio_stream_std_int16 < 50:
                    logger.warning("‚ö†Ô∏è [DIAGNOSTIC AUDIO] L'audio stream√© semble √™tre silencieux apr√®s r√©√©chantillonnage.")

            except Exception as conv_error:
                logger.error(f"‚ùå [DIAGNOSTIC AUDIO] Erreur conversion ou r√©√©chantillonnage numpy: {conv_error}", exc_info=True)
                return
            
            # Envoyer par chunks
            # La taille de chunk recommand√©e pour LiveKit est 10ms d'audio
            # √Ä 48kHz, 10ms = 480 √©chantillons
            chunk_size_samples = int(current_sample_rate * 0.01) # 10ms de samples
            
            total_chunks = len(audio_to_stream) // chunk_size_samples
            logger.info(f"üîä [DIAGNOSTIC AUDIO] Envoi de {total_chunks} chunks de {chunk_size_samples} samples √† {current_sample_rate}Hz")
            
            chunks_sent = 0
            for i in range(0, len(audio_to_stream), chunk_size_samples):
                chunk = audio_to_stream[i:i+chunk_size_samples]
                
                # Padding si le dernier chunk est plus petit
                if len(chunk) < chunk_size_samples:
                    chunk = np.pad(chunk, (0, chunk_size_samples - len(chunk)), 'constant', constant_values=0)
                
                chunk_bytes = chunk.tobytes()
                
                # Cr√©er et envoyer la frame
                frame = rtc.AudioFrame(
                    data=chunk_bytes,
                    sample_rate=current_sample_rate,
                    num_channels=1,
                    samples_per_channel=chunk_size_samples
                )
                
                # Log d√©taill√© pour les premiers chunks
                if chunks_sent < 5:
                    logger.info(f"üîä [OPTIMISATION M√âMOIRE] Envoi chunk {chunks_sent}: {len(chunk_bytes)} bytes, SR={current_sample_rate}, SamplesPC={chunk_size_samples}")
                
                # OPTIMISATION M√âMOIRE: Utiliser l'AudioSource persistant
                await self.audio_source.capture_frame(frame)
                chunks_sent += 1
                
                # Petit d√©lai pour √©viter la surcharge
                # Ce d√©lai est crucial pour un streaming fluide et √©viter le blocage
                await asyncio.sleep(chunk_size_samples / current_sample_rate) # Dormir exact. la dur√©e du chunk
            
            logger.info(f"‚úÖ [OPTIMISATION M√âMOIRE] Audio compl√®tement diffus√©: {chunks_sent} chunks envoy√©s")
            
            # V√©rifier l'√©tat final avec la publication persistante
            if self._publication and hasattr(self._publication, 'track'):
                track_state = getattr(self._publication.track, 'state', 'unknown')
                logger.info(f"üîä [OPTIMISATION M√âMOIRE] √âtat final du track persistant: {track_state}")
            
            # Garder le track actif un peu plus longtemps pour s'assurer que le client a le temps de le jouer
            logger.info("üîä [OPTIMISATION M√âMOIRE] Maintien du track persistant actif pendant 1 seconde apr√®s diffusion...")
            await asyncio.sleep(1.0)
            
            logger.info("‚úÖ [OPTIMISATION M√âMOIRE] Diffusion termin√©e avec succ√®s - objets audio r√©utilisables")
            
        except Exception as e:
            logger.error(f"‚ùå [DIAGNOSTIC AUDIO] ERREUR CRITIQUE lors de la diffusion: {e}", exc_info=True)
            logger.error(f"‚ùå [DIAGNOSTIC AUDIO] Type d'erreur: {type(e).__name__}")
            logger.error(f"‚ùå [DIAGNOSTIC AUDIO] Message d'erreur: {str(e)}")
    
    async def _emit_event(self, event: agents.stt.SpeechEvent):
        """√âmettre un √©v√©nement sur le flux STT - CORRECTION LIVEKIT V1.X"""
        try:
            # CORRECTION: Dans LiveKit v1.x, les √©v√©nements STT sont g√©r√©s diff√©remment
            # Nous devons cr√©er notre propre syst√®me d'√©v√©nements ou les ignorer
            
            if hasattr(self.stt_stream, '_event_queue') and self.stt_stream._event_queue:
                await self.stt_stream._event_queue.put(event)
                logger.debug(f"‚úÖ CORRECTION: √âv√©nement √©mis via _event_queue: {event.type}")
            elif hasattr(self.stt_stream, 'emit') and callable(self.stt_stream.emit):
                # Essayer la m√©thode emit si disponible
                try:
                    await self.stt_stream.emit(event)
                    logger.debug(f"‚úÖ CORRECTION: √âv√©nement √©mis via emit(): {event.type}")
                except Exception as emit_error:
                    logger.debug(f"‚ö†Ô∏è CORRECTION: emit() √©chou√©: {emit_error}")
            else:
                # Dans LiveKit v1.x, les √©v√©nements STT peuvent √™tre optionnels
                # Log en debug au lieu de warning pour r√©duire le bruit
                logger.debug(f"üîß CORRECTION: √âv√©nement STT ignor√© (pas de queue): {event.type}")
                
                # Cr√©er une queue temporaire si n√©cessaire
                if not hasattr(self.stt_stream, '_event_queue'):
                    self.stt_stream._event_queue = asyncio.Queue()
                    logger.info("üîß CORRECTION: Event queue cr√©√©e pour STT stream")
                    await self.stt_stream._event_queue.put(event)
                    
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è CORRECTION: Erreur √©mission √©v√©nement (non-critique): {e}")
            

class CustomLLM(agents.llm.LLM):
    """Service LLM personnalis√© utilisant l'API Mistral"""
    
    def __init__(self, session: aiohttp.ClientSession):
        super().__init__()
        self._session = session
        logger.info("üß† CustomLLM initialis√© avec une session HTTP partag√©e")
    
    async def chat(
        self,
        *,
        chat_ctx: agents.llm.ChatContext,
        fnc_ctx: Optional[Any] = None,
        temperature: Optional[float] = None,
        n: Optional[int] = None,
        parallel_tool_calls: Optional[bool] = None,
    ) -> "agents.llm.LLMStream":
        """Cr√©er un flux de chat avec le LLM."""
        logger.info("üß† CustomLLM.chat() appel√©")

        # Simplification: extraire directement les messages du contexte
        # LiveKit Agents v1.1.3 utilise chat_ctx.messages pour l'acc√®s aux messages
        messages_from_ctx = []
        if hasattr(chat_ctx, 'messages'):
            for msg in chat_ctx.messages:
                # V√©rifier si msg.content est une cha√Æne ou une liste de dictionnaires
                if isinstance(msg.content, str):
                    # Si c'est une cha√Æne, la transformer en format [{'type': 'text', 'text': '...'}]
                    processed_content = [{"type": "text", "text": msg.content}]
                elif isinstance(msg.content, list):
                    # Si c'est une liste, v√©rifier si les √©l√©ments sont des dictionnaires valides
                    # Pour simplifier, nous allons supposer que c'est le format correct d√©sir√©
                    processed_content = msg.content
                else:
                    # G√©rer les cas inattendus, par exemple en retournant une cha√Æne vide ou en loggant une erreur
                    logger.warning(f"‚ö†Ô∏è Type de contenu inattendu pour ChatMessage: {type(msg.content)}")
                    processed_content = [{"type": "text", "text": str(msg.content)}] # Convertir en cha√Æne au cas o√π

                messages_from_ctx.append({"role": str(msg.role), "content": processed_content})
            logger.info(f"üß† Messages extraits du contexte: {len(messages_from_ctx)}")
            for i, msg in enumerate(messages_from_ctx):
                logger.info(f"üß† Message {i}: role='{msg.get('role', 'N/A')}', content='{msg.get('content', 'N/A')[:50]}...'")
        else:
            logger.warning("‚ö†Ô∏è ChatContext.messages n'est pas disponible. Utilisation des messages par d√©faut.")
            messages_from_ctx = [
                {"role": "system", "content": [{"type": "text", "text": "Tu es un coach vocal expert. Sois concis et encourageant."}]},
                {"role": "user", "content": [{"type": "text", "text": "Bonjour, je teste le syst√®me vocal."}]} # Message par d√©faut
            ]

        # Prioriser les messages du contexte, sinon utiliser les messages par d√©faut
        messages = messages_from_ctx if messages_from_ctx else [
            {"role": "system", "content": [{"type": "text", "text": "Tu es un coach vocal expert. Sois concis et encourageant."}]},
            {"role": "user", "content": [{"type": "text", "text": "Bonjour"}]}
        ]
        logger.info(f"üß† Messages finaux pour LLM: {len(messages)}")

        return CustomLLMStream(
            llm_instance=self,
            session=self._session,
            messages=messages,
            temperature=temperature or 0.7,
            chat_ctx=chat_ctx,
            tools=[],
            conn_options=self._create_default_conn_options()
        )
    
    def _create_default_conn_options(self):
        """Cr√©er des options de connexion par d√©faut pour √©viter l'erreur NoneType"""
        from types import SimpleNamespace
        conn_options = SimpleNamespace()
        conn_options.max_retry = 3
        conn_options.retry_interval = 1.0
        return conn_options
    
    async def aclose(self):
        """Fermer les ressources (la session est g√©r√©e √† l'ext√©rieur maintenant)"""
        pass

class CustomLLMStream(agents.llm.LLMStream):
    """Stream personnalis√© pour le LLM - Compatible LiveKit v1.x"""
    
    def __init__(self, llm_instance, session: aiohttp.ClientSession, messages: List[Dict[str, str]], temperature: float, chat_ctx, tools=None, conn_options=None):
        super().__init__(llm=llm_instance, chat_ctx=chat_ctx, tools=tools or [], conn_options=conn_options)
        
        self._session = session
        self._messages = messages
        self._temperature = temperature
        self._response_queue = asyncio.Queue()
        self._task: Optional[asyncio.Task] = None
        self._running = False
        self._closed = False
    
    async def __aenter__(self):
        """D√©marrer le stream - CORRECTION v1.x"""
        logger.info("üß† [V1.X] CustomLLMStream.__aenter__() - D√©marrage du stream")
        # CORRECTION v1.x: Forcer le d√©marrage de _run() si n√©cessaire
        self._running = True
        
        # CORRECTION: D√©marrer manuellement _run() car LiveKit v1.x ne le fait pas automatiquement
        logger.info("üß† [V1.X] D√©marrage manuel de _run()...")
        self._task = asyncio.create_task(self._run())
        logger.info("‚úÖ [V1.X] T√¢che _run() cr√©√©e et d√©marr√©e")
        
        # Attendre un peu pour que la connexion s'√©tablisse
        await asyncio.sleep(0.1)
        
        logger.info("‚úÖ [V1.X] Stream marqu√© comme actif et _run() en cours")
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Fermer le stream - CORRECTION v1.x avec protection double fermeture"""
        if hasattr(self, '_closed') and self._closed:
            return  # D√©j√† ferm√©
            
        logger.info("üß† [V1.X] CustomLLMStream.__aexit__() - Fermeture du stream")
        self._running = False
        self._closed = True
        
        # CORRECTION v1.x: Plus de t√¢che _fetch_response √† annuler
        # La m√©thode _run() se termine automatiquement quand _running = False
        if self._task and not self._task.done():
            logger.info("üß† [V1.X] Annulation de la t√¢che existante...")
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                logger.info("üß† [V1.X] T√¢che annul√©e avec succ√®s")
        
        # Vider la queue pour √©viter les blocages
        try:
            while not self._response_queue.empty():
                self._response_queue.get_nowait()
        except:
            pass
        
        logger.info("‚úÖ [V1.X] Stream ferm√©")
    
    # CORRECTION v1.x: _fetch_response() supprim√©e - logique d√©plac√©e dans _run()
    
    async def _run(self):
        """M√©thode abstraite requise par LLMStream dans LiveKit v1.x - CORRECTION COMPL√àTE"""
        logger.info("üß† CustomLLMStream._run() d√©marr√© - IMPL√âMENTATION v1.x")
        self._running = True
        self._task = asyncio.current_task() # Initialiser _task pour le monitoring du SDK
        
        try:
            # CORRECTION v1.x: Cette m√©thode doit g√©rer le flux de donn√©es LLM
            # Elle remplace la logique de __aenter__ et _fetch_response
            
            logger.info("üß† [V1.X] D√©marrage de la r√©cup√©ration des donn√©es LLM...")
            
            headers = {
                "Authorization": f"Bearer {MISTRAL_API_KEY}",
                "Content-Type": "application/json"
            }
            
            data = {
                "model": MISTRAL_MODEL,
                "messages": self._messages,
                "temperature": self._temperature,
                "stream": True
            }
            
            logger.info(f"üß† [V1.X] Appel API Mistral avec {len(self._messages)} messages")
            
            async with self._session.post(
                MISTRAL_BASE_URL,
                headers=headers,
                json=data,
                timeout=aiohttp.ClientTimeout(total=60)
            ) as response:
                if response.status == 200:
                    logger.info("‚úÖ [V1.X] Connexion API Mistral √©tablie")
                    
                    async for line in response.content:
                        if not self._running:
                            logger.info("üß† [V1.X] Stream arr√™t√© par signal externe")
                            break
                            
                        if line:
                            line_str = line.decode('utf-8').strip()
                            if line_str.startswith("data: "):
                                data_str = line_str[6:]
                                if data_str == "[DONE]":
                                    logger.info("‚úÖ [V1.X] Stream termin√© normalement")
                                    break
                                try:
                                    chunk_data = json.loads(data_str)
                                    await self._response_queue.put(chunk_data)
                                    logger.debug(f"üß† [V1.X] Chunk trait√©: {chunk_data.get('choices', [{}])[0].get('delta', {}).get('content', '')[:50]}")
                                except json.JSONDecodeError:
                                    logger.warning(f"‚ö†Ô∏è [V1.X] Impossible de d√©coder: {data_str}")
                else:
                    error_text = await response.text()
                    logger.error(f"‚ùå [V1.X] Erreur Mistral API {response.status}: {error_text}")
                    
        except asyncio.CancelledError:
            logger.info("üß† [V1.X] CustomLLMStream._run() annul√©")
            self._running = False
        except Exception as e:
            logger.error(f"‚ùå [V1.X] Erreur dans _run(): {e}", exc_info=True)
            self._running = False
        finally:
            # Signal de fin pour les consommateurs
            await self._response_queue.put(None)
            logger.info("üß† [V1.X] CustomLLMStream._run() termin√©")
    
    async def __anext__(self):
        """Obtenir le prochain chunk avec protection contre fermeture"""
        if self._closed:
            raise StopAsyncIteration
            
        try:
            chunk = await self._response_queue.get()
            if chunk is None or self._closed:
                raise StopAsyncIteration
            return chunk
        except asyncio.CancelledError:
            raise StopAsyncIteration

class CustomTTS(agents.tts.TTS):
    """Service TTS personnalis√© utilisant OpenAI"""
    
    def __init__(self, session: aiohttp.ClientSession):
        super().__init__(
            capabilities=agents.tts.TTSCapabilities(
                streaming=True
            ),
            sample_rate=24000,
            num_channels=1
        )
        self._session = session
        logger.info("üîä CustomTTS initialis√© avec une session HTTP partag√©e")
    
    async def synthesize(
        self,
        text: str,
        *,
        voice: Optional[str] = None,
    ) -> "agents.tts.ChunkedStream":
        """Synth√©tiser du texte en audio"""
        logger.info(f"üîä CustomTTS.synthesize() appel√© avec: '{text[:50]}...'")
        
        return CustomTTSStream(self._session, text)
    
    async def aclose(self):
        """Fermer les ressources (la session est g√©r√©e √† l'ext√©rieur maintenant)"""
        pass

class CustomTTSStream:
    """Stream personnalis√© pour le TTS OpenAI"""
    
    def __init__(self, session: aiohttp.ClientSession, text: str):
        self._session = session
        self._text = text
        self._audio_queue = asyncio.Queue()
        self._task: Optional[asyncio.Task] = None
        self._closed = False
        self._running = False
    
    async def __aenter__(self):
        """D√©marrer le stream avec protection contre double d√©marrage"""
        if self._running:
            return self
        
        self._running = True
        self._closed = False
        self._task = asyncio.create_task(self._generate_audio())
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Fermer le stream avec protection contre double fermeture"""
        if hasattr(self, '_closed') and self._closed:
            return  # D√©j√† ferm√©
        
        self._closed = True
        
        if self._task and not self._task.done():
            self._task.cancel()
            try:
                await self._task
            except asyncio.CancelledError:
                pass
        
        # Vider la queue pour √©viter les blocages
        try:
            while not self._audio_queue.empty():
                self._audio_queue.get_nowait()
        except:
            pass
    
    async def _generate_audio(self):
        """G√©n√©rer l'audio avec OpenAI TTS - VERSION DIAGNOSTIC"""
        try:
            if self._closed:
                return
                
            logger.info("üéØ [DIAGNOSTIC TTS] D√âBUT G√âN√âRATION AUDIO OPENAI")
            logger.info(f"üéØ [DIAGNOSTIC TTS] Texte √† synth√©tiser: '{self._text[:100]}...'")
            logger.info(f"üéØ [DIAGNOSTIC TTS] Longueur du texte: {len(self._text)} caract√®res")
            
            # V√©rifier la cl√© API
            if not OPENAI_API_KEY:
                logger.error("‚ùå [DIAGNOSTIC TTS] OPENAI_API_KEY non configur√©e!")
                return
            
            logger.info(f"üéØ [DIAGNOSTIC TTS] Cl√© API OpenAI: {OPENAI_API_KEY[:10]}...")
            
            headers = {
                "Authorization": f"Bearer {OPENAI_API_KEY}",
                "Content-Type": "application/json"
            }
            
            # CORRECTION: Adapter le format pour le service TTS local
            data = {
                "text": self._text,
                "voice": "alloy"
            }
            
            logger.info(f"üéØ [DIAGNOSTIC TTS] Param√®tres: text='{self._text[:50]}...', voice=alloy")
            logger.info("üéØ [DIAGNOSTIC TTS] Envoi de la requ√™te au service TTS local...")
            
            # CORRECTION: Utiliser le service TTS local au lieu de l'API OpenAI externe
            tts_url = "http://openai-tts:5002/api/tts"
            logger.info(f"üéØ [DIAGNOSTIC TTS] CORRECTION: Appel du service TTS local: {tts_url}")

            async with self._session.post(
                tts_url,
                headers={"Content-Type": "application/json"},
                json=data,
                timeout=aiohttp.ClientTimeout(total=30)
            ) as response:
                logger.info(f"üéØ [DIAGNOSTIC TTS] R√©ponse re√ßue: status={response.status}")
                
                if response.status == 200:
                    audio_data = await response.read()
                    logger.info(f"‚úÖ [DIAGNOSTIC TTS] Audio WAV g√©n√©r√© avec succ√®s: {len(audio_data)} bytes")

                    # CORRECTION: V√©rifier si les donn√©es commencent par l'en-t√™te WAV
                    if audio_data.startswith(b'RIFF'):
                        logger.info("‚úÖ [DIAGNOSTIC TTS] Format WAV d√©tect√© - extraction des donn√©es PCM")
                        import io
                        import wave
                        
                        try:
                            # Extraire les donn√©es PCM du fichier WAV
                            with io.BytesIO(audio_data) as wav_buffer:
                                with wave.open(wav_buffer, 'rb') as wav_file:
                                    # Lire les param√®tres WAV
                                    n_channels = wav_file.getnchannels()
                                    sampwidth = wav_file.getsampwidth()
                                    framerate = wav_file.getframerate()
                                    n_frames = wav_file.getnframes()
                                    
                                    logger.info(f"üéØ [DIAGNOSTIC TTS] Param√®tres WAV: {n_channels}ch, {sampwidth}bytes/sample, {framerate}Hz, {n_frames} frames")
                                    
                                    # Extraire les donn√©es PCM brutes
                                    pcm_data = wav_file.readframes(n_frames)
                                    logger.info(f"‚úÖ [DIAGNOSTIC TTS] Donn√©es PCM extraites: {len(pcm_data)} bytes")
                                    
                                    # Cr√©er le chunk audio avec les donn√©es PCM
                                    chunk = type('AudioChunk', (), {
                                        'data': pcm_data
                                    })()
                                    
                                    logger.info("üéØ [DIAGNOSTIC TTS] Ajout du chunk audio PCM √† la queue")
                                    await self._audio_queue.put(chunk)
                                    logger.info("‚úÖ [DIAGNOSTIC TTS] Chunk audio PCM ajout√© avec succ√®s")
                                    
                        except Exception as wav_error:
                            logger.error(f"‚ùå [DIAGNOSTIC TTS] Erreur lors de l'extraction PCM du WAV: {wav_error}")
                            # Fallback: utiliser les donn√©es brutes comme PCM
                            chunk = type('AudioChunk', (), {
                                'data': audio_data
                            })()
                            await self._audio_queue.put(chunk)
                    else:
                        logger.info("‚úÖ [DIAGNOSTIC TTS] Format PCM brut d√©tect√© - utilisation directe")
                        # Les donn√©es sont d√©j√† en PCM brut
                        chunk = type('AudioChunk', (), {
                            'data': audio_data
                        })()
                        
                        logger.info("üéØ [DIAGNOSTIC TTS] Ajout du chunk audio PCM brut √† la queue")
                        await self._audio_queue.put(chunk)
                        logger.info("‚úÖ [DIAGNOSTIC TTS] Chunk audio PCM brut ajout√© avec succ√®s")
                    
                else:
                    error_text = await response.text()
                    logger.error(f"‚ùå [DIAGNOSTIC TTS] Erreur API OpenAI {response.status}")
                    logger.error(f"‚ùå [DIAGNOSTIC TTS] D√©tails: {error_text}")
                    
                    # Analyser l'erreur
                    if response.status == 401:
                        logger.error("‚ùå [DIAGNOSTIC TTS] Erreur d'authentification - v√©rifier OPENAI_API_KEY")
                    elif response.status == 429:
                        logger.error("‚ùå [DIAGNOSTIC TTS] Limite de taux d√©pass√©e")
                    elif response.status == 500:
                        logger.error("‚ùå [DIAGNOSTIC TTS] Erreur serveur OpenAI")
            
        except aiohttp.ClientError as e:
            logger.error(f"‚ùå [DIAGNOSTIC TTS] Erreur r√©seau: {type(e).__name__}: {e}")
        except asyncio.TimeoutError:
            logger.error("‚ùå [DIAGNOSTIC TTS] Timeout lors de la g√©n√©ration audio (30s)")
        except Exception as e:
            logger.error(f"‚ùå [DIAGNOSTIC TTS] Erreur inattendue: {type(e).__name__}: {e}", exc_info=True)
        finally:
            logger.info("üéØ [DIAGNOSTIC TTS] Ajout du signal de fin (None) √† la queue")
            await self._audio_queue.put(None)
            logger.info("‚úÖ [DIAGNOSTIC TTS] G√©n√©ration audio termin√©e")
    
    def __aiter__(self):
        """It√©rateur asynchrone"""
        return self
    
    async def __anext__(self):
        """Obtenir le prochain chunk audio avec protection contre fermeture"""
        if self._closed:
            raise StopAsyncIteration
            
        try:
            chunk = await self._audio_queue.get()
            if chunk is None or self._closed:
                raise StopAsyncIteration
            return chunk
        except asyncio.CancelledError:
            raise StopAsyncIteration

class EloquenceAgent(Agent):
    def __init__(self, stt_service, llm_service, tts_service):
        super().__init__(instructions="Je suis un agent vocal Eloquence IA, pr√™t √† dialoguer en temps r√©el et √† diffuser des r√©ponses audio.")
        self.stt_service = stt_service
        self.llm_service = llm_service
        self.tts_service = tts_service
        self.audio_source = None
        self.audio_track = None
        self.publication = None
    
    async def on_connected(self, room: rtc.Room) -> None:
        """Handler d√©clench√© lorsque l'agent est connect√© √† la room - SIGNATURE CORRIG√âE"""
        logger.debug("üîó [DIAGNOSTIC] on_connected appel√© - d√©but de la fonction")
        logger.info(f"‚úÖ [AGENT CONNECTED] Connexion √©tablie pour Room: {room.name}")
        logger.info("--- D√âBUT: CONFIGURE AUDIO PUBLICATION EN ON_CONNECTED ---")

        try:
            # Attendre un peu pour s'assurer que la connexion est stable
            await asyncio.sleep(2)
            
            logger.info("üîä [STARTUP] --- D√©but de la PUBLICATION AUDIO et envoi du MESSAGE DE BIENVENUE ---")
            
            logger.info("üîä [STARTUP - √âTAPE 1] Cr√©ation de la source audio...")
            self.audio_source = rtc.AudioSource(sample_rate=48000, num_channels=1)
            logger.info(f"‚úÖ [STARTUP - √âTAPE 1] Source audio cr√©√©e: {self.audio_source}")
            
            logger.info("üîä [STARTUP - √âTAPE 2] Cr√©ation du track audio local...")
            track_name = f"agent-audio-{uuid.uuid4().hex[:8]}"
            self.audio_track = rtc.LocalAudioTrack.create_audio_track(track_name, self.audio_source)
            logger.info(f"‚úÖ [STARTUP - √âTAPE 2] Track audio local cr√©√©: {self.audio_track}")
            
            logger.info("üîä [STARTUP - √âTAPE 3] Publication du track audio sur la Room LiveKit...")
            options = rtc.TrackPublishOptions(
                source=rtc.TrackSource.SOURCE_MICROPHONE,
                dtx=False
            )
            
            self.publication = await room.local_participant.publish_track(self.audio_track, options)
            logger.info(f"‚úÖ [STARTUP - √âTAPE 3] Track publi√© avec succ√®s. SID: {self.publication.sid}")
            logger.info("--- FIN: CONFIGURE AUDIO PUBLICATION EN ON_CONNECTED ---")
            
            logger.info("üîä [STARTUP - √âTAPE 4] Envoi du message de bienvenue...")
            try:
                await self.send_welcome_message()
                logger.info("‚úÖ [STARTUP - √âTAPE 4] Message de bienvenue envoy√© avec succ√®s.")
            except Exception as welcome_error:
                logger.error(f"‚ùå [STARTUP - √âTAPE 4] Erreur lors de l'envoi du message de bienvenue: {welcome_error}", exc_info=True)
            
            # CORRECTION: Attendre un peu plus pour s'assurer que le TTS est termin√©
            logger.info("üîä [STARTUP - √âTAPE 5] Attente de 10 secondes pour s'assurer que le TTS est termin√©...")
            await asyncio.sleep(10)
            logger.info("‚úÖ [STARTUP - √âTAPE 5] Attente termin√©e.")

        except Exception as e:
            logger.error(f"‚ùå [ON_CONNECTED] Erreur critique lors de la configuration post-connexion: {e}", exc_info=True)
        finally:
            logger.info("--- Fin de la configuration 'on_connected' ---")

    async def on_track_subscribed(self, track: rtc.Track, publication: rtc.RemoteTrackPublication, participant: rtc.RemoteParticipant) -> None:
        """Handler pour les pistes audio entrantes - SIGNATURE CORRIG√âE"""
        logger.debug("üîó [DIAGNOSTIC] on_track_subscribed appel√© - d√©but de la fonction")
        logger.debug(f"DEBUG: on_track_subscribed appel√© pour track: {track.name}, kind: {track.kind}")
        if track.kind == rtc.TrackKind.KIND_AUDIO:
            logger.info(f"üéß [TRACK SUBSCRIBED] D√©tection d'une piste AUDIO: {track.name}")
            logger.info(f"üéß [TRACK SUBSCRIBED] Piste audio de {participant.identity} ({track.name}) re√ßue.")
            # Le traitement audio sera g√©r√© par le gestionnaire d'√©v√©nements principal
        else:
            logger.info(f"‚ÑπÔ∏è [TRACK SUBSCRIBED] Piste non-audio ({track.kind}) de {participant.identity} ignor√©e.")
    
    async def send_welcome_message(self):
        """Envoie un message de bienvenue via TTS"""
        try:
            welcome_text = "Bonjour ! Je suis votre assistant vocal Eloquence. Je suis maintenant connect√© et pr√™t √† vous aider."
            
            # Utiliser le service TTS configur√©
            if self.tts_service and self.audio_source:
                logger.info("üéµ [WELCOME] G√©n√©ration du message de bienvenue via TTS...")
                
                # Synth√©tiser l'audio avec gestion compl√®te du stream
                logger.info("üéµ [WELCOME] D√©marrage de la synth√®se TTS...")
                stream = await self.tts_service.synthesize(welcome_text)
                
                # Collecter les chunks audio avec diagnostic
                audio_chunks = []
                chunk_count = 0
                
                logger.info("üéµ [WELCOME] Collecte des chunks audio du stream TTS...")
                async with stream as active_stream:
                    async for chunk in active_stream:
                        if hasattr(chunk, 'data'):
                            audio_chunks.append(chunk.data)
                            chunk_count += 1
                            logger.info(f"üéµ [WELCOME] Chunk {chunk_count} re√ßu: {len(chunk.data)} bytes")
                
                logger.info(f"üéµ [WELCOME] Total chunks collect√©s: {chunk_count}")
                
                if audio_chunks:
                    combined_audio = b''.join(audio_chunks)
                    logger.info(f"‚úÖ [WELCOME] Audio g√©n√©r√©: {len(combined_audio)} bytes")
                    
                    # Diffuser l'audio et attendre que ce soit termin√©
                    logger.info("üéµ [WELCOME] D√©but de la diffusion audio...")
                    await self.stream_audio_to_livekit(combined_audio)
                    logger.info("‚úÖ [WELCOME] Message de bienvenue diffus√© avec succ√®s")
                else:
                    logger.warning("‚ö†Ô∏è [WELCOME] Aucune donn√©e audio g√©n√©r√©e")
            else:
                logger.warning("‚ö†Ô∏è [WELCOME] Service TTS ou AudioSource non disponible")
                
        except Exception as e:
            logger.error(f"‚ùå [WELCOME] Erreur lors de l'envoi du message de bienvenue: {e}", exc_info=True)
    
    async def stream_audio_to_livekit(self, audio_data: bytes):
        """Diffuse l'audio PCM vers LiveKit"""
        try:
            if not self.audio_source:
                logger.error("‚ùå [STREAM] AudioSource non disponible")
                return
            
            # Convertir en numpy array int16
            audio_array = np.frombuffer(audio_data, dtype=np.int16)
            
            # R√©√©chantillonner de 24kHz √† 48kHz si n√©cessaire
            original_rate = 24000
            target_rate = 48000
            
            if original_rate != target_rate:
                audio_float = audio_array.astype(np.float32)
                num_samples = int(len(audio_float) * target_rate / original_rate)
                resampled = resample(audio_float, num_samples)
                
                # Normaliser et convertir en int16
                max_val = np.max(np.abs(resampled))
                if max_val > 0:
                    audio_array = (resampled * 32767 / max_val).astype(np.int16)
                else:
                    audio_array = np.zeros_like(resampled, dtype=np.int16)
            
            # Envoyer par chunks de 10ms
            chunk_size = int(target_rate * 0.01)  # 480 samples pour 10ms √† 48kHz
            
            for i in range(0, len(audio_array), chunk_size):
                chunk = audio_array[i:i+chunk_size]
                
                # Padding si n√©cessaire
                if len(chunk) < chunk_size:
                    chunk = np.pad(chunk, (0, chunk_size - len(chunk)), 'constant')
                
                frame = rtc.AudioFrame(
                    data=chunk.tobytes(),
                    sample_rate=target_rate,
                    num_channels=1,
                    samples_per_channel=chunk_size
                )
                
                await self.audio_source.capture_frame(frame)
                await asyncio.sleep(0.01)  # 10ms delay
            
            logger.info("‚úÖ [STREAM] Audio diffus√© avec succ√®s")
            
        except Exception as e:
            logger.error(f"‚ùå [STREAM] Erreur lors de la diffusion: {e}", exc_info=True)
    
    async def process_incoming_audio(self, track: rtc.Track, room: rtc.Room = None):
        """Traite l'audio entrant du client"""
        try:
            logger.info("üé§ [PROCESS] D√©marrage du traitement audio entrant")
            
            stt_stream = self.stt_service.stream()
            
            # Cr√©er le contexte de traitement avec la room
            async with StreamAdapterContext(
                stt_stream,
                room=room,  # Utiliser la room pass√©e en param√®tre
                llm_service=self.llm_service,
                tts_service=self.tts_service,
                audio_source=self.audio_source,
                audio_track=self.audio_track
            ) as context:
                
                audio_stream = rtc.AudioStream(track)
                async for audio_frame_event in audio_stream:
                    context.push_frame(audio_frame_event.frame)
                    
        except Exception as e:
            logger.error(f"‚ùå [PROCESS] Erreur lors du traitement audio: {e}", exc_info=True)


async def create_and_configure_agent(ctx: JobContext) -> None:
    """Cr√©er et configurer l'agent vocal avec l'architecture manuelle qui fonctionne + VAD Silero."""
    print("=" * 80)
    print("üöÄ [AGENT MANUEL + VAD] Agent vocal Eloquence avec architecture manuelle + VAD Silero")
    print(f"üöÄ [AGENT MANUEL + VAD] Timestamp: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 80)

    logger.info("--- ARCHITECTURE MANUELLE + VAD: Retour √† l'architecture qui fonctionnait ---")
    
    http_session: Optional[aiohttp.ClientSession] = None
    try:
        http_session = aiohttp.ClientSession()
        register_http_session(http_session)  # Enregistrer pour fermeture propre
        logger.info("üîß [AGENT MANUEL + VAD] Session HTTP partag√©e cr√©√©e.")

        # Initialiser les services
        logger.info("üîß [AGENT MANUEL + VAD] Initialisation des services...")
        custom_llm = CustomLLM(http_session)
        custom_tts = CustomTTS(http_session)
        custom_stt = CustomSTT()
        logger.info("‚úÖ [AGENT MANUEL + VAD] Services initialis√©s.")

        # Charger VAD Silero pour am√©liorer la d√©tection vocale
        logger.info("üéØ [VAD] Chargement de Silero VAD...")
        vad = silero.VAD.load()
        logger.info("‚úÖ [VAD] Silero VAD charg√© avec succ√®s.")
        
        # DIAGNOSTIC VAD: V√©rifier les m√©thodes disponibles
        logger.info(f"üîç [VAD DIAGNOSTIC] Type de l'objet VAD: {type(vad)}")
        logger.info(f"üîç [VAD DIAGNOSTIC] M√©thodes disponibles: {[method for method in dir(vad) if not method.startswith('_')]}")
        logger.info(f"üîç [VAD DIAGNOSTIC] VAD est callable: {callable(vad)}")
        
        # Tester si VAD a une m√©thode sp√©cifique
        if hasattr(vad, '__call__'):
            logger.info("‚úÖ [VAD DIAGNOSTIC] VAD a une m√©thode __call__")
        if hasattr(vad, 'detect'):
            logger.info("‚úÖ [VAD DIAGNOSTIC] VAD a une m√©thode detect")
        if hasattr(vad, 'predict'):
            logger.info("‚úÖ [VAD DIAGNOSTIC] VAD a une m√©thode predict")

        # Cr√©er l'agent avec l'architecture manuelle qui fonctionnait
        logger.info("üîß [AGENT MANUEL + VAD] Cr√©ation de l'agent Eloquence...")
        agent = EloquenceAgent(custom_stt, custom_llm, custom_tts)
        logger.info("‚úÖ [AGENT MANUEL + VAD] Agent Eloquence cr√©√©.")

        # Connecter √† la room
        logger.info("üîó [AGENT MANUEL + VAD] Connexion √† la room...")
        await ctx.connect()
        logger.info("‚úÖ [AGENT MANUEL + VAD] Room connect√©e avec succ√®s.")

        # Configurer les gestionnaires d'√©v√©nements manuellement
        logger.info("üîß [AGENT MANUEL + VAD] Configuration des gestionnaires d'√©v√©nements...")
        
        @ctx.room.on("participant_connected")
        def on_participant_connected(participant: rtc.RemoteParticipant):
            logger.info(f"üîó [PARTICIPANT] Participant connect√©: {participant.identity}")

        @ctx.room.on("track_subscribed")
        def on_track_subscribed(track: rtc.Track, publication: rtc.RemoteTrackPublication, participant: rtc.RemoteParticipant):
            logger.info(f"üéß [TRACK] Track souscrit: {track.name} de {participant.identity}")
            if track.kind == rtc.TrackKind.KIND_AUDIO:
                logger.info("üé§ [AUDIO TRACK] D√©marrage du traitement audio avec VAD...")
                # Cr√©er une t√¢che pour traiter l'audio avec VAD
                asyncio.create_task(process_audio_with_vad(track, custom_stt, custom_llm, custom_tts, ctx.room, vad))

        logger.info("‚úÖ [AGENT MANUEL + VAD] Gestionnaires d'√©v√©nements configur√©s.")

        # Appeler on_connected pour initialiser l'agent
        logger.info("üîß [AGENT MANUEL + VAD] Initialisation de l'agent...")
        await agent.on_connected(ctx.room)
        logger.info("‚úÖ [AGENT MANUEL + VAD] Agent initialis√© avec succ√®s.")

        # Attendre les interactions
        logger.info("üîó [AGENT MANUEL + VAD] Agent pr√™t - en attente d'interactions vocales...")
        logger.info("üéØ [VAD STATUS] VAD Silero actif - d√©tection vocale am√©lior√©e...")
        
        try:
            # Attendre 10 minutes pour permettre les tests
            await asyncio.sleep(600)  # 10 minutes
            logger.info("üîó [AGENT MANUEL + VAD] Timeout de 10 minutes atteint - arr√™t de l'agent")
        except asyncio.CancelledError:
            logger.info("üîó [AGENT MANUEL + VAD] Agent annul√© par signal externe")
            raise
        
    except Exception as e:
        logger.error(f"‚ùå [AGENT MANUEL + VAD ERROR] Erreur fatale: {e}", exc_info=True)
        raise
    finally:
        logger.info("üßπ [AGENT MANUEL + VAD CLEANUP] Nettoyage des services...")
        if http_session and not http_session.closed:
            try:
                await http_session.close()
                logger.debug("‚úÖ [AGENT] Session HTTP principale ferm√©e proprement")
            except Exception as close_error:
                logger.warning(f"‚ö†Ô∏è [AGENT] Erreur fermeture session principale: {close_error}")
        logger.info("--- Fin de create_and_configure_agent MANUEL + VAD ---")


async def process_audio_with_vad(track: rtc.AudioTrack, stt: CustomSTT, llm: CustomLLM, tts: CustomTTS, room: rtc.Room, vad):
    """Traiter l'audio avec VAD Silero pour une meilleure d√©tection vocale."""
    logger.info("üé§ [VAD AUDIO] D√©marrage du traitement audio avec VAD Silero...")
    
    audio_buffer = []
    is_speaking = False
    speech_start_time = None
    silence_duration = 0
    SILENCE_THRESHOLD = 1.0  # 1 seconde de silence pour terminer
    
    audio_stream = rtc.AudioStream(track)
    
    async for audio_frame_event in audio_stream:
        try:
            # CORRECTION: V√©rification de s√©curit√© pour √©viter l'erreur AttributeError
            if not hasattr(audio_frame_event, 'frame'):
                logger.warning("‚ö†Ô∏è [VAD FRAME] AudioFrameEvent sans attribut 'frame' - ignor√©")
                continue
                
            if not hasattr(audio_frame_event.frame, 'data'):
                logger.warning("‚ö†Ô∏è [VAD FRAME] AudioFrame sans attribut 'data' - ignor√©")
                continue
            
            # Convertir le frame audio en numpy array
            audio_data = np.frombuffer(audio_frame_event.frame.data, dtype=np.int16)
            frame = audio_frame_event.frame
            
            # V√©rification suppl√©mentaire des attributs du frame
            if not hasattr(frame, 'sample_rate') or not hasattr(frame, 'samples_per_channel'):
                logger.warning("‚ö†Ô∏è [VAD FRAME] Frame sans attributs sample_rate ou samples_per_channel - ignor√©")
                continue
            
            # DIAGNOSTIC VAD: Analyser les donn√©es audio avant VAD
            logger.debug(f"üîç [VAD DIAGNOSTIC] Audio data shape: {audio_data.shape}, dtype: {audio_data.dtype}")
            logger.debug(f"üîç [VAD DIAGNOSTIC] Sample rate: {frame.sample_rate}, samples per channel: {frame.samples_per_channel}")
            logger.debug(f"üîç [VAD DIAGNOSTIC] Audio data range: min={np.min(audio_data)}, max={np.max(audio_data)}")
            
            # Convertir les donn√©es audio au format attendu par VAD Silero (float32, normalis√© -1 √† 1)
            try:
                # VAD Silero attend des donn√©es float32 normalis√©es entre -1 et 1
                audio_data_float = audio_data.astype(np.float32) / 32768.0
                logger.debug(f"üîç [VAD DIAGNOSTIC] Audio data converted to float32: shape={audio_data_float.shape}, range: min={np.min(audio_data_float):.6f}, max={np.max(audio_data_float):.6f}")
                
                # CORRECTION: Utiliser une heuristique bas√©e sur l'√©nergie audio
                # Le VAD Silero dans LiveKit v1.x n√©cessite une approche diff√©rente
                logger.debug("üîç [VAD CORRECTION] Utilisation d'une heuristique bas√©e sur l'√©nergie audio")
                
                try:
                    # Calculer l'√©nergie RMS de l'audio
                    energy = np.sqrt(np.mean(audio_data_float**2))
                    
                    # Calculer la probabilit√© de parole bas√©e sur l'√©nergie
                    # Seuils ajustables selon les besoins
                    MIN_ENERGY = 0.001  # Seuil minimum pour consid√©rer comme du bruit
                    MAX_ENERGY = 0.1    # Seuil maximum pour normaliser
                    
                    if energy < MIN_ENERGY:
                        speech_prob = 0.0  # Silence
                    else:
                        # Normaliser l'√©nergie entre 0 et 1
                        normalized_energy = min(energy / MAX_ENERGY, 1.0)
                        speech_prob = normalized_energy
                    
                    logger.debug(f"üîç [VAD HEURISTIQUE] √ânergie: {energy:.6f}, Probabilit√©: {speech_prob:.3f}")
                    
                except Exception as energy_error:
                    logger.warning(f"‚ö†Ô∏è [VAD HEURISTIQUE] Erreur calcul √©nergie: {energy_error}")
                    # Valeur par d√©faut en cas d'erreur
                    speech_prob = 0.5
                    
                logger.debug(f"‚úÖ [VAD DIAGNOSTIC] VAD appel√© avec succ√®s, r√©sultat: {speech_prob}")
                
            except Exception as vad_error:
                logger.error(f"‚ùå [VAD DIAGNOSTIC] Erreur lors de l'appel VAD: {vad_error}")
                logger.error(f"‚ùå [VAD DIAGNOSTIC] Type d'erreur: {type(vad_error).__name__}")
                logger.error(f"‚ùå [VAD DIAGNOSTIC] D√©tails: {str(vad_error)}")
                # Utiliser une valeur par d√©faut en cas d'erreur
                speech_prob = 0.5
            
            logger.debug(f"üéØ [VAD] Probabilit√© de parole: {speech_prob:.3f}")
            
            # Seuil de d√©tection vocale (ajustable)
            SPEECH_THRESHOLD = 0.5
            
            if speech_prob > SPEECH_THRESHOLD:
                if not is_speaking:
                    logger.info("üó£Ô∏è [VAD] D√©but de parole d√©tect√©")
                    is_speaking = True
                    speech_start_time = time.time()
                    audio_buffer = []
                
                # Ajouter l'audio au buffer
                audio_buffer.append(audio_data)
                silence_duration = 0
                
            else:
                if is_speaking:
                    silence_duration += frame.samples_per_channel / frame.sample_rate
                    audio_buffer.append(audio_data)
                    
                    if silence_duration >= SILENCE_THRESHOLD:
                        logger.info("üîá [VAD] Fin de parole d√©tect√©e - traitement STT...")
                        
                        # Concat√©ner tout l'audio du buffer
                        if audio_buffer:
                            full_audio = np.concatenate(audio_buffer)
                            
                            # Traiter avec STT
                            try:
                                # Convertir en WAV pour STT
                                wav_data = convert_audio_to_wav(full_audio, frame.sample_rate)
                                
                                # Transcription
                                transcription = await transcribe_audio_with_whisper(wav_data)
                                
                                if transcription and transcription.strip():
                                    logger.info(f"üìù [STT] Transcription: {transcription}")
                                    
                                    # Traitement LLM
                                    response = await process_with_llm(transcription, llm)
                                    logger.info(f"ü§ñ [LLM] R√©ponse: {response}")
                                    
                                    # Synth√®se TTS
                                    audio_response = await synthesize_with_tts(response, tts)
                                    
                                    # Diffuser la r√©ponse
                                    await stream_audio_to_room(audio_response, room)
                                    
                                else:
                                    logger.info("üîá [STT] Aucune transcription d√©tect√©e")
                                    
                            except Exception as e:
                                logger.error(f"‚ùå [VAD PROCESSING] Erreur traitement: {e}")
                        
                        # Reset
                        is_speaking = False
                        audio_buffer = []
                        silence_duration = 0
                        
        except Exception as e:
            logger.error(f"‚ùå [VAD FRAME] Erreur traitement frame: {e}")
            logger.error(f"‚ùå [VAD FRAME] Type de audio_frame_event: {type(audio_frame_event)}")
            logger.error(f"‚ùå [VAD FRAME] Attributs disponibles: {dir(audio_frame_event) if hasattr(audio_frame_event, '__dict__') else 'N/A'}")
            continue


def convert_audio_to_wav(audio_data: np.ndarray, sample_rate: int) -> bytes:
    """Convertir les donn√©es audio en format WAV."""
    try:
        # Cr√©er un buffer en m√©moire pour le WAV
        wav_buffer = io.BytesIO()
        
        with wave.open(wav_buffer, 'wb') as wav_file:
            wav_file.setnchannels(1)  # Mono
            wav_file.setsampwidth(2)  # 16-bit
            wav_file.setframerate(sample_rate)
            wav_file.writeframes(audio_data.astype(np.int16).tobytes())
        
        wav_buffer.seek(0)
        return wav_buffer.read()
        
    except Exception as e:
        logger.error(f"‚ùå [WAV CONVERSION] Erreur: {e}")
        raise


async def transcribe_audio_with_whisper(wav_data: bytes) -> str:
    """Transcrire l'audio avec Whisper."""
    session = None
    try:
        whisper_url = "http://whisper-stt:8001/transcribe"
        
        session = aiohttp.ClientSession()
        register_http_session(session)  # Enregistrer pour fermeture propre
        form = aiohttp.FormData()
        form.add_field('audio', wav_data, filename='audio.wav', content_type='audio/wav')
        form.add_field('language', 'fr')
        form.add_field('model', 'whisper-large-v3-turbo')
        
        async with session.post(whisper_url, data=form, timeout=aiohttp.ClientTimeout(total=30)) as response:
            if response.status == 200:
                data = await response.json()
                return data.get('text', '').strip()
            else:
                logger.error(f"‚ùå [STT] Erreur Whisper {response.status}")
                return ""
                    
    except Exception as e:
        logger.error(f"‚ùå [STT] Erreur transcription: {e}")
        return ""
    finally:
        if session and not session.closed:
            try:
                await session.close()
                logger.debug("‚úÖ [STT] Session HTTP ferm√©e proprement")
            except Exception as close_error:
                logger.warning(f"‚ö†Ô∏è [STT] Erreur fermeture session: {close_error}")


async def process_with_llm(text: str, llm: CustomLLM) -> str:
    """Traiter le texte avec le LLM."""
    try:
        # Cr√©er un contexte de chat simple
        chat_ctx = agents.llm.ChatContext()
        
        # CORRECTION: Cr√©er les messages avec le format LISTE requis par Pydantic validation
        system_msg = ChatMessage(role="system", content=["Tu es un assistant vocal. R√©ponds de mani√®re concise et naturelle."])
        user_msg = ChatMessage(role="user", content=[text])
        
        chat_ctx.messages = [system_msg, user_msg]
        
        # Appeler le LLM
        stream = await llm.chat(chat_ctx=chat_ctx)
        
        response_text = ""
        async with stream as active_stream:
            async for chunk in active_stream:
                if isinstance(chunk, dict) and chunk.get('choices'):
                    delta = chunk['choices'][0].get('delta', {})
                    content = delta.get('content')
                    if content:
                        response_text += content
        
        return response_text if response_text else "Je n'ai pas de r√©ponse pour le moment."
        
    except Exception as e:
        logger.error(f"‚ùå [LLM] Erreur traitement: {e}")
        return "Une erreur est survenue."


async def synthesize_with_tts(text: str, tts: CustomTTS) -> bytes:
    """Synth√©tiser le texte avec TTS."""
    try:
        stream = await tts.synthesize(text)
        
        audio_chunks = []
        async with stream as active_stream:
            async for chunk in active_stream:
                if hasattr(chunk, 'data'):
                    audio_chunks.append(chunk.data)
        
        return b''.join(audio_chunks) if audio_chunks else b""
        
    except Exception as e:
        logger.error(f"‚ùå [TTS] Erreur synth√®se: {e}")
        return b""


async def stream_audio_to_room(audio_data: bytes, room: rtc.Room):
    """Diffuser l'audio dans la room LiveKit."""
    try:
        logger.info("üéµ [AUDIO STREAM] Diffusion audio dans la room...")
        
        # Cr√©er une source audio
        source = rtc.AudioSource(sample_rate=48000, num_channels=1)
        track = rtc.LocalAudioTrack.create_audio_track("agent-response", source)
        
        # Publier le track
        options = rtc.TrackPublishOptions()
        publication = await room.local_participant.publish_track(track, options)
        logger.info(f"‚úÖ [AUDIO STREAM] Track publi√©: {publication.sid}")
        
        # Les donn√©es "audio_data" sont d√©j√† en PCM brut (du CustomTTSStream)
        # Convertir en numpy array
        original_rate = 24000 # Le TTS g√©n√®re 24kHz
        target_rate = 48000
        
        audio_array = np.frombuffer(audio_data, dtype=np.int16)
        
        # R√©√©chantillonner de 24kHz √† 48kHz si n√©cessaire
        if len(audio_array) > 0 and original_rate != target_rate:
            logger.info(f"üéµ [AUDIO STREAM] R√©√©chantillonnage de {original_rate}Hz √† {target_rate}Hz...")
            audio_float = audio_array.astype(np.float32)
            num_samples = int(len(audio_float) * target_rate / original_rate)
            resampled = resample(audio_float, num_samples)
            
            # Normaliser et convertir en int16
            max_val = np.max(np.abs(resampled))
            if max_val > 0:
                audio_array_to_stream = (resampled * 32767 / max_val).astype(np.int16)
            else:
                audio_array_to_stream = np.zeros_like(resampled, dtype=np.int16)
            logger.info(f"‚úÖ [AUDIO STREAM] R√©√©chantillonnage termin√©. Taille: {audio_array_to_stream.shape}")
        else:
            audio_array_to_stream = audio_array
            logger.info(f"üéµ [AUDIO STREAM] Pas de r√©√©chantillonnage n√©cessaire. Taille: {audio_array_to_stream.shape}")
            
        # Envoyer par chunks de 10ms
        chunk_size = int(target_rate * 0.01)  # 480 samples pour 10ms √† 48kHz

        if len(audio_array_to_stream) == 0:
            logger.warning("‚ö†Ô∏è [AUDIO STREAM] Aucune donn√©e audio √† diffuser apr√®s traitement.")
            return
            
        logger.info(f"üéµ [AUDIO STREAM] D√©marrage diffusion de {len(audio_array_to_stream)} samples √† {target_rate}Hz...")
        
        for i in range(0, len(audio_array_to_stream), chunk_size):
            chunk = audio_array_to_stream[i:i+chunk_size]
            
            # Padding si n√©cessaire pour le dernier chunk
            if len(chunk) < chunk_size:
                chunk = np.pad(chunk, (0, chunk_size - len(chunk)), 'constant', constant_values=0)
            
            frame = rtc.AudioFrame(
                data=chunk.tobytes(), # PCM int16 bytes
                sample_rate=target_rate,
                num_channels=1,
                samples_per_channel=chunk_size
            )
            
            await source.capture_frame(frame)
            await asyncio.sleep(0.01)  # 10ms delay
        
        logger.info("‚úÖ [AUDIO STREAM] Diffusion termin√©e")
        
    except Exception as e:
        logger.error(f"‚ùå [AUDIO STREAM] Erreur lors de la diffusion: {e}", exc_info=True)


async def process_audio_track(track: rtc.Track, stt: CustomSTT, llm: CustomLLM, tts: CustomTTS, room: rtc.Room):
    """Traiter une piste audio avec optimisation m√©moire"""
    logger.info("üîß [OPTIMISATION M√âMOIRE] D√©but du traitement d'une piste audio entrante.")
    logger.info("üîß [OPTIMISATION M√âMOIRE] Tentative de cr√©ation des objets audio persistants (pour r√©utilisation).")
    
    try:
        persistent_audio_source = rtc.AudioSource(sample_rate=48000, num_channels=1)
        logger.info(f"‚úÖ [OPTIMISATION M√âMOIRE] AudioSource persistante cr√©√©e: {persistent_audio_source}")
        
        persistent_audio_track = rtc.LocalAudioTrack.create_audio_track(
            "eloquence_tts_response_persistent",
            persistent_audio_source
        )
        logger.info(f"‚úÖ [OPTIMISATION M√âMOIRE] LocalAudioTrack persistant cr√©√©: {persistent_audio_track}")
        
        stt_stream = stt.stream()
        
        async with StreamAdapterContext(
            stt_stream,
            room=room,
            llm_service=llm,
            tts_service=tts,
            audio_source=persistent_audio_source,
            audio_track=persistent_audio_track
        ) as context:
            logger.info("üîß [OPTIMISATION M√âMOIRE] StreamAdapterContext cr√©√© avec objets audio persistants.")
            logger.debug(f"üéß [AUDIO PROCESS] Traitement d'une frame audio entrante: {{audio_frame_event.frame.samples_per_channel}} samples")
            
            audio_stream = rtc.AudioStream(track)
            async for audio_frame_event in audio_stream:
                context.push_frame(audio_frame_event.frame)
                
    except Exception as e:
        logger.error(f"‚ùå [OPTIMISATION M√âMOIRE] Erreur lors de la cr√©ation ou de l'utilisation des objets audio persistants durant le traitement de piste: {e}", exc_info=True)
        logger.info("üîÑ [OPTIMISATION M√âMOIRE] Fallback vers la m√©thode sans objets audio persistants pour le traitement de piste.")
        stt_stream = stt.stream()
        async with StreamAdapterContext(stt_stream, room=room, llm_service=llm, tts_service=tts) as context:
            audio_stream = rtc.AudioStream(track)
            async for audio_frame_event in audio_stream:
                context.push_frame(audio_frame_event.frame)

async def send_welcome_message_immediate(audio_source):
    """Envoie un message de bienvenue imm√©diatement via l'AudioSource"""
    logger.info("üéµ [WELCOME] D√©but de la g√©n√©ration et envoi du message de bienvenue...")
    session = None
    try:
        welcome_text = "Bonjour ! Je suis votre assistant vocal Eloquence. Je suis maintenant connect√© et pr√™t √† vous aider. Vous pouvez commencer √† me parler."
        
        tts_url = "http://openai-tts:8001/synthesize"
        tts_payload = {
            "text": welcome_text,
            "voice": "alloy",
            "response_format": "wav"
        }
        
        logger.info(f"üéµ [WELCOME] Appel de l'API TTS ({tts_url}) pour le message de bienvenue.")
        session = aiohttp.ClientSession()
        register_http_session(session)  # Enregistrer pour fermeture propre
        async with session.post(tts_url, json=tts_payload) as response:
            if response.status == 200:
                audio_data = await response.read()
                logger.info(f"‚úÖ [WELCOME] Message de bienvenue g√©n√©r√© avec succ√®s: {len(audio_data)} bytes re√ßus du TTS.")
                
                logger.info("üéµ [WELCOME] Diffusion imm√©diate du message de bienvenue via AudioSource...")
                await stream_audio_to_source(audio_data, audio_source)
                logger.info("‚úÖ [WELCOME] Message de bienvenue diffus√© avec succ√®s sur AudioSource.")
                
            else:
                error_text = await response.text()
                logger.error(f"‚ùå [WELCOME] Erreur de l'API TTS lors de la g√©n√©ration du message de bienvenue: HTTP {response.status} - {error_text}")
                    
    except Exception as e:
        logger.error(f"‚ùå [WELCOME] Erreur inattendue lors de l'envoi du message de bienvenue: {e}", exc_info=True)
    finally:
        if session and not session.closed:
            try:
                await session.close()
                logger.debug("‚úÖ [WELCOME] Session HTTP ferm√©e proprement")
            except Exception as close_error:
                logger.warning(f"‚ö†Ô∏è [WELCOME] Erreur fermeture session: {close_error}")

async def stream_audio_to_source(audio_data: bytes, audio_source):
    """Diffuse des donn√©es audio WAV vers une AudioSource, en g√©rant l'en-t√™te."""
    logger.info(f"üéµ [STREAM AUDIO] D√©but de la diffusion de {len(audio_data)} bytes vers AudioSource.")
    try:
        # Utiliser io.BytesIO pour traiter les donn√©es en m√©moire comme un fichier
        with io.BytesIO(audio_data) as pcm_file:
            with wave.open(pcm_file, 'rb') as wav_file:
                n_channels = wav_file.getnchannels()
                sampwidth = wav_file.getsampwidth()
                framerate = wav_file.getframerate()
                n_frames = wav_file.getnframes()
                
                logger.info(f"üéµ [WAV PARAMS] Canaux: {n_channels}, Largeur d'√©chantillon: {sampwidth}, Taux: {framerate}, Frames: {n_frames}")

                # Lire les donn√©es audio brutes (PCM)
                pcm_data = wav_file.readframes(n_frames)

        logger.info("üéµ [STREAM AUDIO] Conversion des donn√©es PCM en numpy array (int16)...")
        audio_array = np.frombuffer(pcm_data, dtype=np.int16)
        logger.info(f"‚úÖ [STREAM AUDIO - WELCOME] Numpy array cr√©√© √† partir de WAV. Forme: {audio_array.shape}, Dtype: {audio_array.dtype}")
        
        # --- NOUVEL AJOUT DE DIAGNOSTIC ---
        if audio_array.size > 0:
            audio_min_raw = np.min(audio_array)
            audio_max_raw = np.max(audio_array)
            audio_mean_raw = np.mean(audio_array)
            audio_std_raw = np.std(audio_array)
            logger.info(f"üîä [STREAM AUDIO - WELCOME - ETAPE 1] Stats RAW int16 (pr√©-resampling): min={audio_min_raw}, max={audio_max_raw}, mean={audio_mean_raw:.2f}, std={audio_std_raw:.2f}")
            if audio_std_raw < 50:
                logger.warning("‚ö†Ô∏è [STREAM AUDIO - WELCOME - ETAPE 1] L'audio int16 RAW (pr√©-resampling) semble √™tre silencieux.")
        else:
            logger.warning("‚ö†Ô∏è [STREAM AUDIO - WELCOME - ETAPE 1] Audio array RAW est vide.")
        # --- FIN NOUVEL AJOUT ---

        if len(audio_array) > 0:
            from scipy.signal import resample
            
            target_sample_rate = 48000
            if framerate != target_sample_rate:
                logger.info(f"üéµ [STREAM AUDIO - WELCOME - ETAPE 2] R√©√©chantillonnage de {framerate}Hz √† {target_sample_rate}Hz...")
                audio_array_float = audio_array.astype(np.float32)
                num_samples_resampled = int(len(audio_array_float) * target_sample_rate / framerate)
                resampled_float = resample(audio_array_float, num_samples_resampled)
 
                max_abs_resampled_float = np.max(np.abs(resampled_float))
                if max_abs_resampled_float > 0:
                    audio_to_stream = (resampled_float * 32767 / max_abs_resampled_float).astype(np.int16)
                else:
                    audio_to_stream = np.zeros_like(resampled_float, dtype=np.int16) # Toutes les valeurs √† z√©ro si max_abs_val est nul
                logger.info(f"‚úÖ [STREAM AUDIO - WELCOME - ETAPE 2] Audio r√©√©chantillonn√© √† {target_sample_rate}kHz. Forme: {audio_to_stream.shape}")
            else:
                audio_to_stream = audio_array
            
            # --- NOUVEL AJOUT DE DIAGNOSTIC ---
            if audio_to_stream.size > 0:
                audio_stream_min = np.min(audio_to_stream)
                audio_stream_max = np.max(audio_to_stream)
                audio_stream_mean = np.mean(audio_to_stream)
                audio_stream_std = np.std(audio_to_stream)
                logger.info(f"üîä [STREAM AUDIO - WELCOME - ETAPE 3] Stats STREAM int16 (post-resampling): min={audio_stream_min}, max={audio_stream_max}, mean={audio_stream_mean:.2f}, std={audio_stream_std:.2f}")
                if audio_stream_std < 50:
                    logger.warning("‚ö†Ô∏è [STREAM AUDIO - WELCOME - ETAPE 3] L'audio stream√© semble √™tre silencieux APR√àS r√©√©chantillonnage/normalisation.")
            # --- FIN NOUVEL AJOUT ---

            chunk_size = int(target_sample_rate * 0.01)  # 480 samples pour 10ms √† 48kHz
            num_chunks = 0
            
            logger.info(f"üéµ [STREAM AUDIO - WELCOME] Envoi de l'audio par chunks de {chunk_size} samples (10ms)...")
            for i in range(0, len(audio_to_stream), chunk_size):
                chunk = audio_to_stream[i:i+chunk_size]
                
                if len(chunk) < chunk_size:
                    chunk = np.pad(chunk, (0, chunk_size - len(chunk)), 'constant', constant_values=0)
                
                frame = rtc.AudioFrame(
                    data=chunk.tobytes(),
                    sample_rate=target_sample_rate,
                    num_channels=1,
                    samples_per_channel=chunk_size
                )
                
                await audio_source.capture_frame(frame)
                num_chunks += 1
                if num_chunks % 50 == 0:
                     logger.info(f"üéµ [STREAM AUDIO - WELCOME] {num_chunks} chunks envoy√©s...")
                
                await asyncio.sleep(0.01)  # 10ms delay pour chaque frame
            
            logger.info(f"‚úÖ [STREAM AUDIO] Diffusion termin√©e. Total {num_chunks} chunks envoy√©s.")
                
    except Exception as e:
        logger.error(f"‚ùå [STREAM AUDIO] Erreur lors de la diffusion des donn√©es audio vers la source: {e}", exc_info=True)

async def test_audio_publication_immediate(ctx):
    """Test de validation de la publication audio"""
    logger.info("üß™ [TEST] D√©marrage du test de validation de la publication audio...")
    try:
        local_participant = ctx.room.local_participant
        if local_participant:
            publications = local_participant.track_publications.values()
            audio_tracks = [pub for pub in publications if pub.kind == rtc.TrackKind.KIND_AUDIO]
            
            logger.info(f"‚úÖ [TEST] Trouv√© {len(audio_tracks)} piste(s) audio publi√©e(s) par le participant local.")
            
            if audio_tracks:
                for track_pub in audio_tracks:
                    logger.info(f"  - Track publi√©: SID={track_pub.sid}, Name='{track_pub.name}', "
                                f"Kind={track_pub.kind}, Muted={track_pub.muted}, Source={track_pub.source}")
                logger.info("‚úÖ [TEST] Au moins une piste audio est publi√©e.")
                return True
            else:
                logger.warning("‚ö†Ô∏è [TEST] Aucune piste audio trouv√©e ou publi√©e par le participant local.")
                return False
        else:
            logger.warning("‚ö†Ô∏è [TEST] Participant local non disponible pour le test de publication.")
            return False
        
    except Exception as e:
        logger.error(f"‚ùå [TEST] Erreur lors de la validation de la publication audio: {e}", exc_info=True)
        return False

# Point d'entr√©e principal
if __name__ == "__main__":
    logging.basicConfig(level=logging.INFO, handlers=[logging.StreamHandler()])

    async def shutdown(loop: asyncio.AbstractEventLoop):
        """Nettoie les t√¢ches avant de fermer la boucle."""
        logger.info("üîå [SHUTDOWN] Annulation de toutes les t√¢ches en cours...")
        tasks = [t for t in asyncio.all_tasks() if t is not asyncio.current_task()]
        [task.cancel() for task in tasks]

        logger.info(f"‚è≥ [SHUTDOWN] Attente de la fin des {len(tasks)} t√¢ches...")
        await asyncio.gather(*tasks, return_exceptions=True)
        
        # CORRECTION: Fermer toutes les sessions HTTP AVANT loop.stop()
        logger.info("üåê [SHUTDOWN] Fermeture de toutes les sessions HTTP...")
        await close_all_http_sessions()
        
        logger.info("üõë [SHUTDOWN] Arr√™t de la boucle d'√©v√©nements.")
        loop.stop()

    async def main():
        """Cr√©e et ex√©cute le worker de l'agent."""
        worker_opts = agents.WorkerOptions(
            entrypoint_fnc=create_and_configure_agent,
            ws_url=os.environ.get("LIVEKIT_URL"),
            api_key=os.environ.get("LIVEKIT_API_KEY"),
            api_secret=os.environ.get("LIVEKIT_API_SECRET"),
        )
        worker = agents.Worker(worker_opts)
        logger.info("‚úÖ [MAIN] Worker cr√©√©. D√©marrage...")
        await worker.run()

    loop = asyncio.get_event_loop()
    try:
        logger.info("üöÄ D√©marrage de l'agent Eloquence (boucle infinie)...")
        loop.create_task(main())
        loop.run_forever()
    except KeyboardInterrupt:
        logger.info("üö® [MAIN] Interruption clavier d√©tect√©e.")
    finally:
        logger.info("üßπ [MAIN] D√©marrage de la s√©quence d'arr√™t propre...")
        loop.run_until_complete(shutdown(loop))
        loop.close()
        logger.info("‚úÖ [MAIN] Agent arr√™t√© proprement.")

# C'est le test final.
