#!/usr/bin/env python3
"""
Validation finale du systÃ¨me d'interpellation intelligente
Test complet avec vrais agents et prompts
"""
import asyncio
import logging
import os
from typing import Dict, List

# Configuration des logs
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Import du systÃ¨me
try:
    from interpellation_system import AdvancedInterpellationDetector
    from enhanced_multi_agent_manager import EnhancedMultiAgentManager, get_enhanced_manager
    from multi_agent_config import MultiAgentConfig
    IMPORTS_SUCCESS = True
except ImportError as e:
    logger.error(f"âŒ Erreur import: {e}")
    IMPORTS_SUCCESS = False

class ValidationTestSuite:
    """Suite de tests de validation du systÃ¨me d'interpellation"""
    
    def __init__(self):
        self.test_results = []
        self.passed_tests = 0
        self.total_tests = 0
    
    def add_result(self, test_name: str, passed: bool, details: str = ""):
        """Ajoute un rÃ©sultat de test"""
        self.total_tests += 1
        if passed:
            self.passed_tests += 1
        
        result = {
            'test_name': test_name,
            'passed': passed,
            'details': details
        }
        self.test_results.append(result)
        
        status = "âœ… PASSÃ‰" if passed else "âŒ Ã‰CHEC"
        print(f"{status}: {test_name}")
        if details:
            print(f"   {details}")
    
    def print_summary(self):
        """Affiche le rÃ©sumÃ© des tests"""
        print("\n" + "="*60)
        print("ğŸ“Š RÃ‰SUMÃ‰ DES TESTS DE VALIDATION")
        print("="*60)
        print(f"Tests rÃ©ussis: {self.passed_tests}/{self.total_tests}")
        print(f"Taux de rÃ©ussite: {(self.passed_tests/self.total_tests)*100:.1f}%")
        
        if self.passed_tests == self.total_tests:
            print("\nğŸ‰ TOUS LES TESTS SONT PASSÃ‰S !")
            print("âœ… Le systÃ¨me d'interpellation est prÃªt pour la production")
        else:
            print(f"\nâš ï¸ {self.total_tests - self.passed_tests} test(s) ont Ã©chouÃ©")
            print("âŒ Des corrections sont nÃ©cessaires")
            
            # Afficher les tests Ã©chouÃ©s
            failed_tests = [r for r in self.test_results if not r['passed']]
            print("\nTests Ã©chouÃ©s:")
            for test in failed_tests:
                print(f"   - {test['test_name']}: {test['details']}")

async def test_detector_patterns():
    """Test des patterns de dÃ©tection d'interpellation"""
    
    print("ğŸ§ª TEST: PATTERNS DE DÃ‰TECTION")
    print("-" * 40)
    
    detector = AdvancedInterpellationDetector()
    validator = ValidationTestSuite()
    
    # Test patterns directs Sarah
    test_cases_sarah = [
        "Sarah, que pensez-vous ?",
        "Journaliste, votre avis ?",
        "Sarah Johnson, pouvez-vous nous Ã©clairer ?",
        "Madame Johnson, votre analyse ?",
        "Notre journaliste, qu'en pensez-vous ?"
    ]
    
    for i, message in enumerate(test_cases_sarah, 1):
        detections = detector.detect_interpellations(message, "michel_dubois_animateur", [])
        has_sarah_detection = any(d.agent_id == "sarah_johnson_journaliste" and d.interpellation_type == "direct" for d in detections)
        validator.add_result(
            f"Pattern direct Sarah {i}",
            has_sarah_detection,
            f"Message: '{message}'"
        )
    
    # Test patterns directs Marcus
    test_cases_marcus = [
        "Marcus, votre expertise ?",
        "Expert, qu'en pensez-vous ?",
        "Marcus Thompson, pouvez-vous nous Ã©clairer ?",
        "Monsieur Thompson, votre analyse ?",
        "Notre expert, qu'en pensez-vous ?"
    ]
    
    for i, message in enumerate(test_cases_marcus, 1):
        detections = detector.detect_interpellations(message, "sarah_johnson_journaliste", [])
        has_marcus_detection = any(d.agent_id == "marcus_thompson_expert" and d.interpellation_type == "direct" for d in detections)
        validator.add_result(
            f"Pattern direct Marcus {i}",
            has_marcus_detection,
            f"Message: '{message}'"
        )
    
    # Test patterns indirects
    test_cases_indirect = [
        ("Qu'en pense notre journaliste ?", "sarah_johnson_journaliste"),
        ("L'avis de notre expert ?", "marcus_thompson_expert"),
        ("Votre enquÃªte rÃ©vÃ¨le quoi ?", "sarah_johnson_journaliste"),
        ("Votre expertise technique ?", "marcus_thompson_expert")
    ]
    
    for i, (message, expected_agent) in enumerate(test_cases_indirect, 1):
        detections = detector.detect_interpellations(message, "michel_dubois_animateur", [])
        has_indirect_detection = any(d.agent_id == expected_agent and d.interpellation_type == "indirect" for d in detections)
        validator.add_result(
            f"Pattern indirect {i}",
            has_indirect_detection,
            f"Message: '{message}' -> {expected_agent}"
        )
    
    # Test pas d'interpellation
    test_cases_no_interpellation = [
        "C'est un sujet intÃ©ressant.",
        "Je suis d'accord avec vous.",
        "Pouvez-vous expliquer ?",
        "C'est une bonne question."
    ]
    
    for i, message in enumerate(test_cases_no_interpellation, 1):
        detections = detector.detect_interpellations(message, "user", [])
        no_detection = len(detections) == 0
        validator.add_result(
            f"Pas d'interpellation {i}",
            no_detection,
            f"Message: '{message}'"
        )
    
    validator.print_summary()
    return validator.passed_tests == validator.total_tests

async def test_enhanced_manager_integration():
    """Test de l'intÃ©gration dans EnhancedMultiAgentManager"""
    
    print("\nğŸ§ª TEST: INTÃ‰GRATION ENHANCED MANAGER")
    print("-" * 40)
    
    validator = ValidationTestSuite()
    
    # Configuration
    config = MultiAgentConfig(
        exercise_id="test_integration",
        room_prefix="test",
        agents=[],
        interaction_rules={},
        turn_management="moderator_controlled"
    )
    
    # Test crÃ©ation manager
    try:
        manager = EnhancedMultiAgentManager("fake_key", "fake_key", config)
        validator.add_result(
            "CrÃ©ation EnhancedMultiAgentManager",
            True,
            "Manager crÃ©Ã© avec succÃ¨s"
        )
    except Exception as e:
        validator.add_result(
            "CrÃ©ation EnhancedMultiAgentManager",
            False,
            f"Erreur: {e}"
        )
        return False
    
    # Test prÃ©sence systÃ¨me d'interpellation
    has_interpellation = hasattr(manager, 'interpellation_manager')
    validator.add_result(
        "SystÃ¨me d'interpellation intÃ©grÃ©",
        has_interpellation,
        "Interpellation manager prÃ©sent" if has_interpellation else "Interpellation manager absent"
    )
    
    # Test mÃ©thode process_user_message_with_interpellations
    has_method = hasattr(manager, 'process_user_message_with_interpellations')
    validator.add_result(
        "MÃ©thode process_user_message_with_interpellations",
        has_method,
        "MÃ©thode prÃ©sente" if has_method else "MÃ©thode absente"
    )
    
    # Test agents configurÃ©s
    agents_count = len(manager.agents) if hasattr(manager, 'agents') else 0
    expected_agents = 3  # Michel, Sarah, Marcus
    validator.add_result(
        "Agents configurÃ©s",
        agents_count == expected_agents,
        f"{agents_count} agents configurÃ©s (attendu: {expected_agents})"
    )
    
    # Test prompts avec rÃ¨gles d'interpellation
    if hasattr(manager, 'agents') and len(manager.agents) > 0:
        sarah_agent = manager.agents.get("sarah_johnson_journaliste", {})
        sarah_prompt = sarah_agent.get("system_prompt", "")
        has_sarah_interpellation = "RÃˆGLES D'INTERPELLATION CRITIQUES" in sarah_prompt
        validator.add_result(
            "Prompt Sarah avec rÃ¨gles d'interpellation",
            has_sarah_interpellation,
            "RÃ¨gles d'interpellation prÃ©sentes" if has_sarah_interpellation else "RÃ¨gles d'interpellation absentes"
        )
        
        marcus_agent = manager.agents.get("marcus_thompson_expert", {})
        marcus_prompt = marcus_agent.get("system_prompt", "")
        has_marcus_interpellation = "RÃˆGLES D'INTERPELLATION CRITIQUES" in marcus_prompt
        validator.add_result(
            "Prompt Marcus avec rÃ¨gles d'interpellation",
            has_marcus_interpellation,
            "RÃ¨gles d'interpellation prÃ©sentes" if has_marcus_interpellation else "RÃ¨gles d'interpellation absentes"
        )
    
    validator.print_summary()
    return validator.passed_tests == validator.total_tests

async def test_context_integration():
    """Test de l'intÃ©gration du contexte utilisateur"""
    
    print("\nğŸ§ª TEST: INTÃ‰GRATION CONTEXTE UTILISATEUR")
    print("-" * 40)
    
    validator = ValidationTestSuite()
    
    # Configuration
    config = MultiAgentConfig(
        exercise_id="test_context",
        room_prefix="test",
        agents=[],
        interaction_rules={},
        turn_management="moderator_controlled"
    )
    
    manager = EnhancedMultiAgentManager("fake_key", "fake_key", config)
    
    # Test configuration contexte
    try:
        manager.set_user_context("Pierre", "Intelligence Artificielle")
        context = manager.get_user_context()
        
        has_correct_name = context.get('user_name') == "Pierre"
        has_correct_subject = context.get('user_subject') == "Intelligence Artificielle"
        
        validator.add_result(
            "Configuration contexte utilisateur",
            has_correct_name and has_correct_subject,
            f"Nom: {context.get('user_name')}, Sujet: {context.get('user_subject')}"
        )
    except Exception as e:
        validator.add_result(
            "Configuration contexte utilisateur",
            False,
            f"Erreur: {e}"
        )
    
    # Test injection dans prompts
    if hasattr(manager, 'agents') and len(manager.agents) > 0:
        michel_agent = manager.agents.get("michel_dubois_animateur", {})
        michel_prompt = michel_agent.get("system_prompt", "")
        
        has_user_name = "Pierre" in michel_prompt
        has_user_subject = "Intelligence Artificielle" in michel_prompt
        
        validator.add_result(
            "Injection contexte dans prompt Michel",
            has_user_name and has_user_subject,
            f"Nom prÃ©sent: {has_user_name}, Sujet prÃ©sent: {has_user_subject}"
        )
    
    validator.print_summary()
    return validator.passed_tests == validator.total_tests

async def run_validation_suite():
    """ExÃ©cute la suite complÃ¨te de validation"""
    
    print("ğŸ¯ VALIDATION COMPLÃˆTE DU SYSTÃˆME D'INTERPELLATION")
    print("=" * 60)
    
    if not IMPORTS_SUCCESS:
        print("âŒ Impossible de continuer - imports Ã©chouÃ©s")
        return False
    
    try:
        # Test 1: Patterns de dÃ©tection
        test1_passed = await test_detector_patterns()
        
        # Test 2: IntÃ©gration Enhanced Manager
        test2_passed = await test_enhanced_manager_integration()
        
        # Test 3: IntÃ©gration contexte
        test3_passed = await test_context_integration()
        
        # RÃ©sumÃ© final
        print("\n" + "="*60)
        print("ğŸ¯ RÃ‰SUMÃ‰ FINAL DE VALIDATION")
        print("="*60)
        
        all_tests = [
            ("Patterns de dÃ©tection", test1_passed),
            ("IntÃ©gration Enhanced Manager", test2_passed),
            ("IntÃ©gration contexte", test3_passed)
        ]
        
        passed_count = sum(1 for _, passed in all_tests if passed)
        total_count = len(all_tests)
        
        for test_name, passed in all_tests:
            status = "âœ… PASSÃ‰" if passed else "âŒ Ã‰CHEC"
            print(f"{status}: {test_name}")
        
        print(f"\nRÃ©sultat global: {passed_count}/{total_count} tests passÃ©s")
        
        if passed_count == total_count:
            print("\nğŸ‰ SYSTÃˆME D'INTERPELLATION COMPLÃˆTEMENT VALIDÃ‰ !")
            print("âœ… Sarah et Marcus rÃ©pondront systÃ©matiquement quand interpellÃ©s")
            print("âœ… Les dÃ©bats TV seront parfaitement orchestrÃ©s")
            print("âœ… Le systÃ¨me est prÃªt pour la production")
            return True
        else:
            print(f"\nâš ï¸ {total_count - passed_count} test(s) ont Ã©chouÃ©")
            print("âŒ Des corrections sont nÃ©cessaires avant la production")
            return False
            
    except Exception as e:
        print(f"\nâŒ ERREUR LORS DE LA VALIDATION: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    success = asyncio.run(run_validation_suite())
    exit(0 if success else 1)

