"""
Syst√®me de r√©ponse garantie pour interpellations directes
"""
import asyncio
import logging
import os
from typing import Dict, Any

logger = logging.getLogger(__name__)


class GuaranteedResponseSystem:
    """Garantit qu'un agent r√©pond TOUJOURS quand interpell√© directement"""

    def __init__(self):
        # Timeouts ULTRA-RAPIDES pour r√©activit√© maximale
        self.response_timeouts: Dict[str, float] = {
            'direct_address': 1.2,      # R√©duit de 2.0 √† 1.2s
            'opinion_request': 1.5,     # R√©duit de 2.5 √† 1.5s
            'action_request': 1.8,      # R√©duit de 3.0 √† 1.8s
            'explanation_request': 2.0, # R√©duit de 3.5 √† 2.0s
            'general_address': 1.2      # R√©duit de 2.0 √† 1.2s
        }

        self.emergency_responses: Dict[str, list[str]] = {
            'sarah': [
                "Sarah: Merci de me donner la parole, je r√©fl√©chis √† votre question.",
                "Sarah: C'est une excellente question, laissez-moi y r√©fl√©chir un instant.",
                "Sarah: Je vous remercie de me solliciter sur ce point important."
            ],
            'marcus': [
                "Marcus: Merci, c'est une question int√©ressante que vous soulevez.",
                "Marcus: Je vous remercie de me donner l'opportunit√© de m'exprimer.",
                "Marcus: C'est effectivement un point crucial √† aborder."
            ],
            'michel': [
                "Michel: Merci, permettez-moi de reformuler la question.",
                "Michel: C'est un point important que vous soulevez.",
                "Michel: Je vous remercie pour cette intervention."
            ]
        }

    async def ensure_response(self, agent_id: str, query: str, context: Dict[str, Any],
                              address_type: str = 'direct_address') -> Dict[str, Any]:
        """Garantit une r√©ponse de l'agent interpell√©"""

        timeout = self.response_timeouts.get(address_type, 3.0)
        agent = context.get('agents', {}).get(agent_id)

        if not agent:
            logger.error(f"‚ùå Agent {agent_id} non trouv√©")
            return {'error': f'Agent {agent_id} not found'}

        logger.info(f"üö® R√âPONSE GARANTIE pour {getattr(agent, 'name', agent_id)} (timeout: {timeout}s)")

        try:
            # Tentative de r√©ponse normale avec timeout strict
            response = await asyncio.wait_for(
                self._generate_normal_response(agent, query, context, address_type),
                timeout=timeout
            )

            if response and str(response).strip():
                logger.info(f"‚úÖ R√âPONSE NORMALE g√©n√©r√©e pour {getattr(agent, 'name', agent_id)}")
                return {
                    'agent_id': agent_id,
                    'agent_name': getattr(agent, 'name', agent_id),
                    'content': response,
                    'type': 'normal_guaranteed_response',
                    'response_time': timeout,
                    'success': True
                }
            else:
                raise Exception("R√©ponse vide g√©n√©r√©e")

        except (asyncio.TimeoutError, Exception) as e:  # noqa: PERF203
            logger.warning(f"‚è∞ TIMEOUT/ERREUR pour {getattr(agent, 'name', agent_id)}: {e}")

            # Fallback : r√©ponse d'urgence IMM√âDIATE
            emergency_response = self._get_emergency_response(agent_id, query, address_type)

            logger.info(f"üÜò R√âPONSE D'URGENCE activ√©e pour {getattr(agent, 'name', agent_id)}")

            return {
                'agent_id': agent_id,
                'agent_name': getattr(agent, 'name', agent_id),
                'content': emergency_response,
                'type': 'emergency_guaranteed_response',
                'response_time': timeout,
                'success': True,
                'fallback_reason': str(e)
            }

    async def _generate_normal_response(self, agent, query: str, context: Dict[str, Any],
                                        address_type: str) -> str:
        """G√©n√®re une r√©ponse normale avec optimisations de vitesse"""

        agent_first_name = str(getattr(agent, 'name', 'Agent')).split()[0]

        # Prompt optimis√© pour r√©activit√©
        prompt = f"""Tu es {getattr(agent, 'name', 'Agent')} dans un d√©bat TV en direct.

Tu viens d'√™tre interpell√©(e) DIRECTEMENT avec cette question/remarque :
"{query}"

üö® SITUATION CRITIQUE : Tu DOIS r√©pondre IMM√âDIATEMENT car tu as √©t√© explicitement sollicit√©(e).

INSTRUCTIONS ABSOLUES :
- Commence OBLIGATOIREMENT par "{agent_first_name}:"
- R√©ponds DIRECTEMENT √† l'interpellation
- Sois naturel(le) et r√©actif(ve) comme dans un vrai d√©bat TV
- Dur√©e: 15-25 secondes maximum (2-3 phrases)
- Style: {getattr(agent, 'interaction_style', 'professionnel')}
- Ton: Engag√© et confiant

CONTEXTE : D√©bat TV en direct, tu ne peux pas ignorer une interpellation directe.

R√©ponds MAINTENANT :"""

        # Configuration ULTRA-RAPIDE pour r√©activit√© maximale
        response_config = {
            'max_tokens': 50,         # Encore plus court ‚Üí ultra-rapide
            'temperature': 0.7,       # Plus cr√©atif pour r√©ponses naturelles
            'top_p': 0.9,
            'presence_penalty': 0.2
        }

        # Appel LLM optimis√© (√† adapter selon votre syst√®me)
        response = await self._call_llm_optimized(prompt, response_config)

        # S'assurer que la r√©ponse commence par le pr√©nom
        if response and not str(response).strip().lower().startswith(f"{agent_first_name.lower()}:"):
            response = f"{agent_first_name}: {str(response).strip()}"

        return str(response)

    def _get_emergency_response(self, agent_id: str, query: str, address_type: str) -> str:
        """G√©n√®re une r√©ponse d'urgence contextuelle"""

        agent_responses = self.emergency_responses.get(agent_id, [
            "Merci de me donner la parole, je vais r√©pondre √† votre question."
        ])

        qlower = (query or "").lower()
        if 'opinion' in address_type or 'pensez' in qlower:
            return agent_responses[0]  # R√©ponse pour demande d'opinion
        elif 'action' in address_type or 'pouvez' in qlower:
            return agent_responses[1] if len(agent_responses) > 1 else agent_responses[0]
        else:
            return agent_responses[-1] if len(agent_responses) > 2 else agent_responses[0]

    async def _call_llm_optimized(self, prompt: str, config: Dict[str, Any]) -> str:
        """Appel LLM optimis√© avec fallbacks robustes"""
        
        # ESSAI 1: LLM Optimizer (si disponible)
        try:
            # Import absolu pour √©viter les erreurs de module
            import sys
            import os
            current_dir = os.path.dirname(os.path.abspath(__file__))
            if current_dir not in sys.path:
                sys.path.insert(0, current_dir)
            
            from llm_optimizer import llm_optimizer

            # Timeout ULTRA-RAPIDE pour r√©activit√© maximale
            result = await asyncio.wait_for(
                llm_optimizer.get_optimized_response(
                    messages=[{"role": "system", "content": prompt}],
                    task_type='multi_agent_orchestration',
                    complexity={'num_agents': 3, 'context_length': len(prompt), 'interaction_depth': 1},
                    use_cache=True,
                    cache_ttl=120,
                ),
                timeout=1.0,  # R√©duit de 1.6 √† 1.0s
            )
            return str(result.get('response') or "Merci de me donner la parole.")
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è LLM Optimizer non disponible: {e}")

        # ESSAI 2: OpenAI direct (si disponible)
        try:
            openai_key = os.getenv('OPENAI_API_KEY')
            if openai_key:
                import openai as openai_client
                client = openai_client.OpenAI(api_key=openai_key)
                
                response = await asyncio.wait_for(
                    client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[{"role": "system", "content": prompt}],
                        temperature=0.8,
                        max_tokens=50,  # R√©duit pour plus de rapidit√©
                    ),
                    timeout=1.2,  # R√©duit de 2.0 √† 1.2s
                )
                return response.choices[0].message.content
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è OpenAI direct non disponible: {e}")

        # ESSAI 3: Mistral (si disponible)
        try:
            mistral_url = os.getenv('MISTRAL_BASE_URL')
            mistral_key = os.getenv('MISTRAL_API_KEY')
            if mistral_url and mistral_key:
                import aiohttp
                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        mistral_url,
                        headers={
                            "Authorization": f"Bearer {mistral_key}",
                            "Content-Type": "application/json"
                        },
                        json={
                            "model": "mistral-small-latest",
                            "messages": [{"role": "system", "content": prompt}],
                            "max_tokens": 70,
                            "temperature": 0.7
                        },
                        timeout=aiohttp.ClientTimeout(total=2.0)
                    ) as resp:
                        if resp.status == 200:
                            data = await resp.json()
                            return data['choices'][0]['message']['content']
        except Exception as e:
            logger.debug(f"‚ö†Ô∏è Mistral non disponible: {e}")

        # FALLBACK FINAL: R√©ponse d'urgence
        logger.warning("üÜò Utilisation du fallback d'urgence pour LLM")
        await asyncio.sleep(0.1)  # Petit d√©lai pour simuler le traitement
        return "Merci de me donner la parole, c'est une excellente question."


