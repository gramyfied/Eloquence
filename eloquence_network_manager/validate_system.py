#!/usr/bin/env python3
"""
Script de Validation Syst√®me - Gestionnaire R√©seau Eloquence
============================================================

Valide tous les crit√®res de r√©ussite obligatoires selon les sp√©cifications.

Auteur: Eloquence Team
Version: 1.0.0
"""

import asyncio
import os
import sys
import json
import time
import tempfile
import psutil
from pathlib import Path
from colorama import Fore, Back, Style, init

# Initialiser colorama
init(autoreset=True)

# Ajouter le r√©pertoire parent au PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent))

from eloquence_network_manager import EloquenceNetworkManager
from exercise_validator import ExerciseDetector, ExerciseValidator


class SystemValidator:
    """Validateur du syst√®me selon les crit√®res de r√©ussite"""
    
    def __init__(self):
        self.results = []
        self.total_tests = 0
        self.passed_tests = 0
        
    def test(self, name: str, condition: bool, details: str = ""):
        """Enregistre le r√©sultat d'un test"""
        self.total_tests += 1
        
        if condition:
            self.passed_tests += 1
            icon = "‚úÖ"
            color = Fore.GREEN
            status = "PASS"
        else:
            icon = "‚ùå"
            color = Fore.RED
            status = "FAIL"
            
        print(f"{color}{icon} {name}: {status}{Style.RESET_ALL}")
        if details:
            print(f"    {details}")
            
        self.results.append({
            'name': name,
            'passed': condition,
            'details': details
        })
        
    def print_summary(self):
        """Affiche le r√©sum√© final"""
        success_rate = (self.passed_tests / self.total_tests) * 100 if self.total_tests > 0 else 0
        
        print(f"\n{'='*80}")
        print(f"{Fore.CYAN}[*] RESUME DE VALIDATION{Style.RESET_ALL}")
        print(f"{'='*80}")
        print(f"Tests ex√©cut√©s: {self.total_tests}")
        print(f"Tests r√©ussis: {self.passed_tests}")
        print(f"Tests √©chou√©s: {self.total_tests - self.passed_tests}")
        print(f"Taux de r√©ussite: {success_rate:.1f}%")
        
        if success_rate >= 90:
            print(f"\n{Fore.GREEN}[+] VALIDATION REUSSIE - Systeme pret pour la production{Style.RESET_ALL}")
        elif success_rate >= 80:
            print(f"\n{Fore.YELLOW}[!] VALIDATION PARTIELLE - Quelques ameliorations necessaires{Style.RESET_ALL}")
        else:
            print(f"\n{Fore.RED}[-] VALIDATION ECHOUEE - Corrections majeures requises{Style.RESET_ALL}")
            
        return success_rate >= 90


async def validate_system():
    """Fonction principale de validation"""
    validator = SystemValidator()
    
    print(f"{Fore.CYAN}[*] VALIDATION DU GESTIONNAIRE RESEAU ELOQUENCE{Style.RESET_ALL}")
    print(f"{'='*80}\n")
    
    # ===== TESTS DE STRUCTURE ET FICHIERS =====
    print(f"{Fore.BLUE}[+] Tests de Structure{Style.RESET_ALL}")
    print("-" * 30)
    
    # V√©rifier la pr√©sence des fichiers obligatoires
    required_files = [
        'eloquence_network_manager.py',
        'exercise_validator.py', 
        'eloquence_network_config.yaml',
        'requirements.txt',
        'README.md'
    ]
    
    for file_name in required_files:
        file_path = Path(file_name)
        validator.test(
            f"Fichier {file_name} pr√©sent",
            file_path.exists(),
            f"Chemin: {file_path.absolute()}"
        )
        
    # V√©rifier la pr√©sence des tests
    test_files = [
        'tests/test_network_manager.py',
        'tests/test_exercise_validator.py'
    ]
    
    for test_file in test_files:
        test_path = Path(test_file)
        validator.test(
            f"Test {test_file} pr√©sent",
            test_path.exists(),
            f"Chemin: {test_path.absolute()}"
        )
    
    # ===== TESTS DE CONFIGURATION =====
    print(f"\n{Fore.BLUE}[+] Tests de Configuration{Style.RESET_ALL}")
    print("-" * 30)
    
    try:
        # Test de chargement de la configuration YAML
        import yaml
        with open('eloquence_network_config.yaml', 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
            
        validator.test(
            "Configuration YAML valide",
            'services' in config and len(config['services']) >= 6,
            f"Services configur√©s: {len(config.get('services', []))}"
        )
        
        # V√©rifier les services critiques
        service_names = [s['name'] for s in config.get('services', [])]
        critical_services = [
            'eloquence_exercises_api', 'livekit_server', 'livekit_token_service',
            'vosk_stt', 'mistral_conversation', 'redis'
        ]
        
        for service in critical_services:
            validator.test(
                f"Service {service} configur√©",
                service in service_names,
                f"Trouv√© dans la configuration"
            )
            
    except Exception as e:
        validator.test("Configuration YAML valide", False, f"Erreur: {str(e)}")
    
    # ===== TESTS D'INITIALISATION =====
    print(f"\n{Fore.BLUE}üöÄ Tests d'Initialisation{Style.RESET_ALL}")
    print("-" * 30)
    
    try:
        # Test d'import des modules
        validator.test("Import EloquenceNetworkManager", True, "Module import√© avec succ√®s")
        validator.test("Import ExerciseDetector", True, "Module import√© avec succ√®s")
        validator.test("Import ExerciseValidator", True, "Module import√© avec succ√®s")
        
        # Test de cr√©ation d'instance
        manager = EloquenceNetworkManager()
        validator.test("Cr√©ation instance EloquenceNetworkManager", True, "Instance cr√©√©e")
        
        # Test de chargement de configuration
        config_loaded = await manager._load_configuration()
        validator.test("Chargement configuration", config_loaded, "Configuration charg√©e depuis YAML")
        
        validator.test(
            "Services d√©tect√©s automatiquement",
            len(manager.services) >= 6,
            f"{len(manager.services)} services d√©tect√©s"
        )
        
    except Exception as e:
        validator.test("Initialisation syst√®me", False, f"Erreur: {str(e)}")
        
    # ===== TESTS FONCTIONNELS =====
    print(f"\n{Fore.BLUE}‚ö° Tests Fonctionnels{Style.RESET_ALL}")
    print("-" * 30)
    
    try:
        # Test des m√©thodes obligatoires du gestionnaire r√©seau
        manager = EloquenceNetworkManager()
        await manager._load_configuration()
        
        # V√©rifier la pr√©sence des m√©thodes obligatoires
        required_methods = [
            'initialize', 'check_service_health', 'check_all_services',
            'auto_fix_issues', 'generate_report'
        ]
        
        for method_name in required_methods:
            validator.test(
                f"M√©thode {method_name} impl√©ment√©e",
                hasattr(manager, method_name) and callable(getattr(manager, method_name)),
                f"M√©thode trouv√©e dans EloquenceNetworkManager"
            )
            
        # Test de v√©rification de sant√© globale
        manager._init_http_session()
        
        # Mock des v√©rifications pour √©viter les erreurs r√©seau
        from unittest.mock import AsyncMock
        
        async def mock_check_service(service_name):
            from eloquence_network_manager import HealthCheckResult
            return HealthCheckResult(service_name, 'healthy', 100)
            
        original_method = manager.check_service_health
        manager.check_service_health = mock_check_service
        
        start_time = time.time()
        health_report = await manager.check_all_services()
        duration = time.time() - start_time
        
        validator.test(
            "Score de sant√© global calcul√©",
            'global_health_score' in health_report,
            f"Score: {health_report.get('global_health_score', 'N/A')}%"
        )
        
        validator.test(
            "Performance v√©rification < 5s",
            duration < 5.0,
            f"Dur√©e: {duration:.2f}s"
        )
        
        validator.test(
            "Rapport structur√© complet",
            all(key in health_report for key in ['services', 'summary', 'global_health_score']),
            "Toutes les cl√©s requises pr√©sentes"
        )
        
        # Restaurer la m√©thode originale
        manager.check_service_health = original_method
        
        await manager.close()
        
    except Exception as e:
        validator.test("Tests fonctionnels", False, f"Erreur: {str(e)}")
        
    # ===== TESTS DU VALIDATEUR D'EXERCICES =====
    print(f"\n{Fore.BLUE}üèÉ Tests Validateur d'Exercices{Style.RESET_ALL}")
    print("-" * 30)
    
    try:
        # Test du d√©tecteur d'exercices
        detector = ExerciseDetector('.')
        
        # Test des patterns de d√©tection
        validator.test(
            "Patterns de d√©tection configur√©s",
            len(detector.detection_patterns) >= 3,
            f"Patterns pour {len(detector.detection_patterns)} langages"
        )
        
        validator.test(
            "Types d'exercices support√©s",
            len(detector.exercise_type_keywords) >= 7,
            f"{len(detector.exercise_type_keywords)} types support√©s"
        )
        
        # Test d'inf√©rence de type d'exercice
        cosmic_type = detector._infer_exercise_type('cosmic_voice_screen', '')
        validator.test(
            "Inf√©rence type d'exercice",
            cosmic_type == 'cosmic',
            f"cosmic_voice_screen ‚Üí {cosmic_type}"
        )
        
        # Test d'inf√©rence des services requis
        from exercise_validator import ExerciseMetadata
        test_exercise = ExerciseMetadata(
            name='test_cosmic',
            file_path='test.dart',
            exercise_type='cosmic',
            language='flutter'
        )
        
        required_services = detector._infer_required_services(test_exercise)
        validator.test(
            "Services requis inf√©r√©s",
            len(required_services) >= 3 and 'livekit_server' in required_services,
            f"Services: {', '.join(required_services)}"
        )
        
    except Exception as e:
        validator.test("Tests validateur d'exercices", False, f"Erreur: {str(e)}")
        
    # ===== TESTS DE PERFORMANCE =====
    print(f"\n{Fore.BLUE}‚ö° Tests de Performance{Style.RESET_ALL}")
    print("-" * 30)
    
    try:
        # Test de d√©tection d'architecture
        manager = EloquenceNetworkManager()
        await manager._load_configuration()
        
        start_time = time.time()
        # Simulation de d√©tection d'architecture
        service_health = {
            'eloquence_exercises_api': type('HealthResult', (), {'status': 'healthy'})(),
            'livekit_server': type('HealthResult', (), {'status': 'healthy'})(),
            'vosk_stt': type('HealthResult', (), {'status': 'healthy'})(),
        }
        
        # Cr√©er un validateur pour tester la d√©tection d'architecture
        temp_validator = ExerciseValidator(manager)
        arch_type = temp_validator._determine_architecture_type(service_health)
        duration = time.time() - start_time
        
        validator.test(
            "D√©tection architecture < 2s",
            duration < 2.0,
            f"Dur√©e: {duration:.3f}s, Type: {arch_type}"
        )
        
        # Test d'utilisation m√©moire
        process = psutil.Process()
        memory_mb = process.memory_info().rss / 1024 / 1024
        
        validator.test(
            "Utilisation m√©moire < 100MB",
            memory_mb < 100,
            f"M√©moire: {memory_mb:.1f}MB"
        )
        
        await manager.close()
        
    except Exception as e:
        validator.test("Tests de performance", False, f"Erreur: {str(e)}")
        
    # ===== TESTS CLI =====
    print(f"\n{Fore.BLUE}üñ•Ô∏è Tests Interface CLI{Style.RESET_ALL}")
    print("-" * 30)
    
    try:
        # Test de pr√©sence des commandes CLI
        import subprocess
        
        # Test help du gestionnaire principal
        result = subprocess.run(
            [sys.executable, 'eloquence_network_manager.py', '--help'],
            capture_output=True, text=True, timeout=10
        )
        
        validator.test(
            "CLI gestionnaire r√©seau fonctionnel",
            result.returncode == 0 and '--check' in result.stdout,
            "Commande --help ex√©cut√©e avec succ√®s"
        )
        
        # Test help du validateur d'exercices
        result = subprocess.run(
            [sys.executable, 'exercise_validator.py', '--help'],
            capture_output=True, text=True, timeout=10
        )
        
        validator.test(
            "CLI validateur exercices fonctionnel",
            result.returncode == 0 and '--scan' in result.stdout,
            "Commande --help ex√©cut√©e avec succ√®s"
        )
        
    except Exception as e:
        validator.test("Tests interface CLI", False, f"Erreur: {str(e)}")
        
    # ===== TESTS D'INT√âGRATION =====
    print(f"\n{Fore.BLUE}üîó Tests d'Int√©gration{Style.RESET_ALL}")
    print("-" * 30)
    
    try:
        # Test d'ex√©cution des tests unitaires
        result = subprocess.run(
            [sys.executable, '-m', 'pytest', 'tests/', '-v', '--tb=short'],
            capture_output=True, text=True, timeout=60, cwd='.'
        )
        
        # Analyser les r√©sultats des tests
        if result.returncode == 0:
            test_output = result.stdout
            passed_tests = test_output.count(' PASSED')
            failed_tests = test_output.count(' FAILED')
            
            validator.test(
                "Tests unitaires passent",
                failed_tests == 0,
                f"Pass√©s: {passed_tests}, √âchou√©s: {failed_tests}"
            )
        else:
            validator.test(
                "Tests unitaires passent",
                False,
                f"Code de sortie: {result.returncode}"
            )
            
    except subprocess.TimeoutExpired:
        validator.test("Tests unitaires passent", False, "Timeout apr√®s 60s")
    except Exception as e:
        validator.test("Tests unitaires passent", False, f"Erreur: {str(e)}")
        
    # ===== TESTS DE DOCUMENTATION =====
    print(f"\n{Fore.BLUE}[+] Tests de Documentation{Style.RESET_ALL}")
    print("-" * 30)
    
    try:
        # V√©rifier la qualit√© du README
        readme_path = Path('README.md')
        if readme_path.exists():
            readme_content = readme_path.read_text(encoding='utf-8')
            
            required_sections = [
                '## üåü Fonctionnalit√©s',
                '## üöÄ Installation', 
                '## üìã Utilisation',
                '## üß™ Tests',
                '## üö® D√©pannage'
            ]
            
            sections_found = sum(1 for section in required_sections if section in readme_content)
            
            validator.test(
                "Documentation compl√®te",
                sections_found >= 4,
                f"{sections_found}/{len(required_sections)} sections trouv√©es"
            )
            
            validator.test(
                "Exemples d'usage inclus",
                '```bash' in readme_content and 'python3' in readme_content,
                "Exemples de commandes trouv√©s"
            )
            
        else:
            validator.test("Documentation compl√®te", False, "README.md manquant")
            
    except Exception as e:
        validator.test("Tests de documentation", False, f"Erreur: {str(e)}")
    
    # ===== R√âSUM√â FINAL =====
    success = validator.print_summary()
    
    # Sauvegarder le rapport de validation
    report = {
        'timestamp': time.time(),
        'total_tests': validator.total_tests,
        'passed_tests': validator.passed_tests,
        'success_rate': (validator.passed_tests / validator.total_tests) * 100,
        'results': validator.results,
        'system_ready': success
    }
    
    with open('validation_report.json', 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
        
    print(f"\n[*] Rapport de validation sauvegarde: validation_report.json")
    
    return success


if __name__ == '__main__':
    try:
        # Configurer l'event loop pour Windows
        if sys.platform.startswith('win'):
            asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
            
        # Ex√©cuter la validation
        success = asyncio.run(validate_system())
        
        # Code de sortie
        sys.exit(0 if success else 1)
        
    except KeyboardInterrupt:
        print(f"\n{Fore.YELLOW}[!] Validation interrompue par l'utilisateur{Style.RESET_ALL}")
        sys.exit(1)
    except Exception as e:
        print(f"\n{Fore.RED}[-] Erreur fatale lors de la validation: {str(e)}{Style.RESET_ALL}")
        sys.exit(1)