import 'dart:async';
import 'dart:io';
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:livekit_client/livekit_client.dart';
import 'package:logger/logger.dart';
import 'package:permission_handler/permission_handler.dart';

class CleanLiveKitService extends ChangeNotifier {
  Room? _room;
  bool _isConnected = false;
  EventsListener? _listener;

  // D√©claration du MethodChannel au niveau de la classe
  final MethodChannel _audioDiagnosticChannel = const MethodChannel('eloquence/audio');

  Room? get room => _room;
  bool get isConnected => _isConnected;
  LocalParticipant? get localParticipant => _room?.localParticipant;

  final _audioStreamController = StreamController<Uint8List>.broadcast();
  Stream<Uint8List> get onAudioReceivedStream => _audioStreamController.stream;

  Future<bool> connect(String url, String token) async {
    _logger.i('Attempting to connect to LiveKit...');
    _logger.i('[DIAGNOSTIC] LiveKit URL: $url');
    _logger.i('[DIAGNOSTIC] Token (first 30 chars): ${token.length > 30 ? token.substring(0, 30) : token}...');
    
    try {
      // √âTAPE 1: Demander permissions microphone AVANT connexion
      _logger.i('[DIAGNOSTIC] Requesting microphone permissions...');
      
      if (kIsWeb) {
        // Pour le web, les permissions sont g√©r√©es automatiquement par le navigateur
        _logger.i('[DIAGNOSTIC] Web platform - permissions handled by browser');
      } else {
        final micPermission = await Permission.microphone.request();
        if (!micPermission.isGranted) {
          _logger.e('[DIAGNOSTIC] Microphone permission denied!');
          return false;
        }
        _logger.i('[DIAGNOSTIC] Microphone permission granted');
      }

      // Configuration pour mobile (TEST: d√©sactiver adaptiveStream et dynacast pour diagnostiquer les underruns)
      final roomOptions = RoomOptions(
        adaptiveStream: true, // R√©activ√© pour la performance
        dynacast: true,       // R√©activ√© pour la performance
        // Configuration optimis√©e pour mobile
        defaultAudioPublishOptions: const AudioPublishOptions(),
        defaultVideoPublishOptions: const VideoPublishOptions(),
      );
      _logger.i('[DIAGNOSTIC] RoomOptions: adaptiveStream et dynacast activ√©s.');

      _room = Room(roomOptions: roomOptions);
      _listener = _room!.createListener();
      _setupRemoteTrackListener();

      _listener!
        ..on<RoomConnectedEvent>((event) async {
          _logger.i('Successfully connected to LiveKit room: ${event.room.name}');
          _isConnected = true;
          
          // √âTAPE 2: Attendre que localParticipant soit disponible
          _logger.i('[DIAGNOSTIC] Waiting for local participant...');
          await _waitForLocalParticipant();
          
          notifyListeners();
        })
        ..on<RoomDisconnectedEvent>((event) {
          _logger.e('Disconnected from LiveKit room. Reason: ${event.reason}', error: event.reason);
          _isConnected = false;
          _room = null;
          _listener?.dispose();
          _listener = null; // Ensure listener is nulled
          notifyListeners();
        })
        ..on<RoomReconnectedEvent>((event) {
          _logger.i('RoomReconnectedEvent: Reconnected to LiveKit room.');
        })
        ..on<RoomReconnectingEvent>((event) {
          _logger.w('RoomReconnectingEvent: Reconnecting to LiveKit room...');
        });

      final connectOptions = ConnectOptions(
        autoSubscribe: true,
        // Configuration ICE pour appareil physique ET Docker avec TURN servers
        rtcConfiguration: const RTCConfiguration(
          iceServers: [
            // Serveurs STUN publics gratuits
            RTCIceServer(
              urls: ['stun:stun.l.google.com:19302'],
            ),
            RTCIceServer(
              urls: ['stun:stun1.l.google.com:19302'],
            ),
            // Serveur STUN de Cloudflare
            RTCIceServer(
              urls: ['stun:stun.cloudflare.com:3478'],
            ),
            // Serveurs TURN publics pour NAT traversal (Docker ‚Üí Web)
            RTCIceServer(
              urls: [
                'turn:openrelay.metered.ca:80',
                'turn:openrelay.metered.ca:443',
                'turn:openrelay.metered.ca:443?transport=tcp'
              ],
              username: 'openrelayproject',
              credential: 'openrelayproject',
            ),
            // Serveur TURN alternatif
            RTCIceServer(
              urls: [
                'turn:relay1.expressturn.com:3478',
              ],
              username: 'efJBIBF6DKC8QY93XK',
              credential: 'Ghq6WzlkMur8W9bT',
            ),
          ],
          // Configuration ICE agressive pour Docker
          iceTransportPolicy: RTCIceTransportPolicy.all,
        ),
      );

      _logger.i('Connecting with ICE servers configured for mobile');
      _logger.i('[DIAGNOSTIC] Starting connection attempt...');
      
      await _room!.connect(
        url,
        token,
        connectOptions: connectOptions,
      );
      
      _logger.i('[DIAGNOSTIC] Connection successful!');
      return true;
    } catch (e, stackTrace) {
      _logger.e('LiveKit connection failed: $e', error: e, stackTrace: stackTrace);
      _logger.e('[DIAGNOSTIC] Error type: ${e.runtimeType}');
      _logger.e('[DIAGNOSTIC] Error details: ${e.toString()}');
      
      // Diagnostic sp√©cifique pour les erreurs courantes
      if (e.toString().contains('SocketException')) {
        _logger.e('[DIAGNOSTIC] Network error - Check IP address and firewall');
      } else if (e.toString().contains('401') || e.toString().contains('403')) {
        _logger.e('[DIAGNOSTIC] Authentication error - Check token');
      } else if (e.toString().contains('timeout')) {
        _logger.e('[DIAGNOSTIC] Connection timeout - Service may be unreachable');
      }
      _isConnected = false;
      _room = null;
      _listener?.dispose();
      _listener = null;
      notifyListeners();
      return false;
    }
  }

  Future<void> disconnect() async {
    _logger.i('Attempting to disconnect from LiveKit...');
    if (_room != null) {
      await _room!.disconnect();
      // L'√©tat _isConnected et notifyListeners sont g√©r√©s par RoomDisconnectedEvent
    }
    _room = null; // Assurer le nettoyage m√™me si pas d'√©v√©nement
    _isConnected = false; // Assurer l'√©tat
    _listener?.dispose();
    _listener = null;
    notifyListeners(); // Notifier au cas o√π l'√©v√©nement ne serait pas d√©clench√©
    _logger.i('Disconnected from LiveKit.');
  }

  Future<void> _waitForLocalParticipant() async {
    _logger.i('[DIAGNOSTIC] Checking for local participant...');
    
    // Attendre jusqu'√† 10 secondes pour que localParticipant soit disponible
    for (int i = 0; i < 50; i++) {
      if (_room?.localParticipant != null) {
        _logger.i('[DIAGNOSTIC] Local participant found after ${i * 200}ms');
        _logger.i('[DIAGNOSTIC] Local participant identity: ${_room!.localParticipant!.identity}');
        return;
      }
      await Future.delayed(Duration(milliseconds: 200));
    }
    
    _logger.e('[DIAGNOSTIC] Local participant still null after 10 seconds!');
  }

  void _setupRemoteTrackListener() {
    if (_room == null || _listener == null) return;

    _listener!
      ..on<TrackSubscribedEvent>((event) async {
        final participantIdentity = event.participant.identity;
        
        // DIAGNOSTIC AM√âLIOR√â : Logger TOUS les d√©tails du track
        _logger.i('[AUDIO DIAGNOSTIC] Track souscrit:');
        _logger.i('  - Participant: $participantIdentity');
        _logger.i('  - Track SID: ${event.track.sid}');
        _logger.i('  - Track Kind: ${event.track.kind}');
        _logger.i('  - Track Source: ${event.track.source}');
        _logger.i('  - Track Muted: ${event.track.muted}');
        
        // DIAGNOSTIC SP√âCIFIQUE : Audio de l'agent
        if (event.track is RemoteAudioTrack) {
          _logger.i('[AUDIO DIAGNOSTIC] D√©tection R√©ussie: C\'est bien un RemoteAudioTrack.');
          if (participantIdentity.contains('agent') ||
              participantIdentity.contains('eloquence') ||
              participantIdentity.contains('roomio')) {
            
            _logger.i('üîä [CRITIQUE] Audio agent d√©tect√© - DIAGNOSTIC COMPLET');
            await _diagnosticAgentAudio(event.track as RemoteAudioTrack);
          } else {
            _logger.i('üé§ [INFO] Audio utilisateur d√©tect√©: $participantIdentity');
          }
        }
      })
      ..on<TrackUnsubscribedEvent>((event) {
        _logger.i('[AUDIO DIAGNOSTIC] Track d√©souscrit: ${event.track.sid}');
      })
      ..on<TrackMutedEvent>((event) {
        _logger.w('[AUDIO WARNING] Track mut√©: ${event.publication.sid} par ${event.participant.identity}');
      })
      ..on<TrackUnmutedEvent>((event) {
        _logger.i('[AUDIO INFO] Track d√©mut√©: ${event.publication.sid} par ${event.participant.identity}');
      })
      ..on<LocalTrackPublishedEvent>((event) {
        _logger.i('[AUDIO INFO] Track local publi√©: ${event.publication.sid}');
      })
      ..on<DataReceivedEvent>((event) {
        // GARDER pour compatibilit√© mais ne pas utiliser pour l'audio
        final participantIdentity = event.participant?.identity ?? '';
        if (participantIdentity.contains('agent') && _audioStreamController.hasListener) {
          _logger.d('Data received from agent: ${event.data.length} bytes (legacy)');
          _audioStreamController.add(Uint8List.fromList(event.data));
        }
      })
      ..on<TrackPublishedEvent>((event) {
        if (event.participant.identity.contains('agent')) {
          _logger.i('üîä [IMPORTANT] Track publi√© par agent - Type: ${event.publication.kind}');
          _logger.i('  - Source: ${event.publication.source}');
          _logger.i('  - Name: ${event.publication.name}');
        }
      });
  }

  Future<void> _diagnosticAgentAudio(RemoteAudioTrack audioTrack) async {
    _logger.i('üîä [DIAGNOSTIC AGENT AUDIO] D√©but diagnostic...');
    
    try {
      // 1. V√âRIFIER √âTAT DU TRACK
      _logger.i('üîä [DIAGNOSTIC] √âtat track:');
      _logger.i('  - Enabled: ${!audioTrack.muted}');
      _logger.i('  - Muted: ${audioTrack.muted}');
      _logger.i('  - SID: ${audioTrack.sid}');
      
      // 2. FORCER ACTIVATION - Note: Les RemoteAudioTrack ne peuvent pas √™tre "enabled" manuellement
      if (audioTrack.muted) {
        _logger.i('üîä [CORRECTION] Track mut√© d√©tect√© (normal pour RemoteAudioTrack)');
        _logger.i('‚úÖ [CORRECTION] Les RemoteAudioTrack se d√©mutent automatiquement');
      }
      
      // 3. FORCER D√âMUTAGE - Non applicable aux RemoteAudioTrack
      _logger.i('üîä [CORRECTION] D√©mutage automatique par LiveKit');
      
      // 4. FORCER VOLUME MAXIMUM
      _logger.i('üîä [CORRECTION] Configuration volume...');
      try {
        // L'API livekit_client g√®re le volume de lecture des tracks distants automatiquement.
        // Il n'y a pas de m√©thode directe pour forcer le volume d'un RemoteAudioTrack apr√®s sa souscription via son objet.
        _logger.i('‚úÖ [CORRECTION] Volume des tracks distants g√©r√© automatiquement par LiveKit. Assurez-vous que le volume syst√®me de l\'appareil est suffisant.');
      } catch (e) {
        _logger.d('‚ÑπÔ∏è [CORRECTION] Erreur ou m√©thode setVolume non disponible pour RemoteAudioTrack: $e');
      }
      
      // Logique de for√ßage du haut-parleur retir√©e d'ici pour √™tre centralis√©e dans _configureAndroidAudio
      // afin d'√©viter les interf√©rences et les probl√®mes de timing/√©tat.

      // 5. DIAGNOSTIC SYST√àME AUDIO (qui comprend la configuration Android/iOS)
      await _diagnosticSystemAudio();
      
      // 6. FORCER CONFIGURATION WEBRTC
      await _forceWebRTCAudioConfig();
      
      // 7. V√âRIFIER STATISTIQUES
      await _checkAudioStats(audioTrack);
      
      _logger.i('‚úÖ [DIAGNOSTIC AGENT AUDIO] Diagnostic termin√©');
      
    } catch (e) {
      _logger.e('‚ùå [DIAGNOSTIC AGENT AUDIO] Erreur: $e');
    }
  }

  Future<void> _diagnosticSystemAudio() async {
    _logger.i('üîä [DIAGNOSTIC SYST√àME] V√©rification syst√®me audio...');
    
    try {
      // V√©rifier permissions
      final micPermission = await Permission.microphone.status;
      _logger.i('üîä [DIAGNOSTIC SYST√àME] Permission micro: $micPermission');
      
      // Forcer permissions si n√©cessaire
      if (!micPermission.isGranted) {
        _logger.i('üîä [CORRECTION] Demande permission micro...');
        await Permission.microphone.request();
      }
      
      // Configuration sp√©cifique Android
      if (Platform.isAndroid) {
        await _configureAndroidAudio();
      }
      
      // Configuration sp√©cifique iOS
      if (Platform.isIOS) {
        await _configureiOSAudio();
      }
      
    } catch (e) {
      _logger.e('‚ùå [DIAGNOSTIC SYST√àME] Erreur: $e');
    }
  }

  Future<void> _configureAndroidAudio() async {
    _logger.i('üîä [ANDROID] Configuration audio Android...');
    
    try {
      // Utiliser _audioDiagnosticChannel qui est maintenant un membre de la classe
      
      try {
        // Obtenir info audio
        final audioInfo = await _audioDiagnosticChannel.invokeMethod('getAudioInfo');
        _logger.i('üîä [ANDROID] Info audio initiale: $audioInfo');
        
        // Forcer le mode audio √† MODE_IN_COMMUNICATION et activer le haut-parleur
        // puis maximiser le volume du flux d'appel vocal.
        final currentAudioModeString = audioInfo['mode'] as String? ?? 'unknown'; // R√©cup√©rer comme String
        final isSpeakerphoneOnInitial = audioInfo['isSpeakerphoneOn'] as bool? ?? false; // R√©cup√©rer l'√©tat initial du haut-parleur

        // Forcer le mode audio si ce n'est pas d√©j√† 'in_communication'
        if (currentAudioModeString != 'in_communication') {
          _logger.i('üîä [ANDROID] For√ßage du mode audio √† MODE_IN_COMMUNICATION (valeur int: 3)...');
          await _audioDiagnosticChannel.invokeMethod('setMode', 3); // 3 correspond √† AudioManager.MODE_IN_COMMUNICATION
        } else {
          _logger.i('‚úÖ [ANDROID] Mode audio d√©j√† √† MODE_IN_COMMUNICATION.');
        }

        // Toujours forcer le haut-parleur pour s'assurer qu'il est actif
        // M√™me si currentAudioModeString est "in_communication", isSpeakerphoneOn peut √™tre false.
        if (!isSpeakerphoneOnInitial) {
           _logger.i('üîä [ANDROID] For√ßage du haut-parleur...');
           await _audioDiagnosticChannel.invokeMethod('setSpeakerphoneOn', {'on': true});
        } else {
           _logger.i('‚úÖ [ANDROID] Haut-parleur d√©j√† actif.'); // Log si d√©j√† actif
        }

        // Toujours forcer le haut-parleur APRES avoir potentiellement chang√© le mode
        _logger.i('üîä [ANDROID] For√ßage du haut-parleur...');
        await _audioDiagnosticChannel.invokeMethod('setSpeakerphoneOn', {'on': true}); 
        
        // Maximiser le volume du STREAM_VOICE_CALL, souvent utilis√© en MODE_IN_COMMUNICATION
        _logger.i('üîä [ANDROID] Maximiser le volume du STREAM_VOICE_CALL...');
        final maxVoiceCallVolume = audioInfo['maxVoiceCallVolume'] as int? ?? 8;
        await _audioDiagnosticChannel.invokeMethod('setStreamVolume', {
          'streamType': 0, // AudioManager.STREAM_VOICE_CALL
          'volume': maxVoiceCallVolume,  
          'flags': 0       
        });

        final updatedAudioInfo = await _audioDiagnosticChannel.invokeMethod('getAudioInfo');
        _logger.i('üí° üîä [ANDROID] Info audio POST-CONFIG: $updatedAudioInfo');

      } catch (e) {
        _logger.w('‚ö†Ô∏è [ANDROID] Platform channel non configur√© ou erreur d\'appel: $e');
        _logger.i('‚ÑπÔ∏è [ANDROID] Configuration native requise pour diagnostics avanc√©s');
      }
      
    } catch (e) {
      _logger.e('‚ùå [ANDROID] Erreur configuration: $e');
    }
  }

  Future<void> _configureiOSAudio() async {
    _logger.i('üîä [iOS] Configuration audio iOS...');
    
    try {
      // Utiliser _audioDiagnosticChannel qui est maintenant un membre de la classe
      
      try {
        // Configurer session audio
        await _audioDiagnosticChannel.invokeMethod('configureAudioSession', {
          'category': 'playAndRecord',
          'mode': 'default',
          'options': ['defaultToSpeaker', 'allowBluetooth']
        });
        _logger.i('‚úÖ [iOS] Session audio configur√©e');
        
        // Activer session
        await _audioDiagnosticChannel.invokeMethod('setActive', true);
        _logger.i('‚úÖ [iOS] Session audio activ√©e');
        
      } catch (e) {
        _logger.w('‚ö†Ô∏è [iOS] Platform channel non configur√©: $e');
        _logger.i('‚ÑπÔ∏è [iOS] Configuration native requise pour diagnostics avanc√©s');
      }
      
    } catch (e) {
      _logger.e('‚ùå [iOS] Erreur configuration: $e');
    }
  }

  Future<void> _forceWebRTCAudioConfig() async {
    _logger.i('üîä [WEBRTC] Configuration WebRTC audio...');
    
    try {
      if (_room?.engine != null) {
        // V√©rifier √©tat connexion
        final connectionState = _room!.connectionState;
        _logger.i('üîä [WEBRTC] √âtat connexion: $connectionState');
        
        if (connectionState != ConnectionState.connected) {
          _logger.w('‚ö†Ô∏è [WEBRTC] Connexion non √©tablie !');
        }
        
        // Note: Les options de lecture audio sont g√©n√©ralement configur√©es automatiquement
        _logger.i('‚úÖ [WEBRTC] Configuration WebRTC par d√©faut active');
      }
      
    } catch (e) {
      _logger.e('‚ùå [WEBRTC] Erreur configuration: $e');
    }
  }

  Future<void> _checkAudioStats(RemoteAudioTrack audioTrack) async {
    _logger.i('üîä [STATS] V√©rification statistiques audio...');

    try {
      // Obtenir statistiques WebRTC basiques
      _logger.i('üîä [STATS] Track SID: ${audioTrack.sid}');
      _logger.i('üîä [STATS] Track muted: ${audioTrack.muted}');
      _logger.i('üîä [STATS] Track kind: ${audioTrack.kind}');

      // Retarder la r√©cup√©ration des stats pour laisser le temps au flux de s'√©tablir
      await Future.delayed(Duration(seconds: 3));

      if (_room != null) {
        final stats = await _room!.engine.subscriber?.pc?.getStats() ?? [];
        _logger.i('üîä [STATS] Stats compl√®tes LiveKit (nombre de rapports: ${stats.length})');

        for (final report in stats) {
          _logger.i('   - Report ID: ${report.id}, Type: ${report.type}, Timestamp: ${report.timestamp}');
          final Map<String, dynamic> values = Map<String, dynamic>.from(report.values);

          if (report.type == 'inbound-rtp') {
            final String? kind = values['kind'] as String?;

            if (kind == 'audio') {
                _logger.i('   -> [STATS WEBRTC AUDIO INBOUND]');
                _logger.i('      - codecId: ${values['codecId'] ?? 'N/A'}');
                _logger.i('      - bytesReceived: ${values['bytesReceived'] ?? 'N/A'}');
                _logger.i('      - packetsReceived: ${values['packetsReceived'] ?? 'N/A'}');
                _logger.i('      - packetsLost: ${values['packetsLost'] ?? 'N/A'}');
                _logger.i('      - jitter: ${values['jitter'] ?? 'N/A'}');
                _logger.i('      - totalSamplesReceived: ${values['totalSamplesReceived'] ?? 'N/A'}');
                _logger.i('      - audioOutputLevel: ${values['audioOutputLevel'] ?? 'N/A'}');
                _logger.i('      - concealedSamples: ${values['concealedSamples'] ?? 'N/A'}');
                _logger.i('      - totalAudioEnergy: ${values['totalAudioEnergy'] ?? 'N/A'}');
            }
          } else if (report.type == 'outbound-rtp') {
              final String? kind = values['kind'] as String?;
              if (kind == 'audio') {
                _logger.i('   -> [STATS WEBRTC AUDIO OUTBOUND]');
                _logger.i('      - codecId: ${values['codecId'] ?? 'N/A'}');
                _logger.i('      - bytesSent: ${values['bytesSent'] ?? 'N/A'}');
                _logger.i('      - packetsSent: ${values['packetsSent'] ?? 'N/A'}');
              }
          }
        }
      }
      
      // Collect stats from publisher (if available)
      final List<dynamic> publisherStats = await _room!
          .engine.publisher?.pc
          ?.getStats() ?? [];
      
      for (final report in publisherStats) {
        _logger.i('   - Publisher Report ID: ${report.id}, Type: ${report.type}, Timestamp: ${report.timestamp}');
        final Map<String, dynamic> values = Map<String, dynamic>.from(report.values);
        if (values['type'] == 'outbound-rtp') {
          final String? kind = values['kind'] as String?;
          if (kind == 'audio' || kind == 'video') { // Log both for now
            _logger.i('   -> [STATS WEBRTC AUDIO/VIDEO OUTBOUND FROM PUBLISHER]');
            _logger.i('      - codecId: ${values['codecId'] ?? 'N/A'}');
            _logger.i('      - bytesSent: ${values['bytesSent'] ?? 'N/A'}');
            _logger.i('      - packetsSent: ${values['packetsSent'] ?? 'N/A'}');
            _logger.i('      - retries: ${values['retransmittedBytesSent'] ?? 'N/A'}');
          }
        }
      }

      _logger.i('üîä [STATS] Stats de base collect√©es');
      
      // Nouveau Timer p√©riodique pour v√©rifier la continuit√© du track et les stats
      // Il va d√©sormais logger les stats compl√®tes √† chaque it√©ration
      Timer.periodic(Duration(seconds: 5), (timer) async {
        if (timer.tick <= 3) {
          _logger.d('üêõ üîä [STATS - P√©riodique] Track actif: ${audioTrack.sid}, muted: ${audioTrack.muted}');
          if (_room != null) {
            final newStats = await _room!.engine.subscriber?.pc?.getStats() ?? [];
            for (final report in newStats) {
              final Map<String, dynamic> values = Map<String, dynamic>.from(report.values);
              if (values['type'] == 'inbound-rtp') {
                final String? kind = values['kind'] as String?;
                if (kind == 'audio') {
                  _logger.i('   -> [STATS WEBRTC AUDIO INBOUND - P√©riodique]');
                  _logger.i('      - bytesReceived: ${values['bytesReceived'] ?? 'N/A'}');
                  _logger.i('      - packetsReceived: ${values['packetsReceived'] ?? 'N/A'}');
                  _logger.i('      - packetsLost: ${values['packetsLost'] ?? 'N/A'}');
                  _logger.i('      - audioOutputLevel: ${values['audioOutputLevel'] ?? 'N/A'}');
                  _logger.i('      - concealedSamples: ${values['concealedSamples'] ?? 'N/A'}');
                  _logger.i('      - totalAudioEnergy: ${values['totalAudioEnergy'] ?? 'N/A'}');
                }
              }
            }
          }
        } else {
          timer.cancel();
          _logger.i('üîä [STATS] Monitoring des stats audio termin√©.');
        }
      });
      
    } catch (e, stackTrace) {
      _logger.e('‚ùå [STATS] Erreur statistiques: $e', error: e, stackTrace: stackTrace);
    }
  }

  Future<void> publishMyAudio() async {
    if (_room?.localParticipant != null) {
      try {
        _logger.i('Attempting to publish microphone...');
        final LocalTrackPublication<LocalTrack>? publication =
            await _room!.localParticipant!.setMicrophoneEnabled(true);

        if (publication != null) {
          _logger.i('Microphone publication state updated. SID: ${publication.sid}');
          _logger.i('Track Name: ${publication.name}, Source: ${publication.source}');
          _logger.i('Track Muted: ${publication.muted}');
        } else {
          _logger.w('Microphone publication returned null.');
        }
        notifyListeners(); // Notifier que l'√©tat de publication a chang√©
      } catch (e) {
        _logger.e('Error publishing microphone: $e');
      }
    } else {
      _logger.w('Cannot publish audio, local participant is null.');
    }
  }

  Future<void> unpublishMyAudio() async {
    if (_room?.localParticipant != null) {
      try {
        await _room!.localParticipant!.setMicrophoneEnabled(false);
        _logger.i('Microphone unpublished successfully.');
        notifyListeners(); // Notifier que l'√©tat de publication a chang√©
      } catch (e) {
        _logger.e('Error unpublishing microphone: $e');
      }
    } else {
      _logger.w('Cannot unpublish audio, local participant is null.');
    }
  }

  final Logger _logger = Logger(
    printer: PrettyPrinter(
      methodCount: 1,
      errorMethodCount: 5,
      lineLength: 120,
      colors: true,
      printEmojis: true,
      dateTimeFormat: DateTimeFormat.none,
    ),
  );

  @override
  void dispose() {
    _logger.i('Disposing CleanLiveKitService');
    _listener?.dispose();
    _room?.dispose();
    _audioStreamController.close();
    super.dispose();
  }
}
