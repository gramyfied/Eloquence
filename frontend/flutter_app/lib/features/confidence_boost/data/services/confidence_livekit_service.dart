import 'dart:async';
import 'dart:convert';
import 'package:flutter/foundation.dart';
import 'package:flutter/services.dart';
import 'package:livekit_client/livekit_client.dart';
import 'package:http/http.dart' as http;
import 'package:logger/logger.dart';
import 'package:audio_session/audio_session.dart';
import 'package:volume_controller/volume_controller.dart';
import 'package:just_audio/just_audio.dart';
import '../../../../core/config/app_config.dart';
import '../../domain/entities/confidence_scenario.dart';
import '../../domain/entities/confidence_models.dart';

/// Extension pour les phases d'exercice
enum ExercisePhase {
  idle,
  connecting,
  connected,
  ready,
  listening,
  processing,
  responding,
  reconnecting,
  ended,
  error,
  disconnected,
  unknown,
}

/// Service LiveKit complet pour Confidence Boost
/// REMPLACE enti√®rement la solution WebSocket streaming
class ConfidenceLiveKitService {
  static final Logger _logger = Logger();

  // √âtat LiveKit
  Room? _room;
  LocalAudioTrack? _localAudioTrack;
  RemoteAudioTrack? _remoteAudioTrack;
  bool _isConnected = false;
  bool _isPublishing = false;

  // Configuration audio
  AudioSession? _audioSession;
  AudioPlayer? _audioPlayer;
  static const platform = MethodChannel('eloquence.audio/native');

  // Configuration exercice
  ConfidenceScenario? _currentScenario;
  String? _sessionId;
  String? _participantIdentity;

  // Streams temps r√©el - API unifi√©e pour l'UI
  final StreamController<String> _transcriptionController = StreamController.broadcast();
  final StreamController<ConversationMessage> _conversationController = StreamController.broadcast();
  final StreamController<ConfidenceMetrics> _metricsController = StreamController.broadcast();
  final StreamController<ExercisePhase> _phaseController = StreamController.broadcast();
  final StreamController<String> _errorController = StreamController.broadcast();

  // API publique pour l'UI
  Stream<String> get transcriptionStream => _transcriptionController.stream;
  Stream<ConversationMessage> get conversationStream => _conversationController.stream;
  Stream<ConfidenceMetrics> get metricsStream => _metricsController.stream;
  Stream<ExercisePhase> get phaseStream => _phaseController.stream;
  Stream<String> get errorStream => _errorController.stream;

  // Getters d'√©tat
  bool get isConnected => _isConnected;
  bool get isPublishing => _isPublishing;
  String? get sessionId => _sessionId;

  /// D√©marre une session sp√©cialis√©e pour le Tribunal des Id√©es Impossibles
  Future<bool> startTribunalIdeasSession({
    required String userId,
    String? sessionId,
  }) async {
    try {
      _logger.i('‚öñÔ∏è D√©marrage session Tribunal des Id√©es Impossibles via LiveKit');

      // Cr√©er un sc√©nario sp√©cialis√© pour le tribunal
      final tribunalScenario = ConfidenceScenario(
        id: 'tribunal_idees_impossibles',
        title: 'Tribunal des Id√©es Impossibles',
        description: 'D√©fendez des id√©es impossibles devant un tribunal bienveillant',
        difficulty: 'Interm√©diaire',
        prompt: 'D√©fendez une id√©e impossible avec conviction et √©loquence',
        type: ConfidenceScenarioType.presentation,
        durationSeconds: 900, // 15 minutes
        tips: [
          'Structurez votre argumentation',
          'Utilisez des exemples cr√©atifs',
          'Maintenez votre conviction',
          'R√©pondez aux objections avec assurance'
        ],
        keywords: ['argumentation', 'cr√©ativit√©', '√©loquence', 'conviction'],
        icon: '‚öñÔ∏è',
      );

      // G√©n√©rer un ID de session unique
      _sessionId = sessionId ?? 'tribunal_${DateTime.now().millisecondsSinceEpoch}';
      _currentScenario = tribunalScenario;

      _phaseController.add(ExercisePhase.connecting);

      // Configuration audio et connexion LiveKit
      await _configureAudioSession();
      await _ensureAudioVolume();
      await _configureNativeAudio();

      // Obtenir token sp√©cialis√© pour le tribunal
      final tokenData = await _getTribunalToken(userId, _sessionId!);
      if (tokenData == null) {
        throw Exception('Impossible d\'obtenir le token LiveKit pour le tribunal');
      }

      // Cr√©er et configurer la room LiveKit
      _room = Room();
      _setupRoomListeners();

      // Connexion √† LiveKit
      await _room!.connect(
        tokenData['livekit_url'],
        tokenData['token'],
      );

      _participantIdentity = tokenData['participant_identity'];
      _isConnected = true;

      _logger.i('‚úÖ Connexion LiveKit r√©ussie pour le tribunal');
      _phaseController.add(ExercisePhase.connected);

      // D√©marrer publication audio
      await _startAudioPublication();

      // Envoyer configuration sp√©cialis√©e tribunal
      await _sendTribunalConfiguration();

      _phaseController.add(ExercisePhase.ready);
      _logger.i('‚öñÔ∏è Session Tribunal des Id√©es Impossibles pr√™te');

      return true;

    } catch (e, stackTrace) {
      _logger.e('‚ùå Erreur d√©marrage session tribunal: $e', error: e, stackTrace: stackTrace);
      _errorController.add('Erreur connexion tribunal: $e');
      _phaseController.add(ExercisePhase.error);
      await _cleanup();
      return false;
    }
  }

  /// Obtenir token sp√©cialis√© pour le Tribunal des Id√©es Impossibles
  Future<Map<String, dynamic>?> _getTribunalToken(String userId, String sessionId) async {
    try {
      var tokenUrl = AppConfig.livekitTokenUrl.replaceAll('localhost', '192.168.1.44');
      if (tokenUrl.contains(':8003')) {
        tokenUrl = tokenUrl.replaceAll(':8003', ':8004');
        _logger.i('üîß Port corrig√© de 8003 √† 8004 pour le tribunal.');
      }
      
      final participantName = 'tribunal_${userId}_${DateTime.now().millisecondsSinceEpoch}';
      final roomName = 'tribunal_idees_$sessionId';

      _logger.i('‚öñÔ∏è G√©n√©ration token tribunal: $participantName dans $roomName');

      final metadataObject = {
        'exercise_type': 'tribunal_idees_impossibles',
        'scenario_id': 'tribunal_idees_impossibles',
        'scenario_title': 'Tribunal des Id√©es Impossibles',
        'user_id': userId,
        'session_id': sessionId,
        'ai_character': 'juge_magistrat',
        'timestamp': DateTime.now().toIso8601String(),
      };
      
      final requestBody = {
        'participant_name': participantName,
        'room_name': roomName,
        'grants': {
          'roomJoin': true,
          'canPublish': true,
          'canSubscribe': true,
          'canPublishData': true,
        },
        'metadata': metadataObject,
      };

      final response = await http.post(
        Uri.parse('$tokenUrl/generate-token'),
        headers: {
          'Content-Type': 'application/json',
          'Accept': 'application/json',
        },
        body: jsonEncode(requestBody),
      ).timeout(const Duration(seconds: 10));

      if (response.statusCode == 200) {
        final data = jsonDecode(response.body) as Map<String, dynamic>;
        final livekitUrl = AppConfig.livekitUrl.replaceAll('localhost', '192.168.1.44');
        
        return {
          'token': data['token'],
          'livekit_url': livekitUrl,
          'participant_identity': data['participant_identity'] ?? participantName,
          'room_name': data['room_name'] ?? roomName,
        };
      } else {
        throw Exception('Erreur HTTP ${response.statusCode}: ${response.body}');
      }

    } catch (e) {
      _logger.e('‚ùå Erreur obtention token tribunal: $e');
      return null;
    }
  }

  /// Envoyer configuration sp√©cialis√©e pour le tribunal
  Future<void> _sendTribunalConfiguration() async {
    if (!_isConnected || _room?.localParticipant == null) {
      _logger.w('‚ö†Ô∏è Configuration tribunal impossible: pas de connexion active');
      return;
    }

    try {
      final configMessage = {
        'type': 'exercise_config',
        'exercise_type': 'tribunal_idees_impossibles',
        'scenario_id': 'tribunal_idees_impossibles',
        'scenario_title': 'Tribunal des Id√©es Impossibles',
        'scenario_description': 'D√©fendez des id√©es impossibles devant un tribunal bienveillant',
        'session_id': _sessionId,
        'timestamp': DateTime.now().toIso8601String(),
        'ai_character': 'juge_magistrat',
      };

      final jsonData = jsonEncode(configMessage);
      final bytes = utf8.encode(jsonData);

      await _room!.localParticipant!.publishData(bytes, reliable: true);
      _logger.i('‚öñÔ∏è Configuration tribunal envoy√©e');

    } catch (e) {
      _logger.e('‚ùå Erreur envoi configuration tribunal: $e');
      _errorController.add('Erreur configuration tribunal: $e');
    }
  }

  /// D√©marre une session Confidence Boost avec LiveKit
  /// REMPLACE la m√©thode WebSocket startConversation()
  Future<bool> startConfidenceBoostSession({
    required ConfidenceScenario scenario,
    required String userId,
    String? sessionId,
  }) async {
    try {
      _logger.i('üöÄ D√©marrage session Confidence Boost via LiveKit');
      _logger.i('üìù Sc√©nario: ${scenario.title}');

      // G√©n√©rer un ID de session unique
      _sessionId = sessionId ?? 'cb_${DateTime.now().millisecondsSinceEpoch}';
      _currentScenario = scenario;

      _phaseController.add(ExercisePhase.connecting);

      // 0. Configurer l'audio session et le volume
      await _configureAudioSession();
      await _ensureAudioVolume();
      await _configureNativeAudio();

      // 1. Obtenir token LiveKit sp√©cialis√© Confidence Boost
      final tokenData = await _getConfidenceBoostToken(scenario, userId, _sessionId!);
      if (tokenData == null) {
        throw Exception('Impossible d\'obtenir le token LiveKit');
      }

      // 2. Cr√©er et configurer la room LiveKit
      _room = Room();
      _setupRoomListeners();

      // 3. Connexion √† LiveKit avec configuration audio optimis√©e
      await _room!.connect(
        tokenData['livekit_url'],
        tokenData['token'],
      );

      _participantIdentity = tokenData['participant_identity'];
      _isConnected = true;

      _logger.i('‚úÖ Connexion LiveKit r√©ussie');
      _phaseController.add(ExercisePhase.connected);

      // 4. D√©marrer publication audio
      await _startAudioPublication();

      // 5. Envoyer configuration initiale de l'exercice
      await _sendExerciseConfiguration();

      _phaseController.add(ExercisePhase.ready);
      _logger.i('üéØ Session Confidence Boost pr√™te');

      return true;

    } catch (e, stackTrace) {
      _logger.e('‚ùå Erreur d√©marrage session: $e', error: e, stackTrace: stackTrace);
      _errorController.add('Erreur connexion: $e');
      _phaseController.add(ExercisePhase.error);
      await _cleanup();
      return false;
    }
  }

  /// Publication audio avec configuration optimis√©e
  Future<void> _startAudioPublication() async {
    try {
      if (_room?.localParticipant == null) {
        throw Exception('Participant local non disponible');
      }

      // Configuration audio optimis√©e pour conversation
      final audioCaptureOptions = AudioCaptureOptions(
        echoCancellation: true,
        noiseSuppression: true,
        autoGainControl: true,
      );

      _localAudioTrack = await LocalAudioTrack.create(audioCaptureOptions);

      await _room!.localParticipant!.publishAudioTrack(_localAudioTrack!);
      _isPublishing = true;

      _logger.i('üé§ Audio publi√© avec configuration optimis√©e');

    } catch (e) {
      _logger.e('‚ùå Erreur publication audio: $e');
      throw Exception('Impossible de publier l\'audio: $e');
    }
  }

  /// Configuration de l'audio session Flutter
  Future<void> _configureAudioSession() async {
    try {
      _audioSession = await AudioSession.instance;
      await _audioSession!.configure(AudioSessionConfiguration(
        avAudioSessionCategory: AVAudioSessionCategory.playAndRecord,
        avAudioSessionCategoryOptions: AVAudioSessionCategoryOptions.defaultToSpeaker,
        avAudioSessionMode: AVAudioSessionMode.spokenAudio,
        avAudioSessionRouteSharingPolicy: AVAudioSessionRouteSharingPolicy.defaultPolicy,
        avAudioSessionSetActiveOptions: AVAudioSessionSetActiveOptions.none,
        androidAudioAttributes: const AndroidAudioAttributes(
          contentType: AndroidAudioContentType.speech,
          flags: AndroidAudioFlags.audibilityEnforced,
          usage: AndroidAudioUsage.voiceCommunication,
        ),
        androidAudioFocusGainType: AndroidAudioFocusGainType.gain,
        androidWillPauseWhenDucked: false,
      ));
      
      await _audioSession!.setActive(true);
      _logger.i('üîä Audio session configur√©e pour conversation vocale');
    } catch (e) {
      _logger.e('‚ùå Erreur configuration audio session: $e');
      throw Exception('Configuration audio √©chou√©e: $e');
    }
  }

  /// V√©rification et ajustement du volume syst√®me
  Future<void> _ensureAudioVolume() async {
    try {
      final volumeController = VolumeController();
      final currentVolume = await volumeController.getVolume();
      _logger.i('üîä Volume actuel: ${(currentVolume * 100).toInt()}%');
      
      if (currentVolume < 0.3) {
        volumeController.setVolume(0.7);
        _logger.i('üîä Volume ajust√© √† 70%');
      }
    } catch (e) {
      _logger.e('‚ùå Erreur contr√¥le volume: $e');
    }
  }

  /// Configuration audio native Android
  Future<void> _configureNativeAudio() async {
    try {
      await platform.invokeMethod('configureAudioForSpeech');
      await platform.invokeMethod('setAudioToSpeaker');
      _logger.i('üîä Configuration audio native appliqu√©e');
    } catch (e) {
      _logger.e('‚ùå Erreur configuration native: $e');
    }
  }

  /// Configuration des listeners LiveKit
  void _setupRoomListeners() {
    if (_room == null) return;

    // √âv√©nements de connexion
    _room!.addListener(() {
      final state = _room!.connectionState;
      _logger.d('üì° √âtat connexion LiveKit: $state');

      switch (state) {
        case ConnectionState.connected:
          if (!_isConnected) {
            _isConnected = true;
            _phaseController.add(ExercisePhase.connected);
          }
          break;
        case ConnectionState.disconnected:
          if (_isConnected) {
            _isConnected = false;
            _phaseController.add(ExercisePhase.disconnected);
          }
          break;
        case ConnectionState.reconnecting:
          _phaseController.add(ExercisePhase.reconnecting);
          break;
        case ConnectionState.connecting:
          _phaseController.add(ExercisePhase.connecting);
          break;
      }
    });

    // Timer pour v√©rifier p√©riodiquement les nouveaux tracks audio
    Timer.periodic(const Duration(seconds: 1), (timer) {
      if (_room == null) {
        timer.cancel();
        return;
      }
      
      _checkForNewAudioTracks();
    });
    
    // √âcouter les √©v√©nements de participant
    _room!.addListener(() {
      // V√©rifier les changements de participants
      for (final participant in _room!.remoteParticipants.values) {
        _checkParticipantAudioTracks(participant);
      }
    });
  }

  /// V√©rifier et configurer les nouveaux tracks audio
  void _checkForNewAudioTracks() {
    for (final participant in _room!.remoteParticipants.values) {
      _checkParticipantAudioTracks(participant);
    }
  }
  
  /// V√©rifier les tracks audio d'un participant sp√©cifique
  void _checkParticipantAudioTracks(RemoteParticipant participant) {
    for (final publication in participant.audioTrackPublications) {
      if (publication.subscribed && publication.track != null) {
        final track = publication.track as RemoteAudioTrack;
        
        if (_remoteAudioTrack != track) {
          _logger.i('üéµ Nouveau track audio d√©tect√© de ${participant.identity}');
          _handleRemoteAudioTrack(track, participant);
        }
      }
    }
  }
  
  /// G√©rer un nouveau track audio distant avec configuration avanc√©e
  Future<void> _handleRemoteAudioTrack(RemoteAudioTrack track, RemoteParticipant participant) async {
    try {
      _remoteAudioTrack = track;
      
      // D√©marrer le track audio
      await track.start();
      _logger.i('üîä Track audio d√©marr√© pour ${participant.identity}');
      
      // Configuration suppl√©mentaire pour forcer l'audio
      await _ensureAudioRouting();
      
      // V√©rifier si l'audio est effectivement audible apr√®s un court d√©lai
      await Future.delayed(const Duration(milliseconds: 500));
      
      final isAudioPlaying = await _checkAudioPlayback();
      if (!isAudioPlaying) {
        _logger.w('‚ö†Ô∏è Audio LiveKit non d√©tect√©, activation du fallback');
        await _requestAudioFallback();
      } else {
        _logger.i('üîä ‚úÖ Audio LiveKit confirm√© fonctionnel');
      }
      
    } catch (e) {
      _logger.e('‚ùå Erreur configuration RemoteAudioTrack: $e');
      await _requestAudioFallback();
    }
  }
  
  /// Forcer le routage audio vers les haut-parleurs
  Future<void> _ensureAudioRouting() async {
    try {
      // Essayer de forcer l'audio vers les haut-parleurs
      if (_room?.localParticipant != null) {
        _logger.i('üîä Tentative de routage audio vers haut-parleurs');
      }
      
      // Configuration audio session pour routage
      if (_audioSession != null) {
        await _audioSession!.setActive(true);
      }
    } catch (e) {
      _logger.w('‚ö†Ô∏è Routage audio non support√©: $e');
    }
  }
  
  /// V√©rifier si l'audio est effectivement en cours de lecture
  Future<bool> _checkAudioPlayback() async {
    // V√©rification simplifi√©e - v√©rifier si le track existe et est actif
    return _remoteAudioTrack != null;
  }
  
  /// Demander un fallback audio au backend
  Future<void> _requestAudioFallback() async {
    try {
      _logger.i('üîÑ Demande de fallback audio au backend');
      
      // Envoyer une demande de fallback au backend via data channel
      final fallbackRequest = {
        'type': 'audio_fallback_request',
        'session_id': _sessionId,
        'reason': 'livekit_audio_not_playing',
        'timestamp': DateTime.now().toIso8601String(),
      };
      
      final jsonData = jsonEncode(fallbackRequest);
      final bytes = utf8.encode(jsonData);
      
      if (_room?.localParticipant != null) {
        await _room!.localParticipant!.publishData(bytes, reliable: true);
        _logger.i('üì§ Demande de fallback envoy√©e');
      }
      
    } catch (e) {
      _logger.e('‚ùå Erreur demande fallback: $e');
    }
  }
  
  /// Jouer l'audio avec just_audio en fallback
  Future<void> _playWithJustAudio(String audioUrl) async {
    try {
      _audioPlayer ??= AudioPlayer();
      
      // Configurer et jouer l'audio
      await _audioPlayer!.setUrl(audioUrl);
      await _audioPlayer!.play();
      
      _logger.i('üéµ Audio jou√© via just_audio fallback: $audioUrl');
      
      // √âcouter la fin de lecture
      _audioPlayer!.playerStateStream.listen((state) {
        if (state.processingState == ProcessingState.completed) {
          _logger.i('üèÅ Lecture audio fallback termin√©e');
        }
      });
      
    } catch (e) {
      _logger.e('‚ùå Erreur just_audio fallback: $e');
    }
  }
  
  /// Gestion des donn√©es re√ßues via data channel
  void _handleDataReceived(List<int> data, RemoteParticipant? participant) {
    try {
      final jsonString = utf8.decode(data);
      final message = jsonDecode(jsonString) as Map<String, dynamic>;
      
      _logger.d('üì® Donn√©es re√ßues: ${message['type']}');
      
      switch (message['type']) {
        case 'transcription':
          final text = message['text'] as String;
          _transcriptionController.add(text);
          _logger.d('üìù Transcription: $text');
          break;
          
        case 'ai_response':
          final aiMessage = ConversationMessage(
            id: message['id'] ?? DateTime.now().millisecondsSinceEpoch.toString(),
            content: message['content'] as String,
            isUser: false,
            timestamp: DateTime.now(),
          );
          _conversationController.add(aiMessage);
          _logger.d('ü§ñ R√©ponse IA: ${aiMessage.content}');
          break;
          
        case 'audio_fallback_url':
          // Le backend fournit une URL audio en fallback
          final audioUrl = message['audio_url'] as String;
          _logger.i('üîä URL audio fallback re√ßue: $audioUrl');
          _playWithJustAudio(audioUrl);
          break;
          
        case 'metrics':
          final metricsData = message['metrics'] as Map<String, dynamic>;
          final metrics = ConfidenceMetrics(
            confidenceLevel: (metricsData['confidence_level'] ?? 0.0).toDouble(),
            voiceClarity: (metricsData['voice_clarity'] ?? 0.0).toDouble(),
            speakingPace: (metricsData['speaking_pace'] ?? 0.0).toDouble(),
            energyLevel: (metricsData['energy_level'] ?? 0.0).toDouble(),
            timestamp: DateTime.now(),
          );
          _metricsController.add(metrics);
          break;
          
        case 'phase_change':
          final phase = ExercisePhase.values.firstWhere(
            (p) => p.toString().split('.').last == message['phase'],
            orElse: () => ExercisePhase.unknown,
          );
          _phaseController.add(phase);
          break;
      }
    } catch (e) {
      _logger.e('‚ùå Erreur traitement donn√©es: $e');
    }
  }

  /// Envoi de la configuration initiale de l'exercice
  Future<void> _sendExerciseConfiguration() async {
    if (!_isConnected || _room?.localParticipant == null || _currentScenario == null) {
      _logger.w('‚ö†Ô∏è Configuration impossible: pas de connexion active');
      return;
    }

    try {
      final configMessage = {
        'type': 'exercise_config',
        'exercise_type': 'confidence_boost',
        'scenario_id': _currentScenario!.id,
        'scenario_title': _currentScenario!.title,
        'scenario_description': _currentScenario!.description,
        'session_id': _sessionId,
        'timestamp': DateTime.now().toIso8601String(),
        'ai_character': 'thomas', // Ou configurable
      };

      final jsonData = jsonEncode(configMessage);
      final bytes = utf8.encode(jsonData);

      await _room!.localParticipant!.publishData(
        bytes,
        reliable: true,
      );

      _logger.i('üì§ Configuration exercice envoy√©e');

    } catch (e) {
      _logger.e('‚ùå Erreur envoi configuration: $e');
      _errorController.add('Erreur configuration: $e');
    }
  }

  /// Envoi d'un message utilisateur (d√©clenche transcription + IA)
  Future<void> sendUserMessage(String content) async {
    if (!_isConnected || _room?.localParticipant == null) {
      _logger.w('‚ö†Ô∏è Message non envoy√©: pas de connexion active');
      return;
    }

    try {
      // Ajouter le message utilisateur au stream local
      final userMessage = ConversationMessage(
        id: DateTime.now().millisecondsSinceEpoch.toString(),
        content: content,
        isUser: true,
        timestamp: DateTime.now(),
      );
      _conversationController.add(userMessage);

      // Envoyer au backend pour traitement IA
      final message = {
        'type': 'user_message',
        'content': content,
        'session_id': _sessionId,
        'timestamp': DateTime.now().toIso8601String(),
      };

      final jsonData = jsonEncode(message);
      final bytes = utf8.encode(jsonData);

      await _room!.localParticipant!.publishData(
        bytes,
        reliable: true,
      );

      _logger.d('üì§ Message utilisateur envoy√©: $content');

    } catch (e) {
      _logger.e('‚ùå Erreur envoi message: $e');
      _errorController.add('Erreur envoi message: $e');
    }
  }

  /// Obtenir token sp√©cialis√© Confidence Boost
  Future<Map<String, dynamic>?> _getConfidenceBoostToken(
    ConfidenceScenario scenario,
    String userId,
    String sessionId,
  ) async {
    try {
      // Remplacer localhost par IP pour Android et forcer le port correct
      var tokenUrl = AppConfig.livekitTokenUrl.replaceAll('localhost', '192.168.1.44');
      if (tokenUrl.contains(':8003')) {
        tokenUrl = tokenUrl.replaceAll(':8003', ':8004');
        _logger.i('üîß Port corrig√© de 8003 √† 8004.');
      }
      _logger.i('üåê URL finale du token service: $tokenUrl');

      // G√©n√©rer des identifiants uniques
      final participantName = 'flutter_${userId}_${DateTime.now().millisecondsSinceEpoch}';
      final roomName = 'confidence_boost_$sessionId';

      _logger.i('üì° Appel token service: $tokenUrl/generate-token');
      _logger.i('üë§ Participant: $participantName');
      _logger.i('üè† Room: $roomName');

      // M√©tadonn√©es comme objet
      final metadataObject = {
        'exercise_type': 'confidence_boost',
        'scenario_id': scenario.id,
        'scenario_title': scenario.title,
        'user_id': userId,
        'session_id': sessionId,
        'ai_character': 'thomas',
        'timestamp': DateTime.now().toIso8601String(),
      };
      
      final requestBody = {
        'participant_name': participantName,
        'room_name': roomName,
        'grants': {
          'roomJoin': true,
          'canPublish': true,
          'canSubscribe': true,
          'canPublishData': true,
        },
        'metadata': metadataObject,
      };

      final response = await http.post(
        Uri.parse('$tokenUrl/generate-token'),
        headers: {
          'Content-Type': 'application/json',
          'Accept': 'application/json',
        },
        body: jsonEncode(requestBody),
      ).timeout(const Duration(seconds: 10));

      if (response.statusCode == 200) {
        final data = jsonDecode(response.body) as Map<String, dynamic>;
        final livekitUrl = AppConfig.livekitUrl.replaceAll('localhost', '192.168.1.44');
        
        return {
          'token': data['token'],
          'livekit_url': livekitUrl,
          'participant_identity': data['participant_identity'] ?? participantName,
          'room_name': data['room_name'] ?? roomName,
        };
      } else {
        throw Exception('Erreur HTTP ${response.statusCode}: ${response.body}');
      }

    } catch (e) {
      _logger.e('‚ùå Erreur obtention token: $e');
      return null;
    }
  }

  /// Nettoyage des ressources
  Future<void> _cleanup() async {
    try {
      _logger.i('üßπ Nettoyage ressources LiveKit');

      // Arr√™ter publication audio
      if (_localAudioTrack != null) {
        await _localAudioTrack!.stop();
        _localAudioTrack = null;
        _isPublishing = false;
      }

      // Arr√™ter audio distant
      if (_remoteAudioTrack != null) {
        await _remoteAudioTrack!.stop();
        _remoteAudioTrack = null;
      }

      // D√©connecter room
      if (_room != null) {
        await _room!.disconnect();
        _room = null;
      }

      // Arr√™ter audio player
      if (_audioPlayer != null) {
        await _audioPlayer!.stop();
        await _audioPlayer!.dispose();
        _audioPlayer = null;
      }

      // D√©sactiver audio session
      if (_audioSession != null) {
        await _audioSession!.setActive(false);
        _audioSession = null;
      }

      // R√©initialiser √©tat
      _isConnected = false;
      _isPublishing = false;
      _currentScenario = null;
      _sessionId = null;
      _participantIdentity = null;

      _logger.i('‚úÖ Nettoyage termin√©');

    } catch (e) {
      _logger.e('‚ùå Erreur nettoyage: $e');
    }
  }

  /// Terminer la session (m√©thode publique)
  Future<void> endSession() async {
    try {
      _logger.i('üîö Fin de session demand√©e');
      _phaseController.add(ExercisePhase.ended);
      await _cleanup();
    } catch (e) {
      _logger.e('‚ùå Erreur fin de session: $e');
    }
  }

  /// Reconnecter en cas de probl√®me
  Future<bool> reconnect() async {
    try {
      _logger.i('üîÑ Tentative de reconnexion');
      _phaseController.add(ExercisePhase.reconnecting);
      
      if (_currentScenario != null) {
        // Relancer la session avec le m√™me sc√©nario
        return await startConfidenceBoostSession(
          scenario: _currentScenario!,
          userId: 'reconnect_user',
          sessionId: _sessionId,
        );
      }
      
      return false;
    } catch (e) {
      _logger.e('‚ùå Erreur reconnexion: $e');
      return false;
    }
  }

  /// Lib√©rer toutes les ressources (dispose)
  Future<void> dispose() async {
    try {
      _logger.i('üóëÔ∏è Dispose du service LiveKit');
      
      await _cleanup();
      
      // Fermer les streams
      await _transcriptionController.close();
      await _conversationController.close();
      await _metricsController.close();
      await _phaseController.close();
      await _errorController.close();
      
      _logger.i('‚úÖ Service LiveKit dispos√©');
    } catch (e) {
      _logger.e('‚ùå Erreur dispose: $e');
    }
  }
}
